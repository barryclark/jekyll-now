---
layout: post
title: Why does deep and cheap learning work so well? を読んだ
categories: ['ML paper']
---


### TL;DR
- [この論文](https://arxiv.org/abs/1608.08225)を読んでその内容を発表した
<br>

<script async class="speakerdeck-embed" data-id="9c9d229f164648d6bdcb37dc6e4c421e" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>


<br>

## ちょっとコメント
資料を作ったので内容に関してはあんまりここでは書かない。
基本的にはDeep Learningがなぜうまくいくのかという問題に関して数学と物理を用いて定量的・定性的に議論をしたという論文。

数学的な観点に関しては、Deep Learningが多項式ハミルトニアンを記述するのに適していることを示している。
4つのhidden neuronによってmultiplicationを実現できるという定理とその系、そして多項式を表現するのに単層構造は多層構造と比べてneuron数が指数的に増えてしまうという定理が主な登場人物となっている。

物理的な観点に関しては、多項式ハミルトニアンはいくつかの自然な要請によってその自由度が大きく削減されることと、マルコフ過程で各階層毎にデータが生成されることを仮定することで（最小）十分統計量の議論を経由してinferenceがデータ生成過程の逆を辿っていくことで実現されることを示している。
さらに、繰り込み群との対応からDeep Learningが（教師付き）特徴量抽出を実現しているのではないかという議論を展開している。


<br>
数学的な証明の部分はそんなに難しいことをしていないので、ちゃんと読めば理解できるだろう。
繰り込み群の話は読んでいてなかなか面白い話題ではあるが、そもそも物理を知らないとついていくのは難しいかもしれない。
あとこの辺りの話はかなりふわっふわしているので、「興味を持ってくれた読者はあとは君自身の目で確かめよう！」的な印象も受ける。

こういう研究はそこまで主流ではないと思うが、Deep Learningの物理的理解という話題は個人的には興味があるので、今後も定期的にこういう論文が出て欲しい。

---
---
<br>

