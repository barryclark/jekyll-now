---
layout: post
title: "Course Reflection"
author: Josh
---

The world is an intriguing place and, day-to-day, I ask a lot of questions about various things I see.  These questions aren't necessarily well-formed *research* questions with developed notions of the question's significance (that usually comes later after days of mulling over the question), but, at the very least, either a topic or a question (and sometimes both).  For example, while reading Asimov, I'm liable to ask why he uses the word *sardonic* (and variations thereof) so frequently and then, subsequently, how much more frequently he uses this word compared to other authors.  These two related questions first came into my mind 15 years ago, and have been refreshed every time I've read something by Asimov since then.  Until I took this course, however, I would have had no idea how to answer this question.  Well, I suppose that's not entirely true - I could have counted (manually) every time he used the word, counted every time another author used the same word, and then compared the two counts. The labor involved in such an approach, however, never seemed warranted by the relatively unimportant nature of the question.  Now, having taken this course, I know that this is a text-analysis problem that can be approached with one or more text-analysis methods.  These methods might include [n-grams](https://en.wikipedia.org/wiki/N-gram) or [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). And, as it turns out, these methods can be executed with relatively little knowledge of programming in a short amount of time. In a single morning, with minimal experience with [Python](https://www.python.org/), I was able to run a low-fidelity (low-fidelity because there were a lot of bugs which means the validity of the results is suspect) tf-idf analysis to determine just how often Asimov actually uses *sardonic* relative to other novel words.  After years of wondering, I finally had [a partial answer](https://libbyh.github.io/methods-f16/asimov-text-analysis/) to these questions.

Another couple of questions came to mind around this time last year when I was lurking on both the Illinois Tech and UChicago [Yik Yak](https://www.yikyak.com/home) feeds a lot.  I was interested in the apparent differences in culture between the two feeds.  The latter seemed to have far more developed norms (which were reflected both in types of posts and in self-policing of offensive content via up- and down-votes), a plethora of memes, and a *feeling* of community (the former had none of these things).  I wondered whether these differences were specific to these two schools, whether they were related to sense of IRL community that may differ between a school like UChicago and a smaller school like Illinois Tech with many commuter students, whether the differences might have to do with size of the online community irrespective of other factors like on-campus vs. commuter students, or some combination of these factors (and possibly other factors not listed).  I know now that this can probably be approached by hand-coding data and then building [topic models](http://toolingup.stanford.edu/?page_id=205) to detect at least some of the phenomena I observed, and then automatically code data from a plethora of schools for comparison.  I can employ the same methods to answer my research question related to the conditions under which Yik Yak users are likely to self-disclose and to reciprocate self-disclosure as well as when self-disclosure is likely to be met with unsupportive or harassing responses, which may be important in building supportive, online communities that take the best of the online behaviors anonymity engenders while minimizing the worst. 

None of the methods discussed above have anything to do with hypothesis testing, making them all a departure from a place of familiarity for me.  Previously, I would probably have created an experiment or a survey to answer the question about self-disclosure on Yik Yak.  Approaching this with algorithmically with topic models is likely to be cost less, yield higher-powered results (by virtue of the number of posts I could code vs. number of people I could reasonably survey or include in an experiment), and have higher ecological validity than experimental or survey methods.  Now, instead of this being something that I have to wait years to tackle until I'm eligible for some sizable research grants, the only barriers to acquiring new knowledge are my own skill levels (or the skill levels of people I know - digital humanities research is collaborative, after all) with the tools required for topic modelling.  And this brings me to an important part of digital humanities research: collaboration.

I believe that I've met all the learning goals for this class, and am glad to have been able to do so - I feel that meeting these goals has made me a more effective researcher of stuff and communicator of knowledge.  My final project (which should be done any time now...) will demonstrate that I have attained these goals, showing that I know which methods to apply to a situation, manage a project from start to finish (well, not really finished, because the work is iterative and there's always another question to answer...), and use a variety of research tools.

Additionally, I believe I've worked toward developing a number of the cultural competencies identified in the course's syllabus.  Through my final project which entails visualizations of social networks, I've gained competency with simulations.  With each line of code I write for this class toward creating scripts to automate tasks that would otherwise take days (if not weeks) to do manually, I become better at distributed cognition - using tools to extend my mental capacities.  If you go back to my [first blog post](https://libbyh.github.io/methods-f16/joshs-first-post/), you'll not only see that this was the competency that I was most interested in developing, but also that my previous notion of the competency has changed over the course of the semester.  By practicing identifying and evaluating arguments, as well as by creating some of my own, I've become a more able judge of information and sources of information.  Lastly, in tracing the ways that information from the mouths of presidential candidates translates into hashtags (either verbatim, or in reaction to the original information) on Twitter, I've gained some competency with transmedia navigation - a concept that I had never heard of before.

Looking back over the semester as I write this reflection, it's pretty clear that I've learned more in this course than in any other course I took this semester.  I also see a path forward with my pet idea for a senior capstone project (the Yik Yak anonymity thing), which I had previously dismissed as impractical to do as an undergrad with no funding for research participants.  It seems like this course has opened doors to new types of knowledge attained through new methods of inquiry and research.  As a person interested in being a career researcher of things, what more could I ask for?
