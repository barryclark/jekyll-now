---
layout: post
title: Project Luther - Horror Movies
permalink: /Projects/Luther
---
## A New Challenge and Venture

For our second project we got to collect and clean our own data that we scraped from a site of our choice. I love movies and decided to investigate horror movies. I avoid this genre as much as I can during my day to day, so I saw this as a good chance to rub shoulders with these neglected movies. My goal was to predict the gross of a movie based on a number of variables that I collected from IMDb for each movie.

## IMDb

There are a few movie sites with many charts, ratings, and comments from users. Initially I contemplated using the-numbers. Growing up I had nerded out on that site, having grown up in LA and being surrounded by movies. I really enjoyed the dry data and tables that were on the-numbers. However IMDb seemed to be cleaner and better suited for the problem I was trying to solve. IMDb is a goldmine of movie data.  


## Scraping

One of the important steps for this project was the data collection. We had the option of using either Beautiful Soup or Selenium to scrape webpages for our data. For more involved scraping Selenium comes in handy since it acts as a user and can click, select, enter text etc. Beautiful Soup simply scrapes the HTML of the page without any interaction with the front end of the site.

For my project I used Beautiful Soup and scraped 3048 horror movies from IMDb. It was not completely clear how much information I would need, so I decided to scrape as much as I could. I ended up with a healthy set of variables to potentially utilize when modeling the data. I also scraped Box Office Mojo's average tickets per year price table which I used to adjust the gross each movie made. A ticket back in 1910 was 10 cents, whereas today's average ticket price according to the site is $8.65.

Putting the two tables together I had {title	mpaa	duration	genre	imdb	metascore	text	directors_all	stars_all	reviews	gross	intyear	price	tix}.


## Cleaning, oh the Cleaning

Now that I had a database with multiple columns are 3048 rows, I had to make some decisions as to how to clean and organize the data. Since I wanted to predict the gross of the horror movies, I had to put aside all but 804 of the movies from the movies I scraped, since these were the movies with gross information.

I decided to keep the genre data. Instead of having genre as one variable, I split the columns by genre and assigned a 0 or 1 depending on if the movie had been labeled with that genre. Some movies had more than one genre assigned to them. I removed the three columns that had unique text in them, namely the directors, the stars, and the text description of the movies.

For the MPAA ratings, I needed to collapse some of the categories together since some, like X or NC-17 had only a few movies. I made the decision to group the 'Unrated' movies with the racy X and NC-17 horror movies. I had the movies that didn't have any ratings in the middle of my scale, with PG-13 and R around this category. My final MPAA category was one a scale from 1-5 for every movie.

![MPAA]({{ site.url }}/images/mpaa.png)


## Some Models and More Data Cleaning

Using the sklearn package I ran the cleaned data through a few different models. I started with Linear Regression to see what correlations look like for my variables, and if any one variable or a group of them would do a good job in predicting the number of tickets sold. There didn't seem to be much conclusive relationships to predict the gross.

I moved on to Lasso cross-validation where I chopped the data into 5 parts and did a training and testing to see what the model predicts. I also looked at Decision Tree Regression. And lastly I tried Gradient Boosting Tree Regression. Out of these three models, the last seemed to give the lowest root mean squared errors.

I realized that there were many movies clustered near zero for their gross, and noticed that there were 200+ movies which had gross totals of less than half a million. There were even a few movies that were basically $0 in my database, since they made less than $50K. This graph shows the clear skewed distribution of the horror movie grosses.

![500]({{ site.url }}/images/under_500.png)

I decided to put these 200+ movies aside and work with the remaining 589. With all the movies now grossing at over $500K, the data was a lot better balanced and distributed as seen from the next graph.

![501]({{ site.url }}/images/over_500.png)

Running the current data through the Gradient Boosting Regression model, I got a root mean squared error value of 8.427 million tickets. This seemed like a high error. Plotting the test data with their predicted vs. actual grosses looked peculiar as seen in the graph below.

![exorcist]({{ site.url }}/images/with_exorcist.png)

There was one outlier movie, The Exorcist, which made a lot more than the model was predicting. Putting this movie data point aside, cut the error of my model by half. There were four movies that the model predicted would do much better than they did. With these four movies put aside, the RMSE was now 3.917M and my R-squared .247. There seemed to be a fairly acceptable prediction of the number of tickets that a horror movie would make with this model. The features that most contributed to my model were the number of IMDb reviews, the year when the movie was released, the duration, and fourth was the IMDb rating. The rest of the features had pretty low coefficients as seen by the bar graph below.

![features]({{ site.url }}/images/download.png)



## Conclusion

Overall Project Luther honed my skills of examining HTML code and scraping a webpage to extract the desired information. Using Gradient Boosting I modeled the scraped horror movie data to come up with a set of variables that attempted to predict the number of ticket sales.
