---
layout: post
title: Information theory for computer science 101
---


Information theory offers a useful set of tools for understanding all sorts of problems in computer science including algorithms efficiency, communication protocols, compression, and machine learning. The purpose of this blog post is to help break down these often-confusing concepts into basic components so that it becomes easier to build a deep understanding and make practical use of them.

At their core, information theory measures are about understanding how to measure the amount of decision power that is gained from observations. These measures are built on top of probability distributions and allow us to understand aspects of these distributions. The most common measures in information theory are the self-information or surprisal, entropy, Kullback-Leibler divergence, and mutual information. We will cover these in order from simplest to most complex, though none of them are particularly difficult if you look at them from the right angle.

The self-information or simply information is basically just a logarithm. It is defined as $$I(x) = log(p(x))$$ where $$x$$ is a particular outcome of a random variable $$X$$. The logarithm simply allows us to turn the probability into a positive number representing the amount that you would have learned about $$X$$ by observing outcome $$x$$. This idea of the amount that you learn is based on the fact that an outcome with a smaller probability is less likely and therefore you will be more surprised to observe it (hence the alternative name for the self-information, the surprisal). If you are more surprised, then your beliefs will have changed more significantly, and you will have learned more and therefore gained more information. We can also put this in terms of an example. Let’s say that you have a random variable $$W$$ that captures the weather forecast for tomorrow. The variable has 4 outcomes: $$s$$ for sun, $$c$$ for clouds, $$r$$ for rain, and $$h$$ for hail. The probabilities of each outcome are as follows: $$[p(s)=0.1, p(c)=0.6, p(r)=0.299, p(h)=0.001]$$. Let’s fast-forward a day until it is tomorrow and you wakeup. We want to know how much information you will gain about the weather from looking out your window. If you look out your window and see clouds, you will not be that surprised, and so you will not gain that much information. If you see rain, you will be a bit more surprised and so you will gain a bit more information. If you see sun, then you will gain even more, and if you see hail, you will be super surprised and so you will gain tons of information. 

[Insert picture of weather outcomes, their probabilities, and their self information values.]

But exactly how much information will you gain from each observation? In order to quantify this, we need to decide on the unit we want to use to measure information. This unit corresponds to the base of the logarithm. The most common choice of unit is the bit, which corresponds to a choice of base 2 logarithms. Using bits allows us to quantify information in terms of the number of yes/no questions or the number of binary digits we need to represent that information. Other reasonable choices include choosing the number e, which results in a measure called nats and the base 10, which results in the unit called hartleys, which corresponds to the number of decimal digits you need to represent that information. 

In order for these units to make sense, we also need to describe how a string of digits can contain information. Strings can contain information in a similar way as a weather distribution can. A string of a given length represents a certain number of possibilities. For instance there are exactly 2 possible binary strings of length 1, 4 of length 2, and 16 of length 4. In general there are exactly 2^n possible binary strings of length n. Each different string corresponds to a different possible outcome. If you have a binary string of length 4, then you can communicate one of 16 different messages. If your string is of length 32, then there are 4,294,967,296 things you can say. The number of possibilities grows exponentially with the number of bits.

How then does this relate to our weather example? There are 4 possible outcomes, so shouldn’t we need exactly 2 bits of information to describe the outcome? Not quite. The amount of information required to describe the outcome of a particular random variable is called the entropy. This requires knowing both the number of possible outcomes and the probabilities of those outcomes. In general, it requires knowledge of the probability distribution of the random variable. To see the effect of the probabilities on the number of bits required to describe a particular random variable, lets look at another random variable, this time representing the weather for yesterday. This random variable has the same four possible outcomes, but the probabilities are instead: $$[p(s)=0, p(c)=1, p(r)=0, p(h)=0]$$. In this case, we have complete knowledge of the weather yesterday, so the probability distribution contains no uncertainty. It was cloudy yesterday, and if we were to observe that it was cloudy yesterday we would gain no information because we already knew that. If instead we were to observe that it was sunny yesterday, we would gain infinite information because this directly contradicts our beliefs. Infinite information doesn’t really make sense for this case, as there are many real reasons why we might have been mistaken about yesterday’s weather, and so if this were a more realistic example, these probabilities would not be accurate. Instead, we might use a distribution more like: $$[p(s)=0.001, p(c)=0.997, p(r)=0.001, p(h)=0.001]$$.

As mentioned above, the amount of information required to communicate the outcome of a random variable is called the entropy. The entropy equivalently represents the expected surprisal from an observation of that random variable or the amount of uncertainty in the random variable. Formally, we can represent the entropy as: $$H(X)=\sum_{x\in X}{P(x)log(P(x)})$$
. If we break this down, it becomes incredibly simple. The part of the equation inside the sum is simply the probability of an outcome multiplied by the surprisal of that outcome. This gives us the expected amount of information from that outcome. I.e. the expected amount of information from an outcome is the probability of that outcome times the amount of information provided by that outcome. The sum simply allows us to get the expectation of this (which is already weighted by the probability term). Because high information outcomes are necessarily very rare, their contribution to the total entropy is not that high. If you play around with the entropy a bit, you’ll see that the highest entropy for a random variable with a given number of outcomes occurs when all of the outcomes are equiprobable. This is called the uniform distribution because the probabilities are uniform. 
