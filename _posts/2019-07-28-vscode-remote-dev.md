---
layout: post
title: "Musings about Remote Development with Visual Studio Code"
description: "Short snippets of notes on VSCode Remote Developent extensions from the perspective of a remote compute user"
excerpt: Keeping codes and configuration files in sync between client machine and remote server used to be a drawn-out exercise in personal responsibility via SFTP/SCP. VSCode Remote Development looks to change that - for the better.
---
---

When running computation codes on the remote HPC (high-performance computing) cluster, I need to ensure that I am running the correct version of the code with the intended configuration files which define the parameters of the job to be run. I run computational fluid dynamics (CFD) codes using OpenFOAM v5.0 (a C++ library for CFD), and need to keep track of the combination of constants + boundary and initial conditions + spatio-temporal discretization schemes + solvers + number of compute nodes that I was using for the CFD simulations in the form of configuration files. Often, I have to check these configurations using vim on the remote cluster environment before executing my codes on the cluster, which could sometimes lead to frustration as I would need to check across to ensure that changes to these files on the local desktop environment and the remote cluster environment are in sync with each other.

Amongst my scatter-brained angst of ensuring that the correct jobs are run on the HPC and the correct results are retrieved from the HPC back to my local desktop environment, I wonder - is there a more user-friendly way for me to work and make changes directly on the remote environment, without going through vim and (clumsily, since I do tend to make mistakes on vim) mess up my configuration files?

When I started my current role as a data engineer in a Data Analytics team, the team was getting started with moving code development work onboard an on-premise development cloud. Running data preprocessing and model training codes on our laptops took hours or even days to complete, since our work laptops were meant for general-purpose usage and weren't powerful enough (2 cores, 4 threads) to run deep learning codes and computationally-intensive workflows without crashing halfway. Running our codes on CentOS-based compute VM instances in our development cloud was relatively smoother and faster after the VM instances were scaled up to 32 cores; however, editing codes on-the-fly within the remote instance did not feel as intuitive as developing/debugging codes on the GUI code editors on our Windows-based local machine.

Initially, my remote development workflow looks similar to this:
1. Write/edit code on Notepad++ / Visual Studio Code / Jupyter Notebook within local machine
2. Debug code on Windows CMD terminal / Ubuntu on WSL
3. If code runs correctly for reduced computational load (correct logic, no syntax errors etc.),refactor/modify code to run for full computational load if required.
4. Upload code files + dependencies to remote instance via FTP/SFTP using WinSCP/Filezilla
5. Run code on remote instance
6. Download output files generated by code from remote instance to local machine
7. Check if I am getting the expected output files
8. Repeat Step 1

Problem is, what if we are working with large datasets and we can't store them on our local machine? Do we have to buy even more external storage just to be able to download and work with the data on our local machine, only to re-upload changes back to the remote instance?

That's where Remote Development functionalities