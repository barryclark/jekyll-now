---
layout: post
title: Getting started with Machine Learning 
comments: true
description: "Getting started with Machine Learning "
tags: "machine learning, multinomial Bayes, Naive Bayes, classisication, text"
---

#### What is Machine Learning? 
It is a great topic to debate while drinking a beer. But apart from that, it is exacly what it sounds. Making a machine able to answer some questions/solve some problems.


#### What problems can be solved?

Things like classification, identification, projection, anomaly detection, noise reduction.

#### Classification 

One of the common application of ML is classification. Taken from Wikipedia:

>In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.

So I'll try to produce a kind of hello world in terms of ML classification algorithm.

#### Tools

Data Science currently evolves around a few programming languages (Python, R, Scala, Matlab) and ready made API from Amazon, Google, Microsoft and alike.

For this tutorial I will try to implement a classification algorithm using Python and the commonly used library [scikit](https://scikit-learn.org/stable/index.html). The installation requires other packages as well, so I found [Anaconda](https://www.anaconda.com/distribution/) as a ready to use platform.

~~As an editor I will use [Jupyter Notebook](https://jupyter.org/) whichis embedded in Anaconda and runs in your browser.~~  
Decided to use [Spyder](https://docs.spyder-ide.org/) instead, also bundled in Anaconda.

![Spyder screenshot](https://vladcozma.github.io/blog/images/hello-ml/hello-ml-02.jpg "Spyder")  




#### Problem

Given a set of messages and a set of categories, train the machine to output categories to new messages:

 | Message            | Category    |  
 | ------------------ | :--- |  
 | I want a new car   |  shopping   |  
 | I want speed       |  ?          |  


This will be a *supervised* classification, since I will first feed a data set with classifications already done from which it will learn to do new ones. Therefore I used two datasets. ```training-dataset.csv``` and ```validation-dataset.csv```. After training the model with the training dataset I would expect it will produce de same labels found in the validation dataset. that is because 
> Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. { https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation}.

In case you only have one dataset, you can manually split it in two, or use 
```python
from sklearn.model_selection  import train_test_split
# this method split one dataset into train set and validation set
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33) 
```
  
And you will have two datasets, train and test. Playing with the parameters will give you different accuracy levels.

  
##### Step 1 - loading data
  
I found a library, [Pandas](http://pandas.pydata.org/), which provides data analysis structures and tools

```python
import pandas as pd
trainset = pd.read_csv("training-dataset.csv",sep=";")
print('Training dataset loaded. Description below:\n')
trainset.info()
print('\n')
```

```
Training dataset loaded. Description below:

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 889 entries, 0 to 888
Data columns (total 3 columns):
Message                 889 non-null object
Additional Condition    889 non-null object
Label                   889 non-null object
dtypes: object(3)
memory usage: 20.9+ KB
```
I loaded the training dataset in a pandas dataset type (basically a matrix). Same goes for validation set.
  
  
##### Step 2 - Extracting features
  
Having some texts at hand we need to extract some features out of them which will tell us something about the message it is trying to send. To get the texts split into features, I used the TfidfVectorizer class from scikit.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), min_df=min_occ)
```
`TfidVectorizer` is a way of words extraction which gives an importance rating to words discovered in the texts. See [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for detailed explanation. I added some tweaking parameters to enhance accuracy.

  
##### Step 3 - choose a classifier
  
In scikit-learn, a classifier is an object that implements the methods fit(X, y) and predict(T).

The accuracy of your estimation highly depends on the chosen classifier. And the choice of classifier depends on the type and size of the problem. scikit-learn has some classifiers embedded, as seen in the comparation found on [scikit](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) and looks like  

![Classification methods](https://vladcozma.github.io/blog/images/hello-ml/hello-ml-03.jpg "Classification")  

To choose the right estimator, there's a cheet sheet available as well.  

![Choosing the estimator](https://vladcozma.github.io/blog/images/hello-ml/hello-ml-04.jpg "Estimators")  
  
For the given problem, it seems the common classifier is [MultinomialNB](https://en.wikipedia.org/wiki/Multinomial_distribution), which I used.  
```python
from sklearn.naive_bayes import MultinomialNB
classifier_model = MultinomialNB(alpha=alpha_value, fit_prior=False)
```
The parameters are optional, but they can be used to tweak the accuracy of the model.

  
##### Step 4 - training
  
The classifier then has to learn from a model, and that is by fitting to a training set:
  
```python
classifier_model.fit(trainset.Message, trainset.Label)
```
But I can do this after I performed the vectorization on the input message. To achieve this in one go, scikit has a `Pipeline` class that `Sequentially apply a list of transforms and a final estimator`. So it looks like:
```python
from sklearn.pipeline import Pipeline
training = Pipeline([
        ('vectorizer', vectorizer),
        ('classifier', classifier_model),
])
```
Now I can train the classifier using the pipeline (fit method of the pipeline).
```python
classifier = training.fit(trainset.Message, trainset.Label)
```

 
##### Step 5 - using

Now that I have a classifier, I can actually use it to predict a new *label* from a *Message* or validate the accuracy against a validation set, as described earlier.  
```python
classifier.predict(validationset.Message)
classifier.score(validationset.Message, validationset.Label)
```

Also I can save the model to avoid training on the same set over and over. `pickle` module or `joblib` library can be used for this purpose.
```python
import pickle
s = pickle.dumps(classifier)
```
  

  
  
  
Further steps can be taken in andjusting the pipeline for a better accuracy, with some suggestions [here](https://nlpforhackers.io/text-classification/).   
Code is available [here](https://github.com/VladCozma/ml-classification)
  
  
  



Referrences:  

+ (https://scikit-learn.org)

+ (https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)

+ (https://nlpforhackers.io/text-classification/)
	
+ (https://www.dataquest.io/blog/pandas-cheat-sheet)


