--- 
layout: notebook_simple_rnn_post 
title: How to implement a recurrent neural network Part 2 
---


<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Binary-addition-with-a-non-linear-RNN">
    Binary addition with a non-linear RNN
    <a class="anchor-link" href="#Binary-addition-with-a-non-linear-RNN">
     ¶
    </a>
   </h2>
   <p>
    This part will cover:
   </p>
   <ul>
    <li>
     Store data in
     <a href="http://peterroelants.github.io/posts/rnn_implementation_part02/#Dataset">
      tensor
     </a>
    </li>
    <li>
     Optimization with
     <a href="http://peterroelants.github.io/posts/rnn_implementation_part02/#Rmsprop-with-momentum-optimisation">
      Rmsprop and Nesterov momentum
     </a>
    </li>
   </ul>
   <p>
    While the
    <a href="http://peterroelants.github.io/posts/rnn_implementation_part01/">
     first part
    </a>
    of this tutorial described a simple linear recurrent network, this tutorial will describe an RNN with non-linear transfer functions that is able to learn how to perform
    <a href="https://en.wikipedia.org/wiki/Binary_number#Addition">
     binary addition
    </a>
    from examples.
   </p>
   <p>
    First we import the libraries we need and define the dataset.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [1]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Python imports</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c1"># Matrix and vector computation package</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>  <span class="c1"># Plotting library</span>
<span class="c1"># Allow matplotlib to plot inside this notebook</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1"># Set the seed of the numpy random number generator so that the tutorial is reproducable</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Dataset">
    Dataset
    <a class="anchor-link" href="#Dataset">
     ¶
    </a>
   </h2>
   <p>
    This tutorial uses a dataset of 2000 training samples to train the RNN that can be created with the
    <code>
     create_dataset
    </code>
    method defined below. Each sample consists of two 6-bit input numbers ($x_{i1}$, $x_{i2}$) padded with a 0 to make it 7 characters long, and a 7-bit target number ($t_{i}$) so that $t_{i} = x_{i1} + x_{i2}$ ($i$ is the sample index). The numbers are represented as
    <a href="https://en.wikipedia.org/wiki/Binary_number">
     binary numbers
    </a>
    with the
    <a href="https://en.wikipedia.org/wiki/Most_significant_bit">
     most significant bit
    </a>
    on the right (least significant bit first). This is so that our RNN can perform the addition form left to right.
   </p>
   <p>
    The input and target vectors are stored in a 3th-order tensor. A
    <a href="https://en.wikipedia.org/wiki/Tensor">
     tensor
    </a>
    is a generalisation of vectors and matrices, a vector is a 1st-order tensor, a matrix is a 2nd-order tensor. The order of a tensor is the dimensionality of the array data structure needed to represent it.
    <br>
     The dimensions of our training data (
     <code>
      X_train
     </code>
     ,
     <code>
      T_train
     </code>
     ) are printed after the creation of the dataset below. The first order of our data tensors goes over all the samples (2000 samples), the second order goes over the variables per unit of time (7 timesteps), and the third order goes over the variables for each timestep and sample (e.g. input variables $x_{ik1}$, $x_{ik2}$ with $i$ the sample index and $k$ the timestep). The input tensor
     <code>
      X_train
     </code>
     is visualised in the following figure:
    </br>
   </p>
   <p>
    <img alt="Visualisation of input tensor X" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_Tensor.png"/>
   </p>
   <p>
    The following code block initialises the dataset.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [2]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Create dataset</span>
<span class="n">nb_train</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1"># Number of training samples</span>
<span class="c1"># Addition of 2 n-bit numbers can result in a n+1 bit number</span>
<span class="n">sequence_len</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c1"># Length of the binary sequence</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">):</span>
    <span class="sd">"""Create a dataset for binary addition and return as input, targets."""</span>
    <span class="n">max_int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Maximum integer that can be added</span>
    <span class="n">format_str</span> <span class="o">=</span> <span class="s1">'{:0'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sequence_len</span><span class="p">)</span> <span class="o">+</span> <span class="s1">'b}'</span> <span class="c1"># Transform integer in binary format</span>
    <span class="n">nb_inputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Add 2 binary numbers</span>
    <span class="n">nb_outputs</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Result is 1 binary number</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">,</span> <span class="n">nb_inputs</span><span class="p">))</span>  <span class="c1"># Input samples</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">,</span> <span class="n">nb_outputs</span><span class="p">))</span>  <span class="c1"># Target samples</span>
    <span class="c1"># Fill up the input and target matrix</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">nb_samples</span><span class="p">):</span>
        <span class="c1"># Generate random numbers to add</span>
        <span class="n">nb1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_int</span><span class="p">)</span>
        <span class="n">nb2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_int</span><span class="p">)</span>
        <span class="c1"># Fill current input and target row.</span>
        <span class="c1"># Note that binary numbers are added from right to left, but our RNN reads </span>
        <span class="c1">#  from left to right, so reverse the sequence.</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb1</span><span class="p">)]))</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb2</span><span class="p">)]))</span>
        <span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb1</span><span class="o">+</span><span class="n">nb2</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span>

<span class="c1"># Create training samples</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">T_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">nb_train</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'X_train shape: {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'T_train shape: {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">T_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>X_train shape: (2000, 7, 2)
T_train shape: (2000, 7, 1)
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Binary-addition">
    Binary addition
    <a class="anchor-link" href="#Binary-addition">
     ¶
    </a>
   </h3>
   <p>
    Performing binary addition is a good
    <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf">
     toy problem
    </a>
    to illustrate how recurrent neural networks process input streams into output streams. The network needs to learn how to carry a bit to the next state (memory) and when to output a 0 or 1 dependent on the input and state.
   </p>
   <p>
    The following code prints a visualisation of the inputs and target output we want our network to produce.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [3]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Show an example input and target</span>
<span class="k">def</span> <span class="nf">printSample</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""Print a sample in a more visual way."""</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x1</span><span class="p">])</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">t</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">t</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'x1:   {:s}   {:2d}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">x1</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'x2: + {:s}   {:2d} '</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">x2</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'      -------   --'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'t:  = {:s}   {:2d}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">t</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'y:  = {:s}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    
<span class="c1"># Print the first sample</span>
<span class="n">printSample</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:])</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>x1:   1010010   37
x2: + 1101010   43 
      -------   --
t:  = 0000101   80
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Recurrent-neural-network-architecture">
    Recurrent neural network architecture
    <a class="anchor-link" href="#Recurrent-neural-network-architecture">
     ¶
    </a>
   </h2>
   <p>
    Our recurrent network will take 2 input variables for each sample for each timepoint, transform them to states, and output a single probability that the current output is $1$ (instead of $0$). The input is transformed into the states of the RNN where it can hold information so the network knows what to output the next timestep.
   </p>
   <p>
    There are many ways to visualise the RNN we are going to build. We can visualise the network as in the
    <a href="http://peterroelants.github.io/posts/rnn_implementation_part01/">
     previous part
    </a>
    of our tutorial and unfold the processing of each input, state-update and output of a single timestep separately from the other timesteps.
   </p>
   <p>
    <img alt="Structure of the RNN" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_1.png"/>
   </p>
   <p>
    Or we can view the processing of the full input, state-updates, and full output seperately from each other. The full input tensor can be
    <a href="https://en.wikipedia.org/wiki/Map_%28higher-order_function%29">
     mapped
    </a>
    in parallel to be used directly in the RNN state updates. And also the RNN states can be mapped in parallel to the output of each timestep.
   </p>
   <p>
    <img alt="Structure of the RNN tensor processing" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_2.png"/>
   </p>
   <p>
    The steps are abstracted in different classes below. Each class has a
    <code>
     forward
    </code>
    method that performs the
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part03/#1.-Forward-step">
     forward steps
    </a>
    of backpropagation, and a
    <code>
     backward
    </code>
    method that perform the
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part03/#2.-Backward-step">
     backward
    </a>
    steps of backpropagation.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Processing-of-input-and-output-tensors">
    Processing of input and output tensors
    <a class="anchor-link" href="#Processing-of-input-and-output-tensors">
     ¶
    </a>
   </h3>
   <h4 id="Linear-transformation">
    Linear transformation
    <a class="anchor-link" href="#Linear-transformation">
     ¶
    </a>
   </h4>
   <p>
    Neural networks typically transform input vectors by matrix multiplication and vector addition followed by a non-linear transfer function. The 2-dimensional input vectors to our network ($x_{ik1}$, $x_{ik2}$) are transformed by a $2 \times 3$ weight matrix and a bias vector of size 3. Before they can be added to the states of the RNN. The 3-dimensional state vectors are transformed to a 1-dimensional output vector by a $3 \times 1$ weight matrix and a bias vector of size 1 to give the output probabilities.
   </p>
   <p>
    Since we want to process all inputs for each sample and each timestep in one computation we can use the numpy
    <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.tensordot.html">
     <code>
      tensordot
     </code>
    </a>
    function to perform the dot products. This function takes 2 tensors and the axes that need to be aggregated by summation between the elements and a product of the result. For example the transformation of input X ($2000 \times 7 \times 2$) to the states S ($2000 \times 7 \times 3$) with the help of matrix W ($2 \times 3$) can be done by
    <code>
     S = tensordot(X, W, axes=((-1),(0)))
    </code>
    . This method will sum the elements of the last order (-1) of X with the elements of the first order (0) of W and multiply them together. This is the same as doing the matrix dot product for each [$x_{ik1}$, $x_{ik2}$] vector with W.
    <code>
     tensordot
    </code>
    can then make sure that the underlying computations can be done efficiently and in parallel.
   </p>
   <p>
    These linear tensor transformations are used to transform the input X to the states S, and from the states S to the output Y. This linear transformation, together with its gradient is implemented in the
    <code>
     TensorLinear
    </code>
    class below. Note that the weights are initialized by sampling uniformly between $\pm \sqrt{6.0 / (n_{in} + n_{out})}$
    <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">
     as suggested by X. Glorot
    </a>
    .
   </p>
   <h4 id="Logistic-classification">
    Logistic classification
    <a class="anchor-link" href="#Logistic-classification">
     ¶
    </a>
   </h4>
   <p>
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_intermezzo01/">
     Logistic classification
    </a>
    is used to output the probability that the output at current time step k is 1. This function together with its cost and gradient is implemented in the
    <code>
     LogisticClassifier
    </code>
    class below.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [4]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the linear tensor transformation layer</span>
<span class="k">class</span> <span class="nc">TensorLinear</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""The linear tensor layer applies a linear tensor dot product and a bias to its input."""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">tensor_order</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""Initialse the weight W and bias b parameters."""</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">W</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_out</span><span class="p">))</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># Bias paramters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">tensor_order</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Axes summed over in backprop</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform forward step transformation with the help of a tensor product."""</span>
        <span class="c1"># Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b (for i,j in X.shape[0:1])</span>
        <span class="c1"># Same as: Y = np.einsum('ijk,kl-&gt;ijl', X, self.W) + self.b</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),(</span><span class="mi">0</span><span class="p">)))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">gY</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of this layer."""</span>
        <span class="c1"># Same as: gW = np.einsum('ijk,ijl-&gt;kl', X, gY)</span>
        <span class="c1"># Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) (for i,j in X.shape[0:1])</span>
        <span class="n">gW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gY</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">))</span>
        <span class="n">gB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">)</span>
        <span class="c1"># Same as: gX = np.einsum('ijk,kl-&gt;ijl', gY, self.W.T)</span>
        <span class="c1"># Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) (for i,j in gY.shape[0:1])</span>
        <span class="n">gX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">gY</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),(</span><span class="mi">0</span><span class="p">)))</span>  
        <span class="k">return</span> <span class="n">gX</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [5]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the logistic classifier layer</span>
<span class="k">class</span> <span class="nc">LogisticClassifier</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""The logistic layer applies the logistic function to its inputs."""</span>
   
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradient with respect to the cost function at the inputs of this layer."""</span>
        <span class="c1"># Normalise of the number of samples and sequence length.</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Compute the cost at the output."""</span>
        <span class="c1"># Normalise of the number of samples and sequence length.</span>
        <span class="c1"># Add a small number (1e-99) because Y can become 0 if the network learns</span>
        <span class="c1">#  to perfectly predict the output. log(0) is undefined.</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="o">+</span><span class="mf">1e-99</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">T</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="o">+</span><span class="mf">1e-99</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Unfolding-the-recurrent-states">
    Unfolding the recurrent states
    <a class="anchor-link" href="#Unfolding-the-recurrent-states">
     ¶
    </a>
   </h3>
   <p>
    Just as in the
    <a href="(http://peterroelants.github.io/posts/rnn_implementation_part01/">
     previous part
    </a>
    of this tutorial the recurrent states need to be unfolded through time. This unfolding during
    <a href="https://en.wikipedia.org/wiki/Backpropagation_through_time">
     backpropagation through time
    </a>
    is done by the
    <code>
     RecurrentStateUnfold
    </code>
    class. This class holds the shared weight and bias parameters used to update each state, as well as the initial state that is also treated as a parameter and optimized during backpropagation.
   </p>
   <p>
    The
    <code>
     forward
    </code>
    method of
    <code>
     RecurrentStateUnfold
    </code>
    iteratively updates the states through time and returns the resulting state tensor. The
    <code>
     backward
    </code>
    method propagates the gradients at the outputs of each state backwards through time. Note that at each time $k$ the gradient coming from the output Y needs to be added with the gradient coming from the previous state at time $k+1$. The gradients of the weight and bias parameters are summed over all timestep since they are shared parameters in each state update. The final state gradient at time $k=0$ is used to optimise the initial state $S_0$ since the gradient of the inital state is $\partial \xi / \partial S_{0}$.
   </p>
   <p>
    <code>
     RecurrentStateUnfold
    </code>
    makes use of the
    <code>
     RecurrentStateUpdate
    </code>
    class. The
    <code>
     forward
    </code>
    method of this class combines the transformed input and state at time $k-1$ to output state $k$. The
    <code>
     backward
    </code>
    method propagates the gradient backwards through time for one timestep and calculates the gradients of the parameters of this timestep. The non-linear transfer function used in
    <code>
     RecurrentStateUpdate
    </code>
    is the
    <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">
     hyperbolic tangent
    </a>
    (tanh) function. This function, like the logistic function, is a
    <a href="https://en.wikipedia.org/wiki/Sigmoid_function">
     sigmoid function
    </a>
    that goes from $-1$ to $+1$. The
    <a href="https://theclevermachine.wordpress.com/tag/tanh-function/">
     tanh function
    </a>
    is chosen because the maximum gradient of this function is higher than the maximum gradient of the
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part02/#Logistic-function">
     logistic function
    </a>
    which make vanishing gradients less likely. This tanh transfer function is implemented in the
    <code>
     TanH
    </code>
    class.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [6]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define tanh layer</span>
<span class="k">class</span> <span class="nc">TanH</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""TanH applies the tanh function to its inputs."""</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="n">gTanh</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">gTanh</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [7]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define internal state update layer</span>
<span class="k">class</span> <span class="nc">RecurrentStateUpdate</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Update a given state."""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">"""Initialse the linear transformation and tanh transfer function."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">TanH</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Sk</span><span class="p">):</span>
        <span class="sd">"""Return state k+1 from input and state k."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Xk</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Sk</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Sk0</span><span class="p">,</span> <span class="n">Sk1</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of this layer."""</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Sk1</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="n">gSk0</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Sk0</span><span class="p">,</span> <span class="n">gZ</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gZ</span><span class="p">,</span> <span class="n">gSk0</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [8]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define layer that unfolds the states over time</span>
<span class="k">class</span> <span class="nc">RecurrentStateUnfold</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Unfold the recurrent states."""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="n">nbTimesteps</span><span class="p">):</span>
        <span class="s2">" Initialse the shared parameters, the inital state and state update function."</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">nbStates</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># Shared bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbStates</span><span class="p">)</span>  <span class="c1"># Initial state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span> <span class="o">=</span> <span class="n">nbTimesteps</span>  <span class="c1"># Timesteps to unfold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span> <span class="o">=</span> <span class="n">RecurrentStateUpdate</span><span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>  <span class="c1"># State update function</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Iteratively apply forward step to all states."""</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># State tensor</span>
        <span class="n">S</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">S0</span>  <span class="c1"># Set initial state</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="p">):</span>
            <span class="c1"># Update the states iteratively</span>
            <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:])</span>
        <span class="k">return</span> <span class="n">S</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">gY</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of this layer."""</span>
        <span class="n">gSk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gY</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">,:])</span>  <span class="c1"># Initialise gradient of state outputs</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Initialse gradient tensor for state inputs</span>
        <span class="n">gWSum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>  <span class="c1"># Initialise weight gradients</span>
        <span class="n">gBSum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>  <span class="c1"># Initialse bias gradients</span>
        <span class="c1"># Propagate the gradients iteratively</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># Gradient at state output is gradient from previous state plus gradient from output</span>
            <span class="n">gSk</span> <span class="o">+=</span> <span class="n">gY</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:]</span>
            <span class="c1"># Propgate the gradient back through one state</span>
            <span class="n">gZ</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">gSk</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">gSk</span><span class="p">)</span>
            <span class="n">gWSum</span> <span class="o">+=</span> <span class="n">gW</span>  <span class="c1"># Update total weight gradient</span>
            <span class="n">gBSum</span> <span class="o">+=</span> <span class="n">gB</span>  <span class="c1"># Update total bias gradient</span>
        <span class="n">gS0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gSk</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Get gradient of initial state over all samples</span>
        <span class="k">return</span> <span class="n">gZ</span><span class="p">,</span> <span class="n">gWSum</span><span class="p">,</span> <span class="n">gBSum</span><span class="p">,</span> <span class="n">gS0</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="The-full-network">
    The full network
    <a class="anchor-link" href="#The-full-network">
     ¶
    </a>
   </h3>
   <p>
    The full network that will be trained to perform binary addition of two number is defined in the
    <code>
     RnnBinaryAdder
    </code>
    class below. It initialises all the layers upon creation. The
    <code>
     forward
    </code>
    method performs the full backpropagation forward step through all layers and timesteps and returns the intermediary outputs. The
    <code>
     backward
    </code>
    method performs the backward step through all layers and timesteps and returns the gradients of all the parameters. The
    <code>
     getParamGrads
    </code>
    method performs both steps and returns the gradients of the parameters in a list. The order of this list corresponds to the order of the iterator returned by
    <code>
     get_params_iter
    </code>
    . The parameters returned in the iterator of that last method are the same as the parameters of the network and can be used to change the parameters of the network manually.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [9]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the full network</span>
<span class="k">class</span> <span class="nc">RnnBinaryAdder</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""RNN to perform binary addition of 2 numbers."""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_of_inputs</span><span class="p">,</span> <span class="n">nb_of_outputs</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">):</span>
        <span class="sd">"""Initialse the network layers."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nb_of_inputs</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Input layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span> <span class="o">=</span> <span class="n">RecurrentStateUnfold</span><span class="p">(</span><span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>  <span class="c1"># Recurrent layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nb_of_states</span><span class="p">,</span> <span class="n">nb_of_outputs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Linear output transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticClassifier</span><span class="p">()</span>  <span class="c1"># Classification output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward propagation of input X through all layers."""</span>
        <span class="n">recIn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Linear input transformation</span>
        <span class="c1"># Forward propagate through time and return states</span>
        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">recIn</span><span class="p">)</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">sequence_len</span><span class="o">+</span><span class="mi">1</span><span class="p">,:])</span>  <span class="c1"># Linear output transformation</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>  <span class="c1"># Get classification probabilities</span>
        <span class="c1"># Return: input to recurrent layer, states, input to classifier, output</span>
        <span class="k">return</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Perform the backward propagation through all layers.</span>
<span class="sd">        Input: input samples, network output, intput to recurrent layer, states, targets."""</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>  <span class="c1"># Get output gradient</span>
        <span class="n">gRecOut</span><span class="p">,</span> <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">sequence_len</span><span class="o">+</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">gZ</span><span class="p">)</span>
        <span class="c1"># Propagate gradient backwards through time</span>
        <span class="n">gRnnIn</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gS0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">gRecOut</span><span class="p">)</span>
        <span class="n">gX</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gRnnIn</span><span class="p">)</span>
        <span class="c1"># Return the parameter gradients of: linear output weights, linear output bias,</span>
        <span class="c1">#  recursive weights, recursive bias, linear input weights, linear input bias, initial state.</span>
        <span class="k">return</span> <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span><span class="p">,</span> <span class="n">gS0</span>
    
    <span class="k">def</span> <span class="nf">getOutput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Get the output probabilities of input X."""</span>
        <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Y</span>  <span class="c1"># Only return the output.</span>
    
    <span class="k">def</span> <span class="nf">getBinaryOutput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Get the binary output of input X."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">getParamGrads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradients with respect to input X and target T as a list.</span>
<span class="sd">        The list has the same order as the get_params_iter iterator."""</span>
        <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span><span class="p">,</span> <span class="n">gS0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gS0</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWin</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBin</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWrec</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBrec</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWout</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBout</span><span class="p">))]</span>
    
    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the cost of input X w.r.t. targets T."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters.</span>
<span class="sd">        The iterator has the same order as get_params_grad.</span>
<span class="sd">        The elements returned by the iterator are editable in-place."""</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">S0</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span> 
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]))</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Gradient-Checking">
    Gradient Checking
    <a class="anchor-link" href="#Gradient-Checking">
     ¶
    </a>
   </h2>
   <p>
    As in
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/#Gradient-checking">
     part 4 of our previous tutorial on feedforward nets
    </a>
    the gradient computed by backpropagation is compared with the
    <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">
     numerical gradient
    </a>
    to assert that there are no bugs in the code to compute the gradients. This is done by the code below.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [10]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Do gradient checking</span>
<span class="c1"># Define an RNN to test</span>
<span class="n">RNN</span> <span class="o">=</span> <span class="n">RnnBinaryAdder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c1"># Get the gradients of the parameters from a subset of the data</span>
<span class="n">backprop_grads</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getParamGrads</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:],</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>

<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c1"># Set the small change to compute the numerical gradient</span>
<span class="c1"># Compute the numerical gradients of the parameters in all layers.</span>
<span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
    <span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">p_idx</span><span class="p">]</span>
    <span class="c1"># + eps</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
    <span class="n">plus_cost</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>
    <span class="c1"># - eps</span>
    <span class="n">param</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="n">min_cost</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>
    <span class="c1"># reset param value</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
    <span class="c1"># calculate numerical gradient</span>
    <span class="n">grad_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">plus_cost</span> <span class="o">-</span> <span class="n">min_cost</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
    <span class="c1"># Raise error if the numerical grade is not close to the backprop gradient</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grad_backprop</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">grad_num</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">grad_backprop</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'No gradient errors found'</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>No gradient errors found
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Rmsprop-with-momentum-optimisation">
    Rmsprop with momentum optimisation
    <a class="anchor-link" href="#Rmsprop-with-momentum-optimisation">
     ¶
    </a>
   </h2>
   <p>
    While the
    <a href="http://peterroelants.github.io/posts/rnn_implementation_part01/">
     first part
    </a>
    of this tutorial used
    <a href="https://en.wikipedia.org/wiki/Rprop">
     Rprop
    </a>
    to optimise the network, this part will use the
    <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">
     Rmsprop
    </a>
    algorithm with
    <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">
     Nesterov's accelerated gradient
    </a>
    to perform the optimisation. We replaced the Rprop algorithm because Rprop doesn't work well with
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Stochastic-gradient-descent-backpropagation">
     minibatches
    </a>
    due to the stochastic nature of the error surface that can result in sign changes of the gradient.
   </p>
   <p>
    The Rmsprop algorithm was inspired by the Rprop algorithm. It keeps a
    <a href="https://en.wikipedia.org/wiki/Moving_average">
     moving average
    </a>
    (MA) of the squared gradient for each parameter $\theta$ ($MA = \lambda * MA + (1-\lambda) * (\partial \xi / \partial \theta)^2$, with $\lambda$ the moving average hyperparameter). The gradient is then normalised by dividing by the square root of this moving average (
    <code>
     maSquare
    </code>
    = $(\partial \xi / \partial \theta)/\sqrt{MA}$). This normalised gradient is then used to update the parameters. Note that if $\lambda=0$ the gradient is reduced to its sign.
   </p>
   <p>
    This transformed gradient is not used directly to update the parameters, but it is used to update a velocity parameter (
    <code>
     Vs
    </code>
    ) for each parameter of the network. This parameter is similar to the velocity parameter from our
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/#Backpropagation-updates-with-momentum">
     previous tutorial
    </a>
    but it is used in a slightly different way. Nesterov's accelerated gradient is different from regular momentum in that it applies updates in a different way. While the regular momentum algorithm calculates the gradient at the beginning of the iteration, updates the velocity and moves the parameters according to this velocity, Nesterov's accelerated gradient moves the parameters according to the reduced velocity, then calculates the gradients, updates the velocity, and then moves again according to the local gradient. This has as benefit that the gradient is more informative to do the local update, and can correct for a bad velocity update. The Nesterov updates can be described as:
   </p>
   $$\begin{split}
V_{i+1} &amp; = \lambda V_i - \mu \nabla(\theta_i + \lambda V_i) \\
\theta_{i+1} &amp; = \theta_i + V_{i+1} \\
\end{split}$$
   <p>
    With $\nabla(\theta)$ the local gradient at position $\theta$ in the parameter space. And $i$ the iteration number. This formula can be visualised as in the following illustration (See
    <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">
     Sutskever I.
    </a>
    ):
   </p>
   <p>
    <img alt="Illustration of Nesterov Momentum updates" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/NesterovMomentum.png"/>
   </p>
   <p>
    Note that the training converges to a cost of 0. This convergence is actually not guaranteed. If the parameters of the network start out in a bad position the network might convert to a local minimum that is far from the global minimum. The training is also sensitive to the meta parameters
    <code>
     lmbd
    </code>
    ,
    <code>
     learning_rate
    </code>
    ,
    <code>
     momentum_term
    </code>
    ,
    <code>
     eps
    </code>
    . Try rerunning this yourself to see how many times it actually converges.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [11]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Set hyper-parameters</span>
<span class="n">lmbd</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Rmsprop lambda</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># Learning rate</span>
<span class="n">momentum_term</span> <span class="o">=</span> <span class="mf">0.80</span>  <span class="c1"># Momentum term</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>  <span class="c1"># Numerical stability term to prevent division by zero</span>
<span class="n">mb_size</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Size of the minibatches (number of samples)</span>

<span class="c1"># Create the network</span>
<span class="n">nb_of_states</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Number of states in the recurrent layer</span>
<span class="n">RNN</span> <span class="o">=</span> <span class="n">RnnBinaryAdder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c1"># Set the initial parameters</span>
<span class="n">nbParameters</span> <span class="o">=</span>  <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">())</span>  <span class="c1"># Number of parameters in the network</span>
<span class="n">maSquare</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbParameters</span><span class="p">)]</span>  <span class="c1"># Rmsprop moving average</span>
<span class="n">Vs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbParameters</span><span class="p">)]</span>  <span class="c1"># Velocity</span>

<span class="c1"># Create a list of minibatch costs to be plotted</span>
<span class="n">ls_of_costs</span> <span class="o">=</span> <span class="p">[</span><span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])]</span>
<span class="c1"># Iterate over some iterations</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Iterate over all the minibatches</span>
    <span class="k">for</span> <span class="n">mb</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_train</span><span class="o">/</span><span class="n">mb_size</span><span class="p">):</span>
        <span class="n">X_mb</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mb</span><span class="p">:</span><span class="n">mb</span><span class="o">+</span><span class="n">mb_size</span><span class="p">,:,:]</span>  <span class="c1"># Input minibatch</span>
        <span class="n">T_mb</span> <span class="o">=</span> <span class="n">T_train</span><span class="p">[</span><span class="n">mb</span><span class="p">:</span><span class="n">mb</span><span class="o">+</span><span class="n">mb_size</span><span class="p">,:,:]</span>  <span class="c1"># Target minibatch</span>
        <span class="n">V_tmp</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span> <span class="o">*</span> <span class="n">momentum_term</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Vs</span><span class="p">]</span>
        <span class="c1"># Update each parameters according to previous gradient</span>
        <span class="k">for</span> <span class="n">pIdx</span><span class="p">,</span> <span class="n">P</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
            <span class="n">P</span> <span class="o">+=</span> <span class="n">V_tmp</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span>
        <span class="c1"># Get gradients after following old velocity</span>
        <span class="n">backprop_grads</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getParamGrads</span><span class="p">(</span><span class="n">X_mb</span><span class="p">,</span> <span class="n">T_mb</span><span class="p">)</span>  <span class="c1"># Get the parameter gradients    </span>
        <span class="c1"># Update each parameter seperately</span>
        <span class="k">for</span> <span class="n">pIdx</span><span class="p">,</span> <span class="n">P</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
            <span class="c1"># Update the Rmsprop moving averages</span>
            <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmbd</span> <span class="o">*</span> <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">lmbd</span><span class="p">)</span> <span class="o">*</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
            <span class="c1"># Calculate the Rmsprop normalised gradient</span>
            <span class="n">pGradNorm</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="c1"># Update the momentum velocity</span>
            <span class="n">Vs</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">V_tmp</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">-</span> <span class="n">pGradNorm</span>     
            <span class="n">P</span> <span class="o">-=</span> <span class="n">pGradNorm</span>   <span class="c1"># Update the parameter</span>
        <span class="n">ls_of_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_mb</span><span class="p">),</span> <span class="n">T_mb</span><span class="p">))</span>  <span class="c1"># Add cost to list to plot</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [12]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Plot the cost over the iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ls_of_costs</span><span class="p">,</span> <span class="s1">'b-'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'minibatch iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'$</span><span class="se">\\</span><span class="s1">xi$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decrease of cost over backprop iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_png output_subarea ">
     <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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=
">
     </img>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Test-examples">
    Test examples
    <a class="anchor-link" href="#Test-examples">
     ¶
    </a>
   </h2>
   <p>
    The figure above shows that the training converged to a cost of 0. We expect the network to have learned how to perfectly do binary addition for our training examples. If we put some independent test cases through the network and print them out we can see that the network also outputs the correct output for these test cases.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [13]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Create test samples</span>
<span class="n">nb_test</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">Xtest</span><span class="p">,</span> <span class="n">Ttest</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">nb_test</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c1"># Push test data through network</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getBinaryOutput</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
<span class="n">Yf</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>

<span class="c1"># Print out all test examples</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xtest</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">printSample</span><span class="p">(</span><span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">Ttest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:])</span>
    <span class="k">print</span> <span class="s1">''</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>x1:   0100010   34
x2: + 1100100   19 
      -------   --
t:  = 1010110   53
y:  = 1010110

x1:   1010100   21
x2: + 1110100   23 
      -------   --
t:  = 0011010   44
y:  = 0011010

x1:   1111010   47
x2: + 0000000    0 
      -------   --
t:  = 1111010   47
y:  = 1111010

x1:   1000000    1
x2: + 1111110   63 
      -------   --
t:  = 0000001   64
y:  = 0000001

x1:   1010100   21
x2: + 1010100   21 
      -------   --
t:  = 0101010   42
y:  = 0101010

</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This post at
    <a href="http://peterroelants.github.io/posts/rnn_implementation_part02/">
     peterroelants.github.io
    </a>
    is generated from an IPython notebook file.
    <a href="https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/RNN_implementation/rnn_implementation_part02.ipynb">
     Link to the full IPython notebook file
    </a>
   </p>
  </div>
 </div>
</div>

