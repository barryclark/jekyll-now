--- 
layout: notebook_simple_ann_post 
title: How to implement a neural network Part 5 
---


<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Generalization-of-multiple-layers">
    Generalization of multiple layers
    <a class="anchor-link" href="#Generalization-of-multiple-layers">
     ¶
    </a>
   </h2>
   <p>
    This part will cover:
   </p>
   <ul>
    <li>
     <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Generalization-of-the-layers">
      Generalization to multiple layers
     </a>
    </li>
    <li>
     Minibatches with
     <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Stochastic-gradient-descent-backpropagation">
      stochastic gradient descent
     </a>
    </li>
   </ul>
   <p>
    This tutorial generalizes the feedforward neural network into any number of layers. The concepts of a linear projection via matrix multiplication and non-linear transformation will be generalized. The usage of the generalization will be illustrated by building a small feedforward network that consists of two hidden layers to classify handwritten digits. This network will be trained by
    <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">
     stochastic gradient descent
    </a>
    , a popular variant of gradient descent that updates the parameters each step on only a subset of the parameters.
   </p>
   <p>
    The notebook starts out with importing the libraries we need:
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [1]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Python imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c1"># Matrix and vector computation package</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>  <span class="c1"># Plotting library</span>
<span class="c1"># Allow matplotlib to plot inside this notebook</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1"># Set the seed of the numpy random number generator so that the tutorial is reproducable</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">cross_validation</span><span class="p">,</span> <span class="n">metrics</span> <span class="c1"># data and evaluation utils</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">colorConverter</span><span class="p">,</span> <span class="n">ListedColormap</span> <span class="c1"># some plotting functions</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Handwritten-digits-dataset">
    Handwritten digits dataset
    <a class="anchor-link" href="#Handwritten-digits-dataset">
     ¶
    </a>
   </h2>
   <p>
    The dataset used in this tutorial is the
    <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html">
     digits dataset
    </a>
    provided by
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">
     scikit-learn
    </a>
    . This dataset consists of 1797 8x8 images of handwritten digits between 0 and 9. Each 8x8 pixel image is provided as a flattened input vector of 64 variables. Example images for each digit are shown below. Note that this dataset is different from the larger and more famous
    <a href="https://en.wikipedia.org/wiki/MNIST_database">
     MNIST
    </a>
    dataset. The smaller dataset from scikit-learn was chosen to minimize training time for this tutorial. Feel free to experiment and adapt this tutorial to classify the MNIST digits.
   </p>
   <p>
    The dataset will be split into:
   </p>
   <ul>
    <li>
     A training set used to train the model. (inputs:
     <code>
      X_train
     </code>
     , targets:
     <code>
      T_train
     </code>
     )
    </li>
    <li>
     A validation set used to validate the model performance and to stop training if the model starts
     <a href="https://en.wikipedia.org/wiki/Overfitting">
      overfitting
     </a>
     on the training data. (inputs:
     <code>
      X_validation
     </code>
     , targets:
     <code>
      T_validation
     </code>
     )
    </li>
    <li>
     A final
     <a href="https://en.wikipedia.org/wiki/Test_set">
      test set
     </a>
     to evaluate the trained model on data is independent of the training and validation data. (inputs:
     <code>
      X_test
     </code>
     , targets:
     <code>
      T_test
     </code>
     )
    </li>
   </ul>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [2]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># load the data from scikit-learn.</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Load the targets.</span>
<span class="c1"># Note that the targets are stored as digits, these need to be </span>
<span class="c1">#  converted to one-hot-encoding for the output sofmax layer.</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">10</span><span class="p">))</span>
<span class="n">T</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">T</span><span class="p">)),</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Divide the data into a train and test set.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">T_train</span><span class="p">,</span> <span class="n">T_test</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="c1"># Divide the test set into a validation set and final test set.</span>
<span class="n">X_validation</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">T_validation</span><span class="p">,</span> <span class="n">T_test</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_test</span><span class="p">,</span> <span class="n">T_test</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [3]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Plot an example of each image.</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_png output_subarea ">
     <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkUAAABNCAYAAACoshCFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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">
     </img>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Generalization-of-the-layers">
    Generalization of the layers
    <a class="anchor-link" href="#Generalization-of-the-layers">
     ¶
    </a>
   </h2>
   <p>
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/">
     Part 4
    </a>
    of this tutorial series took on the classical view of neural networks where each layer consists of a
    <a href="https://en.wikipedia.org/wiki/Linear_map">
     linear transformation
    </a>
    by a
    <a href="https://en.wikipedia.org/wiki/Transformation_matrix">
     matrix multiplication
    </a>
    and vector addition followed by a non-linear function.
    <br>
     Here the linear transformation is split from the non-linear function and each is abstracted into its own layer. This has the benefit that the forward and backward step of each layer can easily be calculated separately.
    </br>
   </p>
   <p>
    This tutorial defines three example layers as
    <a href="https://docs.python.org/2/tutorial/classes.html">
     Python classes
    </a>
    :
   </p>
   <ul>
    <li>
     A layer to apply the linear transformation (
     <code>
      LinearLayer
     </code>
     ).
    </li>
    <li>
     A layer to apply the logistic function (
     <code>
      LogisticLayer
     </code>
     ).
    </li>
    <li>
     A layer to compute the softmax classification probabilities at the output (
     <code>
      SoftmaxOutputLayer
     </code>
     ).
    </li>
   </ul>
   <p>
    Each layer can compute its output in the forward step with
    <code>
     get_output
    </code>
    , which can then be used as the input for the next layer. The gradient at the input of each layer in the backpropagation step can be computed with
    <code>
     get_input_grad
    </code>
    . This function computes the gradient with the help of the targets if it's the last layer, or the gradients at its outputs (gradients at input of next layer) if it's an intermediate layer.
Each layer has the option to
    <a href="https://docs.python.org/2/library/stdtypes.html#iterator-types">
     iterate
    </a>
    over the parameters (if any) with
    <code>
     get_params_iter
    </code>
    , and get the gradients of these parameters in the same order with
    <code>
     get_params_grad
    </code>
    .
   </p>
   <p>
    Notice that the gradient and cost computed by the softmax layer are divided by the number of input samples. This is to make this gradient and cost independent of the number of input samples so that the size of the mini-batches can be changed without affecting other parameters. More on this later.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [4]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the non-linear functions used</span>
<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">logistic_deriv</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>  <span class="c1"># Derivative of logistic function</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [5]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the layers used in this model</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Base class for the different layers.</span>
<span class="sd">    Defines base methods and documentation of methods."""</span>
    
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters (if any).</span>
<span class="sd">        The iterator has the same order as get_params_grad.</span>
<span class="sd">        The elements returned by the iterator are editable in-place."""</span>
        <span class="k">return</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">get_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return a list of gradients over the parameters.</span>
<span class="sd">        The list has the same order as the get_params_iter iterator.</span>
<span class="sd">        X is the input.</span>
<span class="sd">        output_grad is the gradient at the output of this layer.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step linear transformation.</span>
<span class="sd">        X is the input."""</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer.</span>
<span class="sd">        Y is the pre-computed output of this layer (not needed in this case).</span>
<span class="sd">        output_grad is the gradient at the output of this layer </span>
<span class="sd">         (gradient at input of next layer).</span>
<span class="sd">        Output layer uses targets T to compute the gradient based on the </span>
<span class="sd">         output error instead of output_grad"""</span>
        <span class="k">pass</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [6]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The linear layer performs a linear transformation to its input."""</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
        <span class="sd">"""Initialize hidden layer parameters.</span>
<span class="sd">        n_in is the number of input variables.</span>
<span class="sd">        n_out is the number of output variables."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_out</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters."""</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
                               <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]))</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step linear transformation."""</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        
    <span class="k">def</span> <span class="nf">get_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return a list of gradients over the parameters."""</span>
        <span class="n">JW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">output_grad</span><span class="p">)</span>
        <span class="n">Jb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">JW</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">Jb</span><span class="p">))]</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="n">output_grad</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [7]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="k">class</span> <span class="nc">LogisticLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The logistic layer applies the logistic function to its inputs."""</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">logistic_deriv</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">output_grad</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [8]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="k">class</span> <span class="nc">SoftmaxOutputLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The softmax output layer computes the classification propabilities at the output."""</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the cost at the output of this output layer."""</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Sample-model">
    Sample model
    <a class="anchor-link" href="#Sample-model">
     ¶
    </a>
   </h3>
   <p>
    The following sections will refer to a layer as a layer defined above, and hidden-layer or output-layer as the classical neural network view of a linear transformation followed by a non-linear function.
   </p>
   <p>
    The sample model used to classify the handwritten digits in this tutorial consists of two hidden-layers with logistic functions and a softmax output-layer. The fist hidden-layer takes a vector of 64 pixel values and transforms them to a vector of 20 values. The second hidden-layer projects the previous 20 values to 20 new values. The output-layer outputs probabilities for the 10 possible classes. This architecture is illustrated in the following figure (biases are not shown to keep figure clean).
   </p>
   <p>
    <img alt="Image of the sample neural network model" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/neural_net_implementation/img/SimpleANN05.png"/>
   </p>
   <p>
    The full network is represented as a sequential list where each next layer is added on top of the previous layer by putting it in the next position in the list. The first layer is at position 0 in this list, the last layer is at the last index of this list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [9]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define a sample model to be trained on the data</span>
<span class="n">hidden_neurons_1</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of neurons in the first hidden-layer</span>
<span class="n">hidden_neurons_2</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of neurons in the second hidden-layer</span>
<span class="c1"># Create the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Define a list of layers</span>
<span class="c1"># Add first hidden layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_neurons_1</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogisticLayer</span><span class="p">())</span>
<span class="c1"># Add second hidden layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_neurons_1</span><span class="p">,</span> <span class="n">hidden_neurons_2</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogisticLayer</span><span class="p">())</span>
<span class="c1"># Add output layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_neurons_2</span><span class="p">,</span> <span class="n">T_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SoftmaxOutputLayer</span><span class="p">())</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Backpropagation">
    Backpropagation
    <a class="anchor-link" href="#Backpropagation">
     ¶
    </a>
   </h2>
   <p>
    The details of how backpropagation works in the forward and backward step of vectorized model are explained in
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/">
     part 4
    </a>
    of this tutorial. This section will only illustrate how to perform backpropagation over any number of layers with the generalized model described here.
   </p>
   <h3 id="Forward-step">
    Forward step
    <a class="anchor-link" href="#Forward-step">
     ¶
    </a>
   </h3>
   <p>
    The forward steps are computed by the
    <code>
     forward_step
    </code>
    method defined below. This method iteratively computes the outputs of each layer and feeds it as input to the next layer until the last layer. Each layer's output is computed by calling the
    <code>
     get_output
    </code>
    method. These output activations are stored in the
    <code>
     activations
    </code>
    list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [10]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the forward propagation step as a method.</span>
<span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="n">input_samples</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute and return the forward activation of each layer in layers.</span>
<span class="sd">    Input:</span>
<span class="sd">        input_samples: A matrix of input samples (each row is an input vector)</span>
<span class="sd">        layers: A list of Layers</span>
<span class="sd">    Output:</span>
<span class="sd">        A list of activations where the activation at each index i+1 corresponds to</span>
<span class="sd">        the activation of layer i in layers. activations[0] contains the input samples.  </span>
<span class="sd">    """</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_samples</span><span class="p">]</span> <span class="c1"># List of layer activations</span>
    <span class="c1"># Compute the forward activations for each layer starting from the first</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">input_samples</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Get the output of the current layer</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>  <span class="c1"># Store the output for future processing</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Set the current input as the activations of the previous layer</span>
    <span class="k">return</span> <span class="n">activations</span>  <span class="c1"># Return the activations of each layer</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Backward-step">
    Backward step
    <a class="anchor-link" href="#Backward-step">
     ¶
    </a>
   </h3>
   <p>
    The gradients computed in the backward step are computed by the
    <code>
     backward_step
    </code>
    method defined below. The backward step goes over all the layers in the reversed order. It first gets the initial gradients from the output layer and uses these gradients to compute the gradients of the layers below by iteratively calling the
    <code>
     get_input_grad
    </code>
    method. During each step, it computes the gradients of the cost with respect to the parameters by calling the
    <code>
     get_params_grad
    </code>
    method and returns them in a list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [11]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define the backward propagation step as a method</span>
<span class="k">def</span> <span class="nf">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Perform the backpropagation step over all the layers and return the parameter gradients.</span>
<span class="sd">    Input:</span>
<span class="sd">        activations: A list of forward step activations where the activation at </span>
<span class="sd">            each index i+1 corresponds to the activation of layer i in layers. </span>
<span class="sd">            activations[0] contains the input samples. </span>
<span class="sd">        targets: The output targets of the output layer.</span>
<span class="sd">        layers: A list of Layers corresponding that generated the outputs in activations.</span>
<span class="sd">    Output:</span>
<span class="sd">        A list of parameter gradients where the gradients at each index corresponds to</span>
<span class="sd">        the parameters gradients of the layer at the same index in layers. </span>
<span class="sd">    """</span>
    <span class="n">param_grads</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>  <span class="c1"># List of parameter gradients for each layer</span>
    <span class="n">output_grad</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># The error gradient at the output of the current layer</span>
    <span class="c1"># Propagate the error backwards through all the layers.</span>
    <span class="c1">#  Use reversed to iterate backwards over the list of layers.</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>   
        <span class="n">Y</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Get the activations of the last layer on the stack</span>
        <span class="c1"># Compute the error at the output layer.</span>
        <span class="c1"># The output layer error is calculated different then hidden layer error.</span>
        <span class="k">if</span> <span class="n">output_grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">input_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_input_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># output_grad is not None (layer is not output layer)</span>
            <span class="n">input_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_input_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="c1"># Get the input of this layer (activations of the previous layer)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Compute the layer parameter gradients used to update the parameters</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_params_grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="n">param_grads</span><span class="o">.</span><span class="n">appendleft</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="c1"># Compute gradient at output of previous layer (input of current layer):</span>
        <span class="n">output_grad</span> <span class="o">=</span> <span class="n">input_grad</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">param_grads</span><span class="p">)</span>  <span class="c1"># Return the parameter gradients</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Gradient-Checking">
    Gradient Checking
    <a class="anchor-link" href="#Gradient-Checking">
     ¶
    </a>
   </h2>
   <p>
    As in
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/#Gradient-checking">
     part 4
    </a>
    of this tutorial the gradient computed by backpropagation is compared with the
    <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">
     numerical gradient
    </a>
    to assert that there are no bugs in the code to compute the gradients.
   </p>
   <p>
    The code below gets the parameters of each layer by the help of the
    <code>
     get_params_iter
    </code>
    method that returns an iterator over all the parameters in the layer. The order of parameters returned corresponds to the order of parameter gradients returned by
    <code>
     get_params_grad
    </code>
    during backpropagation.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [12]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Perform gradient checking</span>
<span class="n">nb_samples_gradientcheck</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Test the gradients on a subset of the data</span>
<span class="n">X_temp</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nb_samples_gradientcheck</span><span class="p">,:]</span>
<span class="n">T_temp</span> <span class="o">=</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nb_samples_gradientcheck</span><span class="p">,:]</span>
<span class="c1"># Get the parameter gradients with backpropagation</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
<span class="n">param_grads</span> <span class="o">=</span> <span class="n">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">T_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>

<span class="c1"># Set the small change to compute the numerical gradient</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="c1"># Compute the numerical gradients of the parameters in all layers.</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">layer_backprop_grads</span> <span class="o">=</span> <span class="n">param_grads</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="c1"># Compute the numerical gradient for each parameter in the layer</span>
    <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
        <span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">layer_backprop_grads</span><span class="p">[</span><span class="n">p_idx</span><span class="p">]</span>
        <span class="c1"># + eps</span>
        <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="n">plus_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_temp</span><span class="p">)</span>
        <span class="c1"># - eps</span>
        <span class="n">param</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span>
        <span class="n">min_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_temp</span><span class="p">)</span>
        <span class="c1"># reset param value</span>
        <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="c1"># calculate numerical gradient</span>
        <span class="n">grad_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">plus_cost</span> <span class="o">-</span> <span class="n">min_cost</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
        <span class="c1"># Raise error if the numerical grade is not close to the backprop gradient</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grad_backprop</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">grad_num</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">grad_backprop</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'No gradient errors found'</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>No gradient errors found
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Stochastic-gradient-descent-backpropagation">
    Stochastic gradient descent backpropagation
    <a class="anchor-link" href="#Stochastic-gradient-descent-backpropagation">
     ¶
    </a>
   </h2>
   <p>
    This tutorial uses a variant of gradient descent called
    <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">
     Stochastic gradient descent
    </a>
    (SGD) to optimize the cost function. SGD follows the negative gradient of the cost function on subsets of the total training set. This has a few benefits: One of them is that the training time on large datasets can be reduced because the matrix of samples is much smaller during each sub-iteration and the gradients can be computed faster and with less memory. Another benefit is that computing the cost function on subsets results in noise, i.e. each subset will give a different cost depending on the samples. This will result in noisy (stochastic) gradient updates which might be able to push the gradient descent out of local minima. These, and other, benefits contribute to the popularity of SGD on training large scale machine learning methods such as neural networks.
   </p>
   <p>
    The cost function needs to be independent of the number of input samples because the size of the subsets used during SGD can vary. This is why the
    <a href="https://en.wikipedia.org/wiki/Mean_squared_error">
     mean squared error
    </a>
    (MSE) cost function is used instead of just the squared error. Using the mean instead of the sum is reflected in the gradient and cost computed by the softmax layer being divided by the number of input samples.
   </p>
   <h3 id="Minibatches">
    Minibatches
    <a class="anchor-link" href="#Minibatches">
     ¶
    </a>
   </h3>
   <p>
    The subsets of the training set are often called mini-batches. The following code will divide the training set into mini-batches of around 25 samples per batch. The inputs and targets are combined together in a list of (input, target) tuples.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [13]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Create the minibatches</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># Approximately 25 samples per batch</span>
<span class="n">nb_of_batches</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span>  <span class="c1"># Number of batches</span>
<span class="c1"># Create batches (X,Y) from the training set</span>
<span class="n">XT_batches</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">nb_of_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># X samples</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">T_train</span><span class="p">,</span> <span class="n">nb_of_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># Y targets</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="SGD-updates">
    SGD updates
    <a class="anchor-link" href="#SGD-updates">
     ¶
    </a>
   </h3>
   <p>
    The parameters $\mathbf{\theta}$ of the network are updated by the
    <code>
     update_params
    </code>
    method that iterates over each parameter of each layer and applies the simple
    <a href="https://en.wikipedia.org/wiki/Gradient_descent">
     gradient descent
    </a>
    rule on each mini-batch: $\mathbf{\theta}(k+1) = \mathbf{\theta}(k) - \Delta \mathbf{\theta}(k+1)$. $\Delta \mathbf{\theta}$ is defined as: $\Delta \mathbf{\theta} = \mu \frac{\partial \xi}{\partial \mathbf{\theta}}$ with $\mu$ the learning rate.
   </p>
   <p>
    The update steps will be performed for a number of iterations (
    <code>
     nb_of_iterations
    </code>
    ) over the full training set, where each full iterations consists of multiple updates over the mini-batches. After each full iteration, the resulting network will be tested on the validation set. The training will stop if the cost on the validation set doesn't increase after three full iterations to prevent overfitting or after maximum 300 iterations. All costs will be stored in between for future analysis.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [14]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Define a method to update the parameters</span>
<span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Function to update the parameters of the given layers with the given gradients</span>
<span class="sd">    by gradient descent with the given learning rate.</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_backprop_grads</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">izip</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">(),</span> <span class="n">layer_backprop_grads</span><span class="p">):</span>
            <span class="c1"># The parameter returned by the iterator point to the memory space of</span>
            <span class="c1">#  the original layer and can thus be modified inplace.</span>
            <span class="n">param</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>  <span class="c1"># Update each parameter</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [15]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Perform backpropagation</span>
<span class="c1"># initalize some lists to store the cost for future analysis        </span>
<span class="n">minibatch_costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">training_costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">validation_costs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">max_nb_of_iterations</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># Train for a maximum of 300 iterations</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Gradient descent learning rate</span>

<span class="c1"># Train for the maximum number of iterations</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_nb_of_iterations</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">XT_batches</span><span class="p">:</span>  <span class="c1"># For each minibatch sub-iteration</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>  <span class="c1"># Get the activations</span>
        <span class="n">minibatch_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>  <span class="c1"># Get cost</span>
        <span class="n">minibatch_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">minibatch_cost</span><span class="p">)</span>
        <span class="n">param_grads</span> <span class="o">=</span> <span class="n">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>  <span class="c1"># Get the gradients</span>
        <span class="n">update_params</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># Update the parameters</span>
    <span class="c1"># Get full training cost for future analysis (plots)</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
    <span class="n">train_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_train</span><span class="p">)</span>
    <span class="n">training_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_cost</span><span class="p">)</span>
    <span class="c1"># Get full validation cost</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
    <span class="n">validation_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_validation</span><span class="p">)</span>
    <span class="n">validation_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">validation_cost</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_costs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Stop training if the cost on the validation set doesn't decrease</span>
        <span class="c1">#  for 3 iterations</span>
        <span class="k">if</span> <span class="n">validation_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">validation_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">validation_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]:</span>
            <span class="k">break</span>
    
<span class="n">nb_of_iterations</span> <span class="o">=</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># The number of iterations that have been executed</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    The costs stored during training can be plotted to visualize the performance during training. The resulting plot is shown in the next figure. The cost on the training samples and validation samples goes down very quickly and flattens out after about 40 iterations on the full training set. Notice that the cost on the training set is lower than the cost on the validation set, this is because the network is optimized on the training set and is slightly overfitting. The training stops after around 90 iterations because the validation cost stops decreasing.
    <br>
     Also, notice that the cost of the mini-batches fluctuates around the cost of the full training set. This is the stochastic effect of mini-batches in SGD.
    </br>
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [16]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Plot the minibatch, full training set, and validation costs</span>
<span class="n">minibatch_x_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_iterations</span><span class="o">*</span><span class="n">nb_of_batches</span><span class="p">)</span>
<span class="n">iteration_x_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_iterations</span><span class="p">)</span>
<span class="c1"># Plot the cost over the iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">minibatch_x_inds</span><span class="p">,</span> <span class="n">minibatch_costs</span><span class="p">,</span> <span class="s1">'k-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost minibatches'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iteration_x_inds</span><span class="p">,</span> <span class="n">training_costs</span><span class="p">,</span> <span class="s1">'r-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost full training set'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iteration_x_inds</span><span class="p">,</span> <span class="n">validation_costs</span><span class="p">,</span> <span class="s1">'b-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost validation set'</span><span class="p">)</span>
<span class="c1"># Add labels to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'$</span><span class="se">\\</span><span class="s1">xi$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decrease of cost over backprop iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="n">y2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">nb_of_iterations</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_png output_subarea ">
     <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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">
     </img>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Performance-on-the-test-set">
    Performance on the test set
    <a class="anchor-link" href="#Performance-on-the-test-set">
     ¶
    </a>
   </h2>
   <p>
    Finally, the
    <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">
     accuracy
    </a>
    on the independent test set is computed to measure the performance of the model. The
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">
     scikit-learn
    </a>
    <code>
     accuracy_score
    </code>
    method is used to compute this accuracy from the predictions made by the model. The final accuracy on the test set is $96\%$ as shown below.
   </p>
   <p>
    The results can be analyzed in more detail with the help of a
    <a href="https://en.wikipedia.org/wiki/Confusion_matrix">
     confusion table
    </a>
    . This table shows how many samples of which class are classified as one of the possible classes. The confusion table is shown in the figure below. It is computed by the
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">
     scikit-learn
    </a>
    <code>
     confusion_matrix
    </code>
    method.
    <br>
     Notice that the digit '8' is misclassified five times, two times as '2', two times as '5', and one time as '9'.
    </br>
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [17]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Get results of test data</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">T_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Get the target outputs</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>  <span class="c1"># Get activation of test samples</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Get the predictions made by the network</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  <span class="c1"># Test set accuracy</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'The accuracy on the test set is {:.2f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">))</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>The accuracy on the test set is 0.96
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [18]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython2">
     <pre><span></span><span class="c1"># Show confusion table</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># Get confustion matrix</span>
<span class="c1"># Plot the confusion table</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'${:d}$'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>  <span class="c1"># Digit class names</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="c1"># Show class labels on each axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_top</span><span class="p">()</span>
<span class="n">major_ticks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">minor_ticks</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="c1"># Set plot labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s2">"right"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Predicted label'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True label'</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">'Confusion table'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="c1"># Show a grid to seperate digits</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">b</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sa">u</span><span class="s1">'minor'</span><span class="p">)</span>
<span class="c1"># Color each grid cell according to the number classes predicted</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span>
<span class="c1"># Show the number of samples in each cell</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">conf_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">conf_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">'w'</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">else</span> <span class="s1">'k'</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">conf_matrix</span><span class="p">[</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>       
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_png output_subarea ">
     <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQkAAAEiCAYAAADqAyeRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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">
     </img>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This post at
    <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/">
     peterroelants.github.io
    </a>
    is generated from an IPython notebook file.
    <a href="https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/neural_net_implementation/neural_network_implementation_part05.ipynb">
     Link to the full IPython notebook file
    </a>
   </p>
  </div>
 </div>
</div>

