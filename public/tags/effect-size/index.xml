<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>effect size | James E. Pustejovsky</title>
    <link>/tags/effect-size/</link>
      <atom:link href="/tags/effect-size/index.xml" rel="self" type="application/rss+xml" />
    <description>effect size</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020</copyright><lastBuildDate>Fri, 24 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>effect size</title>
      <link>/tags/effect-size/</link>
    </image>
    
    <item>
      <title>lmeInfo</title>
      <link>/software/lmeinfo/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/software/lmeinfo/</guid>
      <description>&lt;p&gt;lmeInfo provides analytic derivatives and information matrices for fitted linear mixed effects models and generalized least squares models estimated using &lt;code&gt;nlme::lme()&lt;/code&gt; and &lt;code&gt;nlme::gls()&lt;/code&gt;, respectively. The package includes functions for estimating the sampling variance-covariance of variance component parameters using the inverse Fisher information. The variance components include the parameters of the random effects structure (for lme models), the variance structure, and the correlation structure. The expected and average forms of the Fisher information matrix are used in the calculations, and models estimated by full maximum likelihood or restricted maximum likelihood are supported. The package also includes a function for estimating standardized mean difference effect sizes (
&lt;a href=&#34;/publication/design-comparable-effect-sizes/&#34;&gt;Pustejovsky et al., 2014&lt;/a&gt;) based on fitted lme or gls models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R package 
&lt;a href=&#34;https://cran.r-project.org/package=lmeInfo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/lmeInfo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R source code on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Simulating correlated standardized mean differences for meta-analysis</title>
      <link>/simulating-correlated-smds/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/simulating-correlated-smds/</guid>
      <description>


&lt;p&gt;As I’ve discussed in &lt;a href=&#34;/Sometimes-aggregating-effect-sizes-is-fine&#34;&gt;previous posts&lt;/a&gt;, meta-analyses in psychology, education, and other areas often include studies that contribute multiple, statistically dependent effect size estimates.
I’m interested in methods for meta-analyzing and meta-regressing effect sizes from data structures like this, and studying this sort of thing often entails conducting Monte Carlo simulations.
Monte Carlo simulations involve generating artificial data—in this case, a set of studies, each of which has one or more dependent effect size estimates—that follows a certain distributional model, applying different analytic methods to the artificial data, and then repeating the process a bunch of times.
Because we know the true parameters that govern the data-generating process, we can evaluate the performance of the analytic methods in terms of bias, accuracy, hypothesis test calibration and power, confidence interval coverage, and the like.&lt;/p&gt;
&lt;p&gt;In this post, I’ll discuss two alternative methods to simulate meta-analytic datasets that include studies with multiple, dependent effect size estimates: simulating individual participant-level data or simulating summary statistics. I’ll focus on the case of the standardized mean difference (SMD) because it is so common in meta-analyses of intervention studies. For simplicity, I’ll assume that the effect sizes all come from simple, two-group comparisons (without any covariate adjustment or anything like that) and that the individual observations are multi-variate normally distributed within each group. Our goal will be to simulate a set of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, where study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is based on measuring &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; outcomes on a sample of &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; participants, all for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k = (\delta_{1k} \cdots \delta_{J_k k})&amp;#39;\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of true standardized mean differences for study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
I’ll assume that we know these true effect size parameters for all &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, so that I can avoid committing to any particular form of random effects model.&lt;/p&gt;
&lt;div id=&#34;simulating-individual-participant-level-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating individual participant-level data&lt;/h1&gt;
&lt;p&gt;The most direct way to simulate this sort of effect size data is to generate outcome data for every artificial participant in every artificial study. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{ik}^T\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of outcomes for treatment group participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{ik}^C\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector outcomes for control group participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,...,N_k / 2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;. Assuming multi-variate normality of the outcomes, we can generate these outcome vectors as
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y}_{ik}^T \sim N\left(\boldsymbol\delta_k, \boldsymbol\Psi_k\right) \qquad \text{and}\qquad \mathbf{Y}_{ik}^C \sim N\left(\mathbf{0}, \boldsymbol\Psi_k\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Psi_k\)&lt;/span&gt; is the population correlation matrix of the outcomes in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
Note that I am setting the mean outcomes of the control group participants to zero and also specifying that the outcomes all have unit variance within each group.
After simulating data based on these distributions, the effect size estimates for each outcome can be calculated directly, following standard formulas.&lt;/p&gt;
&lt;p&gt;Here’s what this approach looks like in code.
It is helpful to simplify things by focusing on simulating just a single study with multiple, correlated effect sizes.
Focusing first on just the input parameters, a function might look like the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw &amp;lt;- function(delta, J, N, Psi) {
  # stuff
  return(ES_data)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above function skeleton, &lt;code&gt;delta&lt;/code&gt; would be the true effect size parameter &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k\)&lt;/span&gt;, &lt;code&gt;J&lt;/code&gt; would be the number of effect sizes to generate &lt;span class=&#34;math inline&#34;&gt;\((J_k)\)&lt;/span&gt;, &lt;code&gt;N&lt;/code&gt; is the total number of participants &lt;span class=&#34;math inline&#34;&gt;\((N_k)\)&lt;/span&gt;, and &lt;code&gt;Psi&lt;/code&gt; is a matrix of correlations between the outcomes &lt;span class=&#34;math inline&#34;&gt;\((\Psi_k)\)&lt;/span&gt;.
From these parameters, we’ll generate raw data, calculate effect size estimates and standard errors, and return the results in a little dataset.&lt;/p&gt;
&lt;p&gt;To make the function a little bit easier to use, I’m going overload the &lt;code&gt;Psi&lt;/code&gt; argument so that it can be a single number, indicating a common correlation between the outcomes. Thus, instead of having to feed in a &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; matrix, you can specify a single correlation &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt;, and the function will assume that all of the outcomes are equicorrelated. In code, the logic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!is.matrix(Psi)) Psi &amp;lt;- Psi + diag(1 - Psi, nrow = J)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the function with the innards:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw &amp;lt;- function(delta, J, N, Psi) {

  require(mvtnorm) # for simulating multi-variate normal data
  
  # create Psi matrix assuming equicorrelation
  if (!is.matrix(Psi)) Psi &amp;lt;- Psi + diag(1 - Psi, nrow = J)
  
  # generate control group summary statistics
  Y_C &amp;lt;- rmvnorm(n = N / 2, mean = rep(0, J), sigma = Psi)
  ybar_C &amp;lt;- colMeans(Y_C)
  sd_C &amp;lt;- apply(Y_C, 2, sd)
  
  # generate treatment group summary statistics
  delta &amp;lt;- rep(delta, length.out = J)
  Y_T &amp;lt;- rmvnorm(n = N / 2, mean = delta, sigma = Psi)
  ybar_T &amp;lt;- colMeans(Y_T)
  sd_T &amp;lt;- apply(Y_T, 2, sd)

  # calculate Cohen&amp;#39;s d
  sd_pool &amp;lt;- sqrt((sd_C^2 + sd_T^2) / 2)
  ES &amp;lt;- (ybar_T - ybar_C) / sd_pool
  
  # calculate SE of d
  SE &amp;lt;- sqrt(4 / N + ES^2 / (2 * (N - 2)))

  data.frame(ES = ES, SE = SE, N = N)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delta &amp;lt;- rnorm(4, mean = 0.2, sd = 0.1)
r_SMDs_raw(delta = delta, J = 4, N = 40, Psi = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            ES        SE  N
## 1  0.02795295 0.3162440 40
## 2 -0.37019812 0.3190662 40
## 3  0.15861202 0.3167507 40
## 4 -0.29438342 0.3180256 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or if you’d rather specify the full &lt;span class=&#34;math inline&#34;&gt;\(\Psi_k\)&lt;/span&gt; matrix yourself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Psi_k &amp;lt;- 0.6 + diag(0.4, nrow = 4)
Psi_k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]  1.0  0.6  0.6  0.6
## [2,]  0.6  1.0  0.6  0.6
## [3,]  0.6  0.6  1.0  0.6
## [4,]  0.6  0.6  0.6  1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw(delta = delta, J = 4, N = 40, Psi = Psi_k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            ES        SE  N
## 1  0.30796886 0.3181948 40
## 2  0.24528651 0.3174770 40
## 3  0.24496926 0.3174738 40
## 4 -0.03142033 0.3162483 40&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;The function above is serviceable but quite basic. I can think of several additional features that one might like to have for use in research simulations, but I’m feeling both cheeky and lazy at the moment, so I’ll leave them for you, dear reader. Here are some suggested exercises:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;Hedges_g = TRUE&lt;/code&gt;, which controls where the simulated effect size is Hedges’ &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; or Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. If it is Hedges’ g, make sure that the standard error is corrected too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;p_val = TRUE&lt;/code&gt;, which allows the user to control whether or not to return &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values from the test of mean differences for each outcome. Note that the p-values should be for a test of the &lt;em&gt;raw&lt;/em&gt; mean differences between groups, rather than a test of the effect size &lt;span class=&#34;math inline&#34;&gt;\(\delta_{jk} = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;corr_mat = FALSE&lt;/code&gt;, which controls whether the function returns just the simulated effect sizes and SEs or both the simulated effect sizes and the full sampling variance-covariance matrix of the effect sizes. See &lt;a href=&#34;/correlations-between-SMDs&#34;&gt;here&lt;/a&gt; for the relevant formulas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-summary-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating summary statistics&lt;/h1&gt;
&lt;p&gt;Another approach to simulating SMDs is to sample from the distribution of the &lt;em&gt;summary statistics&lt;/em&gt; used in calculating the effect size. This approach should simplify the code, at the cost of having to use a bit of distribution theory. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}_{Tk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}_{Ck}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vectors of sample means for the treatment and control groups, respectively. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_k\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; sample covariance matrix of the outcomes, pooled across the treatment and control groups. Again assuming multi-variate normality, and following the same notation as above:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{\bar{y}}_{Ck} \sim N\left(\mathbf{0}, \frac{2}{N_k} \boldsymbol\Psi_k\right), \qquad \mathbf{\bar{y}}_{Tk} \sim N\left(\boldsymbol\delta_k, \frac{2}{N_k} \boldsymbol\Psi_k\right),
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\left(\mathbf{\bar{y}}_{Tk} - \mathbf{\bar{y}}_{Ck}\right) \sim N\left(\boldsymbol\delta_k, \frac{4}{N_k} \boldsymbol\Psi_k\right).
\]&lt;/span&gt;
This shows how we could directly simulate the numerator of the standardized mean difference.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;/distribution-of-sample-variances&#34;&gt;further bit of distribution theory&lt;/a&gt; says that the pooled sample covariance matrix follows a multiple of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Wishart_distribution&#34;&gt;Wishart distribution&lt;/a&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_k - 2\)&lt;/span&gt; degrees of freedom and scale matrix &lt;span class=&#34;math inline&#34;&gt;\(\Psi_k\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
(N_k - 2) \mathbf{S}_k \sim Wishart\left(N_k - 2, \Psi_k \right).
\]&lt;/span&gt;
Thus, to simulate the denominators of the SMD estimates, we can simulate a single Wishart matrix, pull out the diagonal entries, divide by &lt;span class=&#34;math inline&#34;&gt;\(N_k - 2\)&lt;/span&gt;, and take the square root. In all, we draw a single &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; observation from a multi-variate normal distribution and a single &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; observation from a Wishart distribution. In contrast, the raw data approach requires simulating &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; observations from a multi-variate normal distribution, then calculating &lt;span class=&#34;math inline&#34;&gt;\(4 J_k\)&lt;/span&gt; summary statistics (M and SD for each group on each outcome).&lt;/p&gt;
&lt;div id=&#34;exercises-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Once again, I’ll leave it to you, dear reader, to do the fun programming bits:&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a modified version of the function &lt;code&gt;r_SMDs_raw&lt;/code&gt; that simulates summary statistics instead of raw data (Call it &lt;code&gt;r_SMDs_stats&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;microbenchmark&lt;/code&gt; package (or your preferred benchmarking tool) to compare the computational efficiency of both versions of the function.&lt;/li&gt;
&lt;li&gt;Check your work! Verify that both versions of the function generate the same distributions if the same parameters are used as input.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-approach-is-better&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which approach is better?&lt;/h1&gt;
&lt;p&gt;Like many things in research, there’s no clearly superior method here. The advantage of the summary statistics approach is computational efficiency. It should generally be faster than the raw data approach, and if you need to generate 10,000 meta-analysis each with 80 studies in them, the computational savings might add up. On the other hand, computational efficiency isn’t everything.&lt;/p&gt;
&lt;p&gt;I see two potential advantages of the raw data approach. First is interpretability: simulating raw data is likely easier to understand. It feels tangible and familiar, harkening back to those bygone days we spent learning ANOVA, whereas the summary statistics approach requires a bit of distribution theory to follow (bookmark this blog post!). Second is extensibility: it is relatively straightforward to extend the approach to use other distributional models for the raw dat (perhaps you want to look at outcomes that follow a &lt;a href=&#34;https://en.wikipedia.org/wiki/Multivariate_t-distribution&#34;&gt;multi-variate &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/a&gt;?) or more complicated estimators of the SMD (difference-in-differences? covariate-adjusted? cluster-randomized trial?). To use the summary statistics approach in more complicated scenarios, you’d have to work out the sampling distributions for yourself, or locate the right reference.&lt;/p&gt;
&lt;p&gt;Of course, there’s also no need to choose between these two approaches. As I’m trying to hint at in Exercise 6, it’s actually useful to write both. Then, you can use the (potentially slower) raw data version to verify that the summary statistics version is correct.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-full-meta-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating full meta-analyses&lt;/h1&gt;
&lt;p&gt;So far we’ve got a data-generating function that simulates a single study’s worth of effect size estimates. To study meta-analytic methods, we’ll need to build out the function to simulate multiple studies. To do so, I think it’s useful to use the technique of &lt;a href=&#34;https://r4ds.had.co.nz/iteration.html&#34;&gt;mapping&lt;/a&gt;, as implemented in the &lt;code&gt;purrr&lt;/code&gt; package’s &lt;code&gt;map_*&lt;/code&gt; functions. The idea here is to first generate a “menu” of study-specific parameters for each of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, then apply the &lt;code&gt;r_SMDs&lt;/code&gt; function to each parameter set.&lt;/p&gt;
&lt;p&gt;Let’s consider how to do this for a simple random effects model, where the true effect size parameter is constant within each study (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k = (\delta_k \cdots \delta_k)&amp;#39;\)&lt;/span&gt;), and in a model without covariates. We’ll need to generate a true effect for each study, along with a sample size, an outcome dimension, and a correlation between outcomes. For the true effects, I’ll assume that
&lt;span class=&#34;math display&#34;&gt;\[
\delta_k \sim N(\mu, \tau^2),
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
J_k \sim 2 + Poisson(3),
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
N_k \sim 20 + 2 \times Poisson(10),
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
r_k \sim Beta\left(\rho \nu, (1 - \rho)\nu\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho = \text{E}(r_k)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu &amp;gt; 0\)&lt;/span&gt; controls the variability of &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt; across studies, with smaller &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; corresponding to more variable correlations.
Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(r_k) = \rho (1 - \rho) / (1 + \nu)\)&lt;/span&gt;.
These distributions are just made up, without any particular justification.&lt;/p&gt;
&lt;p&gt;Here’s what these distributional models look like in R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K &amp;lt;- 6
mu &amp;lt;- 0.2
tau &amp;lt;- 0.05
J_mean &amp;lt;- 5
N_mean &amp;lt;- 45
rho &amp;lt;- 0.6
nu &amp;lt;- 39

study_data &amp;lt;- 
  data.frame(
    delta = rnorm(K, mean = mu, sd = tau),
    J = 2 + rpois(K, J_mean - 2),
    N = 20 + 2 * rpois(K, (N_mean - 20) / 2),
    Psi = rbeta(K, rho * nu, (1 - rho) * nu)
  )

study_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       delta J  N       Psi
## 1 0.2164399 7 34 0.5219260
## 2 0.1695641 4 54 0.5560842
## 3 0.2023963 5 50 0.5796910
## 4 0.1786634 3 42 0.5834660
## 5 0.2328779 3 36 0.4235426
## 6 0.1554537 6 48 0.5357454&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the “menu” of study-level characteristics, it’s just a matter of mapping the parameters to the data-generating function. One way to do this is with &lt;code&gt;pmap_df&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
meta_data &amp;lt;- pmap_df(study_data, r_SMDs_raw, .id = &amp;quot;study&amp;quot;)
meta_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    study          ES        SE  N
## 1      1  0.13424472 0.3434074 34
## 2      1  0.61637803 0.3515442 34
## 3      1  0.89283387 0.3606973 34
## 4      1  0.62649158 0.3518235 34
## 5      1  0.43596110 0.3472993 34
## 6      1  0.62143835 0.3516834 34
## 7      1  0.60034463 0.3511104 34
## 8      2 -0.01693615 0.2721706 54
## 9      2  0.10887688 0.2723748 54
## 10     2 -0.06254183 0.2722346 54
## 11     2  0.60622010 0.2785817 54
## 12     3  0.38220860 0.2855201 50
## 13     3  0.17922070 0.2834336 50
## 14     3  0.50984042 0.2875894 50
## 15     3  0.26371265 0.2841204 50
## 16     3  0.30662621 0.2845687 50
## 17     4  0.18850878 0.3093255 42
## 18     4  0.09359422 0.3087841 42
## 19     4  0.06258202 0.3086860 42
## 20     5  0.04903932 0.3333864 36
## 21     5  0.06668237 0.3334314 36
## 22     5  0.24701327 0.3346766 36
## 23     6  0.01605121 0.2886800 48
## 24     6  0.29727147 0.2903341 48
## 25     6  0.26241278 0.2899686 48
## 26     6  0.03451125 0.2886976 48
## 27     6  0.21825135 0.2895705 48
## 28     6 -0.01595245 0.2886799 48&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(meta_data$study)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 1 2 3 4 5 6 
## 7 4 5 3 3 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting it all together into a function, we have&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_meta &amp;lt;- function(K, mu, tau, J_mean, N_mean, rho, nu) {
  require(purrr)
  
  study_data &amp;lt;- 
    data.frame(
      delta = rnorm(K, mean = mu, sd = tau),
      J = 2 + rpois(K, J_mean - 2),
      N = 20 + 2 * rpois(K, (N_mean - 20) / 2),
      Psi = rbeta(K, rho * nu, (1 - rho) * nu)
    )
  
  pmap_df(study_data, r_SMDs_raw, .id = &amp;quot;study&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercises-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Modify &lt;code&gt;r_meta&lt;/code&gt; so that it uses &lt;code&gt;r_SMDs_stats&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add options to &lt;code&gt;r_meta&lt;/code&gt; for &lt;code&gt;Hedges_g&lt;/code&gt;, &lt;code&gt;p_val = TRUE&lt;/code&gt;, and &lt;code&gt;corr_mat = FALSE&lt;/code&gt; and ensure that these get passed along to the &lt;code&gt;r_SMDs&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One way to check that the &lt;code&gt;r_meta&lt;/code&gt; function is working properly is to generate a very large meta-analytic dataset, then to verify that the generated distributions align with expectations. Here’s a very large meta-analytic dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta_data &amp;lt;- 
  r_meta(100000, mu = 0.2, tau = 0.05, 
         J_mean = 5, N_mean = 40, 
         rho = 0.6, nu = 39)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the distribution of the simulated dataset against what you would expect to get based on the input parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the &lt;code&gt;r_meta&lt;/code&gt; function so that &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; are correlated, according to
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
J_k &amp;amp;\sim 2 + Poisson(\mu_J - 2) \\
N_k &amp;amp;\sim 20 + 2 \times Poisson\left(\frac{1}{2}(\mu_N - 20) + \alpha (J_k - \mu_J) \right)
\end{align}
\]&lt;/span&gt;
for user-specified values of &lt;span class=&#34;math inline&#34;&gt;\(\mu_J\)&lt;/span&gt; (the average number of outcomes per study), &lt;span class=&#34;math inline&#34;&gt;\(\mu_N\)&lt;/span&gt; (the average total sample size per study), and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, which controls the degree of dependence between &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-challenge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A challenge&lt;/h2&gt;
&lt;p&gt;The meta-analytic model that we’re using here is quite simple—simplistic, even—and for some simulation studies, something more complex might be needed. For example, we might need to generate data from a model that includes within-study random effects, as in:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{jk} = \mu + u_k + v_{jk}, \quad \text{where}\quad u_k \sim N(0, \tau^2), \quad v_{jk} \sim N(0, \omega^2).
\]&lt;/span&gt;
Even more complex would be to simulate from a multi-level meta-regression model
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{jk} = \mathbf{x}_{jk} \boldsymbol\beta + u_k + v_{jk}, \quad \text{where}\quad u_k \sim N(0, \tau^2), \quad v_{jk} \sim N(0, \omega^2),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{jk}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(1 \times p\)&lt;/span&gt; row-vector of covariates describing outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(p \times 1\)&lt;/span&gt; vector of meta-regression coefficients. In past work, I’ve done this by writing a data-generating function that takes a fixed design matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X} = \left(\mathbf{x}_{11}&amp;#39; \cdots \mathbf{x}_{J_K K}&amp;#39;\right)&amp;#39;\)&lt;/span&gt; as an input argument, along with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt;. The design matrix would also include an identifier for each unique study. There are surely better (simpler, easier to follow) ways to implement the multi-level meta-regression model. I’ll once again leave it to you to work out an approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sometimes, aggregating effect sizes is fine</title>
      <link>/sometimes-aggregating-effect-sizes-is-fine/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/sometimes-aggregating-effect-sizes-is-fine/</guid>
      <description>


&lt;p&gt;In meta-analyses of psychology, education, and other social science research, it is very common that some of the included studies report more than one relevant effect size.
For example, in a meta-analysis of intervention effects on reading outcomes, some studies may have used multiple measures of reading outcomes (each of which meets inclusion criteria), or may have measured outcomes at multiple follow-up times; some studies might have also investigated more than one version of an intervention, and it might be of interest to include effect sizes comparing each version to the no-intervention control condition;
and it’s even possible that some studies may have &lt;em&gt;all&lt;/em&gt; of these features, potentially contributing &lt;em&gt;lots&lt;/em&gt; of effect size estimates.&lt;/p&gt;
&lt;p&gt;These situations create a technical challenge for conducting a meta-analysis.
Because effect size estimates from the same study are correlated, it’s not usually reasonable to use methods that are premised on each effect size estimate being independent (i.e., univariate methods).
Instead, the analyst needs to apply methods that take into account the dependencies among estimates coming from the same study.
It used to be common to use ad hoc approaches for handling dependence, such as averaging the estimates together or selecting one estimate per study and then using univariate methods &lt;span class=&#34;citation&#34;&gt;(cf. Becker &lt;a href=&#34;#ref-Becker2000multivariate&#34; role=&#34;doc-biblioref&#34;&gt;2000&lt;/a&gt;)&lt;/span&gt;.
More sophisticated, multivariate meta-analysis (MVMA) models that directly account for correlations among the effect size estimates had been developed &lt;span class=&#34;citation&#34;&gt;(Kalaian and Raudenbush &lt;a href=&#34;#ref-Kalaian1996multivariate&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; but were challenging to implement and so rarely used (at least, that’s my impression).
More recently, techniques such as multi-level meta-analysis &lt;span class=&#34;citation&#34;&gt;(MLMA; Van den Noortgate et al. &lt;a href=&#34;#ref-VandenNoortgate2013threelevel&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;, &lt;a href=&#34;#ref-VandenNoortgate2015metaanalysis&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; and robust variance estimation &lt;span class=&#34;citation&#34;&gt;(RVE; Hedges, Tipton, and Johnson &lt;a href=&#34;#ref-Hedges2010robust&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; have emerged, which account for dependencies while using all available effect size estimates and still being feasible to implement.
These new techniques of MLMA and RVE are starting to be more widely adopted in practice, and it is not implausible that they will become the standard approach in psychological and educational meta-analysis within a few years.&lt;/p&gt;
&lt;p&gt;Given the extent of interest in MLMA and RVE, one might wonder: are the older ad hoc approaches &lt;em&gt;ever&lt;/em&gt; reasonable or appropriate?
I think that some are, under certain circumstances.
In this post I’ll highlight one such circumstance, where aggregating effect size estimates is not only reasonable but leads to &lt;em&gt;exactly the same results&lt;/em&gt; as a multivariate model. This occurs when two conditions are met:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We are not interested in within-study heterogeneity of effects and&lt;/li&gt;
&lt;li&gt;Any predictors included in the model vary between studies but not within a given study (i.e., effect sizes from the same study all have the same values of the predictors).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In short, if all we care about is understanding between-study variation in effect sizes, then it is fine to aggregate them up to the study level.&lt;/p&gt;
&lt;div id=&#34;a-model-thats-okay-to-average&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A model that’s okay to average&lt;/h1&gt;
&lt;p&gt;To make this argument precise, let me lay out a model where it applies.
For full generality, I’ll consider a meta-regression model for a collection of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, where study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; contributes &lt;span class=&#34;math inline&#34;&gt;\(J_k \geq 1\)&lt;/span&gt; effect size estimates.
Let &lt;span class=&#34;math inline&#34;&gt;\(T_{jk}\)&lt;/span&gt; denote effect size estimate &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, with sampling variance &lt;span class=&#34;math inline&#34;&gt;\(S_{jk}^2\)&lt;/span&gt;.
Effect size estimates from study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; maybe be correlated at the sampling level, with correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho_{ijk}\)&lt;/span&gt; between effect size estimates &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; from study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
I will assume that the correlations are known, although in practice one might need to just take a guess about the degree of correlation, such as by assuming &lt;span class=&#34;math inline&#34;&gt;\(\rho_{ijk} = 0.7\)&lt;/span&gt; for all pairs of estimates from each included study.
Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_k\)&lt;/span&gt; be a row vector of predictor variables for study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
Note that the predictors do not have a subscript &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; because I’m assuming here that they are constant within a study.&lt;/p&gt;
&lt;p&gt;A multivariate meta-regression model for these data might be:
&lt;span class=&#34;math display&#34;&gt;\[
T_{jk} = \mathbf{x}_k \boldsymbol\beta + u_k + e_{jk},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt; is a between-study random effect with variance &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_{jk}\)&lt;/span&gt; is the sampling error for effect size &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; from study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, assumed to have known variance &lt;span class=&#34;math inline&#34;&gt;\(S_{jk}^2\)&lt;/span&gt;.
Errors from the same study are correlated, so &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(e_{ik}, e_{jk}) = \rho_{ijk} S_{ik} S_{jk}\)&lt;/span&gt;.
This is a commonly considered model for dependent effect size estimates.
In the paper that introduced RVE, &lt;span class=&#34;citation&#34;&gt;Hedges, Tipton, and Johnson (&lt;a href=&#34;#ref-Hedges2010robust&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; termed it the “correlated effects” model (implemented in &lt;code&gt;robumeta&lt;/code&gt; as &lt;code&gt;model = &#34;CORR&#34;&lt;/code&gt;, which is the default).
Note that it also satisfies the conditions I outlined above: no within-study random effects, predictors that vary only between study.
We can fit it using the &lt;code&gt;rma.mv()&lt;/code&gt; function in the &lt;code&gt;metafor&lt;/code&gt; package, as I will demonstrate below.&lt;/p&gt;
&lt;p&gt;An alternative to this multivariate model would be to first average the effects within each study, then fit a univariate random effects model.
Just how we do the averaging will matter: we’ll need to use inverse-variance weighting.
Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_k\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of effect size estimates from study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_k\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; sampling covariance matrix for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_k\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{1}_k\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of 1s. The inverse-variance weighted average of the effects from study k can then be written as
&lt;span class=&#34;math display&#34;&gt;\[
\bar{T}_k = V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{T}_k, 
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(V_k = 1 / (\mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{1}_k)\)&lt;/span&gt;. The quantity &lt;span class=&#34;math inline&#34;&gt;\(V_k\)&lt;/span&gt; is also the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(\bar{T}_k\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A conventional, univariate random effects model for the averaged effect sizes is
&lt;span class=&#34;math display&#34;&gt;\[
\bar{T}_k = \mathbf{x}_k \boldsymbol\beta + u_k + \bar{e}_k, 
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(u_k) = \tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\bar{e}_k) = V_k\)&lt;/span&gt;.
This model can be fit using &lt;code&gt;rma.uni&lt;/code&gt; from &lt;code&gt;metafor&lt;/code&gt;.
In fact, doing so will yield the same estimates of model parameters as fitting the multivariate model—for all intents and purposes, they are equivalent models.
There are at several different ways to see that this equivalence holds.
I’ll offer three, from most practical to most theoretical.
(If you’d rather just take my word that this claim is true, feel free to skip down to the &lt;a href=&#34;#so-what&#34;&gt;last section&lt;/a&gt;, where I comment on implications.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computational-equivalence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Computational equivalence&lt;/h1&gt;
&lt;p&gt;One good way to check the equivalence of the univariate and multivariate models is to apply both to a dataset. I’ll use the data from a stylized example described in &lt;span class=&#34;citation&#34;&gt;Tanner-Smith and Tipton (&lt;a href=&#34;#ref-TannerSmith2013robust&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, looking at the effects of alcohol abuse interventions on alcohol consumption among adolescents and young adults. (The data are simulated for teaching purposes, so don’t infer anything about real life from the results below!) The data are included in the &lt;code&gt;robumeta&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

data(corrdat, package = &amp;quot;robumeta&amp;quot;)

# sort by study
corrdat &amp;lt;- arrange(corrdat, studyid, esid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data consist of 172 effect sizes from 39 studies. Some studies report effects at multiple follow-up times and/or for multiple programs compared to a common control condition, leading to dependent effect size estimates.The data also include variables encoding a variety of sample and study characteristics, such as whether the study was conducted with a college student sample and the gender composition of the sample:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(corrdat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   esid studyid effectsize        var binge followup males college
## 1 4006       1  0.2086383 0.03246468     1 51.42857    67       0
## 2 4016       1  0.2244635 0.03244931     1 51.42857    67       0
## 3 4026       1  0.3151743 0.03278697     1 51.42857    67       0
## 4 3513       2  0.2220929 0.01972874     0 17.14286    81       1
## 5 3514       2 -0.1922628 0.02031393     0 17.14286    86       1
## 6 3556       2  0.3273109 0.01987042     0 17.14286    81       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose that we are interested in estimating the differences in average effects by type of sample (college versus adolescent), controlling for the proportion of males in the study. For some reason, there is within-study variation in the percentage of males, so I’ll take the study-level average for this covariate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrdat &amp;lt;-
  corrdat %&amp;gt;%
  group_by(studyid) %&amp;gt;%
  mutate(males = mean(males))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then fit this model using a multi-variate meta-regression in metafor.&lt;/p&gt;
&lt;p&gt;In order to estimate the model, we’ll first need to create a variance-covariance matrix for the effect size estimates in each study, which can be accomplished using &lt;code&gt;impute_covariance_matrix&lt;/code&gt; from &lt;code&gt;clubSandwich&lt;/code&gt; (&lt;a href=&#34;/imputing-covariance-matrices-for-multi-variate-meta-analysis/&#34;&gt;further details here&lt;/a&gt;). I’ll assume a correlation of 0.6 between pairs of effect sizes within a given study:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(clubSandwich)
library(metafor)

V_list &amp;lt;- impute_covariance_matrix(vi = corrdat$var, cluster = corrdat$studyid, r = 0.6)

MV_fit &amp;lt;- rma.mv(effectsize ~ college + males, V = V_list, 
                 random = ~ 1 | studyid,
                 data = corrdat, method = &amp;quot;REML&amp;quot;)
MV_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Multivariate Meta-Analysis Model (k = 172; method: REML)
## 
## Variance Components:
## 
##             estim    sqrt  nlvls  fixed   factor 
## sigma^2    0.0590  0.2429     39     no  studyid 
## 
## Test for Residual Heterogeneity:
## QE(df = 169) = 815.2448, p-val &amp;lt; .0001
## 
## Test of Moderators (coefficients 2:3):
## QM(df = 2) = 9.9016, p-val = 0.0071
## 
## Model Results:
## 
##          estimate      se     zval    pval    ci.lb    ci.ub 
## intrcpt    0.6466  0.2693   2.4007  0.0164   0.1187   1.1744   * 
## college    0.3703  0.1317   2.8123  0.0049   0.1122   0.6283  ** 
## males     -0.0076  0.0038  -1.9832  0.0473  -0.0152  -0.0001   * 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternately, we could aggregate the effects up to the study level and then fit a univariate meta-regression using the same moderators. Here is a function to calculate the aggregated effect size estimates and variances:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agg_effects &amp;lt;- function(yi, vi, r = 0.6) {
  corr_mat &amp;lt;- r + diag(1 - r, nrow = length(vi))
  sd_mat &amp;lt;- tcrossprod(sqrt(vi))
  V_inv_mat &amp;lt;- chol2inv(chol(sd_mat * corr_mat))
  V &amp;lt;- 1 / sum(V_inv_mat)
  data.frame(es = V * sum(yi * V_inv_mat), var = V)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the data-munging:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrdat_agg &amp;lt;-
  corrdat %&amp;gt;%
  group_by(studyid) %&amp;gt;%
  summarise(
    es = list(agg_effects(yi = effectsize, vi = var, r = 0.6)),
    males = mean(males),
    college = mean(college)
  ) %&amp;gt;%
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required.
## Please use `cols = c(es)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(corrdat_agg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   studyid      es    var males college
##     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1       1  0.249  0.0239  67         0
## 2       2 -0.0210 0.0129  81         1
## 3       3  0.726  0.0819  76.2       0
## 4       4  0.370  0.0431  80         1
## 5       5 -0.0911 0.0281  79         0
## 6       6 -0.416  0.0111  74         0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here’s the meta-regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;uni_fit &amp;lt;- rma.uni(es ~ college + males, vi = var, 
                   data = corrdat_agg, method = &amp;quot;REML&amp;quot;)
uni_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mixed-Effects Model (k = 39; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of residual heterogeneity):     0.0590 (SE = 0.0242)
## tau (square root of estimated tau^2 value):             0.2429
## I^2 (residual heterogeneity / unaccounted variability): 61.42%
## H^2 (unaccounted variability / sampling variability):   2.59
## R^2 (amount of heterogeneity accounted for):            19.12%
## 
## Test for Residual Heterogeneity:
## QE(df = 36) = 96.7794, p-val &amp;lt; .0001
## 
## Test of Moderators (coefficients 2:3):
## QM(df = 2) = 9.9016, p-val = 0.0071
## 
## Model Results:
## 
##          estimate      se     zval    pval    ci.lb    ci.ub 
## intrcpt    0.6466  0.2693   2.4007  0.0164   0.1187   1.1744   * 
## college    0.3703  0.1317   2.8123  0.0049   0.1122   0.6283  ** 
## males     -0.0076  0.0038  -1.9832  0.0473  -0.0152  -0.0001   * 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The heterogeneity estimates are nearly equal (the difference is due to using numerical optimization):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MV_fit$sigma2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0589972&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;uni_fit$tau2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05899673&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the meta-regression coefficient estimates are identical to six decimal places:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(MV_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      intrcpt      college        males 
##  0.646561371  0.370274721 -0.007633517&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(uni_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      intrcpt      college        males 
##  0.646561352  0.370274307 -0.007633519&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(coef(MV_fit), coef(uni_fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mean relative difference: 4.243578e-07&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this example we arrive at the same results using either multivariate meta-analysis or univariate meta-analysis of aggregated effect size estimates.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; The main limitation of this illustration is generality—how can we be sure that these results aren’t just a quirk of this particular dataset? Would we get the same results for &lt;em&gt;any&lt;/em&gt; dataset?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-multivariate-to-univariate-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;From multivariate to univariate model&lt;/h1&gt;
&lt;p&gt;Here’s another, somewhat more general perspective on the relationship between the models: the univariate model can be &lt;em&gt;derived&lt;/em&gt; directly from the multivariate one. Start with the multivariate model in matrix form:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{T}_k = \mathbf{x}_k \boldsymbol\beta \mathbf{1}_k + u_k \mathbf{1}_k + \mathbf{e}_k,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{e}_k\)&lt;/span&gt; is the vector of sampling errors for study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\mathbf{e}_k) = \mathbf{S}_k\)&lt;/span&gt;. Pre-multiply both sides by &lt;span class=&#34;math inline&#34;&gt;\(V_k \mathbf{1}_k’ \mathbf{S}_k^{-1}\)&lt;/span&gt; to get
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{T}_k &amp;amp;= V_k \left(\mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{1}_k\right) \mathbf{x}_k \boldsymbol\beta + u_k V_k \left(\mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{1}_k\right) + V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{e}_k \\
\bar{T}_k &amp;amp;= \mathbf{x}_k \boldsymbol\beta + u_k + \bar{e}_k,
\end{aligned}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\bar{e}_k) = V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{S}_k \mathbf{S}_k^{-1} \mathbf{1}_k V_k = V_k\)&lt;/span&gt;, just as in the univariate model.&lt;/p&gt;
&lt;p&gt;This demonstrates that the parameters of the two models are the same quantities—that is, both models are estimating the same thing. But that would also hold if we used &lt;em&gt;any&lt;/em&gt; weighted average of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_k\)&lt;/span&gt;—it needn’t be inverse-variance. The only thing that would be different is &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\bar{e}_k)\)&lt;/span&gt;. To fully establish the equivalence of the two models, I’ll examine the likelihoods of each model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;equivalence-of-likelihoods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Equivalence of likelihoods&lt;/h1&gt;
&lt;p&gt;Multivariate meta-analysis models are typically estimated by full maximum likelihood (FML) or restricted maximum likelihood methods. FML and RML are also commonly used for univariate meta-analysis. With these methods, estimates are obtained as the parameter values that maximize the log likelihood of the model, given the data (or the restricted likelihood for RML). Therefore, we can establish the exact equivalence of parameter estimates by showing that the log likelihood of the univariate and multivariate models differ by a constant value (so that the location of the maxima are identical).&lt;/p&gt;
&lt;div id=&#34;full-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full likelihood&lt;/h2&gt;
&lt;p&gt;For the univariate model, the log-likelihood contribution of study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
l^{U}_k\left(\boldsymbol\beta, \tau^2\right) = -\frac{1}{2} \log\left(\tau^2 + V_k\right) - \frac{1}{2} \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k}.
\]&lt;/span&gt;
For the multivariate model, the log-likelihood contribution of study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
l^{MV}_k\left(\boldsymbol\beta, \tau^2\right) = -\frac{1}{2} A -\frac{1}{2} B
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
A = \log\left|\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right| 
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
B = \left(\mathbf{T}_k - \mathbf{x}_k \boldsymbol\beta \mathbf{1}_k\right)&amp;#39; \left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1} \left(\mathbf{T}_k - \mathbf{x}_k \boldsymbol\beta \mathbf{1}_k\right).
\]&lt;/span&gt;
The term &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be rearranged as
&lt;span class=&#34;math display&#34;&gt;\[
A = \log\left|\left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1} + \mathbf{I}_k\right) \mathbf{S}_k\right|
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{I}_k\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; identity matrix. One of the properties of determinants is that the determinant of a product of two matrices is equal to the product of the determinants. Another is that, for two vectors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{v}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\left|\mathbf{I} + \mathbf{u}\mathbf{v}&amp;#39;\right| = 1 + \mathbf{v}&amp;#39;\mathbf{u}\)&lt;/span&gt;. Applying both of these properties, it follows that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A &amp;amp;= \log\left|\left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1} + \mathbf{I}_k\right) \mathbf{S}_k\right| \\
&amp;amp;= \log \left( \left|\mathbf{I}_k + \tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1}\right| \left|\mathbf{S}_k\right|\right) \\
&amp;amp;= \log \left(1 + \frac{\tau^2}{V_k}\right) + \log \left|\mathbf{S}_k\right| \\
&amp;amp;= \log(\tau^2 + V_k) - \log(V_k) + \log \left|\mathbf{S}_k\right|.
\end{aligned}
\]&lt;/span&gt;
The &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; term takes a little more work.
From &lt;a href=&#34;https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula&#34;&gt;the Sherman-Morrison identity&lt;/a&gt;, we have that:
&lt;span class=&#34;math display&#34; id=&#34;eq:Sherman&#34;&gt;\[
\left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1} = \mathbf{S}_k^{-1} - \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1},
\tag{1}
\]&lt;/span&gt;
by which it follows that
&lt;span class=&#34;math display&#34; id=&#34;eq:inversevariance&#34;&gt;\[
\mathbf{1}_k&amp;#39;\left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1}\mathbf{1}_k = \frac{1}{\tau^2 + V_k}.
\tag{2}
\]&lt;/span&gt;
Now, rearrange the &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; term to get
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B &amp;amp;= \left[\mathbf{T}_k - \bar{T}_k \mathbf{1}_k + \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \mathbf{1}_k\right]&amp;#39; \left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1} \left[\mathbf{T}_k - \bar{T}_k \mathbf{1}_k + \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \mathbf{1}_k\right] \\
&amp;amp;= B_1 + 2 B_2 + B_3
\end{aligned}
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B_1 &amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1} \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) \\
B_2 &amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1} \mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\
B_3 &amp;amp;= \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \mathbf{1}_k&amp;#39; \left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1} \mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)
\end{aligned}
\]&lt;/span&gt;
Applying &lt;a href=&#34;#eq:Sherman&#34;&gt;(1)&lt;/a&gt; to &lt;span class=&#34;math inline&#34;&gt;\(B_1\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B_1 &amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \left[\mathbf{S}_k^{-1} - \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1}\right] \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) \\ 
&amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) - \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) \\
&amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right).
\end{aligned}
\]&lt;/span&gt;
The second term drops out because &lt;span class=&#34;math inline&#34;&gt;\(\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1} \mathbf{1}_k = \bar{T}_k / V_k - \bar{T}_k / V_k = 0\)&lt;/span&gt;. Along similar lines,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B_2 &amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \left[\mathbf{S}_k^{-1} - \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1}\right] \mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\ 
&amp;amp;= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1}\mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) - \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k&amp;#39;\mathbf{S}_k^{-1}\mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\
&amp;amp;= 0.
\end{aligned}
\]&lt;/span&gt;
Finally, the third term simplifies using &lt;a href=&#34;#eq:inversevariance&#34;&gt;(2)&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
B_3 = \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k}.
\]&lt;/span&gt;
Thus, the full &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; term reduces to
&lt;span class=&#34;math display&#34;&gt;\[
B = \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) + \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k}
\]&lt;/span&gt;
and the multivariate log likelihood contribution is
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l^{MV}_k\left(\boldsymbol\beta, \tau^2\right) &amp;amp;= -\frac{1}{2} \log(\tau^2 + V_k) + \frac{1}{2} \log(V_k) - \frac{1}{2}\log \left|\mathbf{S}_k\right| - \frac{1}{2} \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) -\frac{1}{2} \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k} \\
&amp;amp;= l^U_k\left(\boldsymbol\beta, \tau^2\right) + \frac{1}{2} \log(V_k) - \frac{1}{2}\log \left|\mathbf{S}_k\right| - \frac{1}{2} \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)&amp;#39; \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right).
\end{aligned}
\]&lt;/span&gt;
The last three terms depend on the data (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_k\)&lt;/span&gt;) but not on the parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;. Therefore, the univariate and multivariate likelihoods will be maximized at the same parameter values, i.e., the FML estimators are identical.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;restricted-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Restricted likelihood&lt;/h2&gt;
&lt;p&gt;In practice, it is more common to use RML estimation rather than FML.
The RML estimators maximize a different objective function that includes the full likelihood, plus an additional term. The RML objective function for the univariate model is
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{k=1}^K l^U_k(\boldsymbol\beta, \tau^2) - \frac{1}{2} R^U(\tau^2)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
R^U(\tau^2) = \log \left|\sum_{k=1}^k\frac{\mathbf{x}_k&amp;#39; \mathbf{x}_k}{\tau^2 + V_k} \right|.
\]&lt;/span&gt;
For the multivariate model, the RML objective is
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{k=1}^K l^{MV}_k(\boldsymbol\beta, \tau^2) - \frac{1}{2} R^{MV}(\tau^2).
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
R^{MV}(\tau^2) &amp;amp;= \log \left|\sum_{k=1}^k \mathbf{x}_k&amp;#39;\mathbf{1}_k&amp;#39;\left(\tau^2\mathbf{1}_k\mathbf{1}_k&amp;#39; + \mathbf{S}_k\right)^{-1}\mathbf{1}_k \mathbf{x}_k \right|\\
&amp;amp;= \log \left|\sum_{k=1}^k\frac{\mathbf{x}_k&amp;#39; \mathbf{x}_k}{\tau^2 + V_k} \right| \\
&amp;amp;= R^U(\tau^2)
\end{aligned}
\]&lt;/span&gt;
because of &lt;a href=&#34;#eq:inversevariance&#34;&gt;(2)&lt;/a&gt;. Thus, the univariate and multivariate models also have the same RML estimators.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;so-what&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;So what?&lt;/h1&gt;
&lt;p&gt;Beyond being a good excuse to write a bunch of matrix algebra, why does any of this matter? I think there are two main implications. First, it is useful to recognize the equivalence of these models in order to understand when the multivariate model is &lt;em&gt;necessary&lt;/em&gt;. If both of the conditions that I’ve described hold, then it is entirely acceptable to use aggregation rather than the more complicated multivariate model.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Using the simpler univariate model might be desirable in practice because it makes the analysis easier to follow, because it makes it easier to run diagnostics or create illustrations of the results, or because of software limitations. Conversely, if either of the conditions does not hold, then there may be differences between the two approaches and the analyst will need to think carefully about which method better addresses their research questions.&lt;/p&gt;
&lt;p&gt;A second implication is computational: because it gives the same results, the univariate model could be used as a short-cut for fitting the multivariate model. Compare the differences in computational time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
microbenchmark(
  uni = rma.uni(es ~ college + males, vi = var, 
                data = corrdat_agg, method = &amp;quot;REML&amp;quot;),
  multi = rma.mv(effectsize ~ college + males, V = V_list, 
                 random = ~ 1 | studyid,
                 data = corrdat, method = &amp;quot;REML&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##   expr     min       lq      mean  median       uq      max neval
##    uni  8.6496  9.04635  9.658892  9.2733  9.71695  13.3316   100
##  multi 78.4050 82.76230 87.136962 84.4908 86.62210 186.1949   100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the aggregation is done in advance, it is &lt;em&gt;way&lt;/em&gt; quicker to fit the univariate model. The short-cut would be useful if we needed to estimate &lt;em&gt;lots&lt;/em&gt; of multi-variate meta-regressions (as long as the equivalence conditions hold). For example, if we needed to bootstrap the multivariate model, we could pre-compute the aggregated effects and then just bootstrap the much simpler, much quicker univariate model.&lt;/p&gt;
&lt;p&gt;I suspect that the results I’ve presented here can be further generalized, but this will need a bit of further investigation. For one, there are also equivalences between variance estimators: using the CR2 cluster-robust variance estimator for the multivariate model is equivalent to using the HC2 heteroskedasticity-robust variance estimator for the univariate model with aggregated effects.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;
For another, the same sort of equivalence relationships hold even if there are additional random effects in the model, so long as the random effects are at the study level or higher levels of aggregation (e.g., lab effects, where labs are nested within studies).
I’ll leave these generalizations as exercises for a future rainy day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Becker2000multivariate&#34;&gt;
&lt;p&gt;Becker, Betsy Jane. 2000. “Multivariate Meta-analysis.” In &lt;em&gt;Handbook of Applied Multivariate Statistics and Mathematical Modeling&lt;/em&gt;, edited by Steven D Brown and Howard E. A. Tinsley, 499–525. San Diego, CA: Academic Press. &lt;a href=&#34;https://doi.org/10.1016/B978-012691360-6/50018-5&#34;&gt;https://doi.org/10.1016/B978-012691360-6/50018-5&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-borenstein2009introduction&#34;&gt;
&lt;p&gt;Borenstein, Michael, Larry V. Hedges, Julian P T Higgins, and Hannah R Rothstein. 2009. &lt;em&gt;Introduction to Meta-Analysis&lt;/em&gt;. Chichester, UK: John Wiley &amp;amp; Sons, Ltd. &lt;a href=&#34;https://doi.org/10.1002/9780470743386&#34;&gt;https://doi.org/10.1002/9780470743386&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hedges2010robust&#34;&gt;
&lt;p&gt;Hedges, Larry V., Elizabeth Tipton, and Matthew C Johnson. 2010. “Robust variance estimation in meta-regression with dependent effect size estimates.” &lt;em&gt;Research Synthesis Methods&lt;/em&gt; 1 (1): 39–65. &lt;a href=&#34;https://doi.org/10.1002/jrsm.5&#34;&gt;https://doi.org/10.1002/jrsm.5&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Kalaian1996multivariate&#34;&gt;
&lt;p&gt;Kalaian, Hripsime a., and Stephen W Raudenbush. 1996. “A multivariate mixed linear model for meta-analysis.” &lt;em&gt;Psychological Methods&lt;/em&gt; 1 (3): 227–35. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.1.3.227&#34;&gt;https://doi.org/10.1037/1082-989X.1.3.227&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-TannerSmith2013robust&#34;&gt;
&lt;p&gt;Tanner-Smith, Emily E, and Elizabeth Tipton. 2013. “Robust variance estimation with dependent effect sizes: Practical considerations including a software tutorial in Stata and SPSS.” &lt;em&gt;Research Synthesis Methods&lt;/em&gt; 5 (1): 1–34. &lt;a href=&#34;https://doi.org/10.1002/jrsm.1091&#34;&gt;https://doi.org/10.1002/jrsm.1091&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-VandenNoortgate2013threelevel&#34;&gt;
&lt;p&gt;Van den Noortgate, Wim, José Antonio López-López, Fulgencio Marín-Martínez, and Julio Sánchez-Meca. 2013. “Three-level meta-analysis of dependent effect sizes.” &lt;em&gt;Behavior Research Methods&lt;/em&gt; 45 (2): 576–94. &lt;a href=&#34;https://doi.org/10.3758/s13428-012-0261-6&#34;&gt;https://doi.org/10.3758/s13428-012-0261-6&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-VandenNoortgate2015metaanalysis&#34;&gt;
&lt;p&gt;———. 2015. “Meta-analysis of multiple outcomes: a multilevel approach.” &lt;em&gt;Behavior Research Methods&lt;/em&gt; 47 (4): 1274–94. &lt;a href=&#34;https://doi.org/10.3758/s13428-014-0527-2&#34;&gt;https://doi.org/10.3758/s13428-014-0527-2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A common special case is that the sampling variances for effect sizes within a given study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; are &lt;em&gt;all equal&lt;/em&gt;, so that &lt;span class=&#34;math inline&#34;&gt;\(S_{ik} = s_{jk} = S_k\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i,j = 1,...,J_ik\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;. We might further posit that there is a constant sampling correlation between every pair of effect sizes within a given study, so that &lt;span class=&#34;math inline&#34;&gt;\(\rho_{ijk} = \rho_k\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i,j = 1,...,J_ik\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;. If both of these conditions hold, then the inverse-variance weighted average effect size simplifies to the arithmetic average
&lt;span class=&#34;math display&#34;&gt;\[
\bar{T}_k = \frac{1}{J_k} \sum_{j=1}^{J_k} T_{jk}
\]&lt;/span&gt;
with sampling variance
&lt;span class=&#34;math display&#34;&gt;\[
V_k = \frac{(J_k - 1)\rho_k + 1}{J} \times S_k^2
\]&lt;/span&gt;
&lt;span class=&#34;citation&#34;&gt;(cf. Borenstein et al. &lt;a href=&#34;#ref-borenstein2009introduction&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;, Eq. (24.6), p. 230)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The same thing holds if we use FML rather than RML estimation—try it for yourself and see!&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;As RVE and MLMA become more wide-spread, I could imagine it happening that a meta-analyst who uses aggregation and a univariate model might get push-back from a reviewer, who uncritically recommends using a “more advanced” method to handle dependence. The results in this post provide a way for the meta-analyst to establish that doing so would be unnecessary.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Here’s verification with the computational example from above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# multivariate CR2
coef_test(MV_fit, vcov = &amp;quot;CR2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Coef. Estimate      SE t-stat d.f. p-val (Satt) Sig.
## 1 intrcpt  0.64656 0.17647   3.66 11.5      0.00345   **
## 2 college  0.37027 0.18648   1.99 11.9      0.07053    .
## 3   males -0.00763 0.00287  -2.66 14.5      0.01826    *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# univariate HC2
coef_test(uni_fit, vcov = &amp;quot;CR2&amp;quot;, cluster = corrdat_agg$studyid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Coef. Estimate      SE t-stat d.f. p-val (Satt) Sig.
## 1 intrcpt  0.64656 0.17622   3.67 11.5      0.00342   **
## 2 college  0.37027 0.18597   1.99 11.9      0.06985    .
## 3   males -0.00763 0.00287  -2.66 14.5      0.01808    *&lt;/code&gt;&lt;/pre&gt;
&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Converting from odds ratios to standardized mean differences: What to do with logistic regression coefficients?</title>
      <link>/converting-odds-ratios-to-standardized-mean-differences/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      <guid>/converting-odds-ratios-to-standardized-mean-differences/</guid>
      <description>


&lt;p&gt;One of the central problems in research synthesis is that studies use a variety of different types of outcome measures to assess a construct. This is the main reason that meta-analysis often uses standardized, scale-free effect sizes (such as standardized mean differences), so that findings from studies that use different measures can be combined and contrasted on a common metric. In syntheses of education research (as well as other fields), a further issue that sometimes arises is that some included studies might report effects on a dichotomous outcome, while others report effects (of the same intervention, say) but using a continuous outcome measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit &amp;lt;- function(x) log(x) - log(1 - x)

simulate_OR_to_SMD &amp;lt;- function(p0, SMD, r, n0, n1) {
  
  # simulate data
  trt &amp;lt;- c(rep(0, n0), rep(1, n1))
  Y &amp;lt;- rlogis(n0 + n1, location = logit(p0) + trt * SMD * pi / sqrt(3))
  X &amp;lt;- r * (Y - trt * SMD) * sqrt(3) / pi + rnorm(n0 + n1, sd = sqrt(1 - r^2))
  B &amp;lt;- Y &amp;gt; 0

  # calculate LORs
  logit_fit &amp;lt;- glm(B ~ trt + X, family = &amp;quot;binomial&amp;quot;)
  LOR_marginal &amp;lt;- as.numeric(diff(logit(tapply(B, trt, mean))))
  LOR_logit &amp;lt;- coef(logit_fit)[[&amp;quot;trt&amp;quot;]]
  LORs &amp;lt;- c(LOR_marginal, LOR_logit)
  
  # convert to SMDs
  SMDs &amp;lt;- LORs * sqrt(3) / pi
  
  data.frame(type = c(&amp;quot;marginal&amp;quot;,&amp;quot;conditional&amp;quot;), LOR_est = LORs, SMD_est = SMDs)
} 

simulate_OR_to_SMD(p0 = 0.6, SMD = 0.4, r = 0.7, n0 = 10000, n1 = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          type   LOR_est   SMD_est
## 1    marginal 0.7468089 0.4117373
## 2 conditional 0.8711806 0.4803070&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages ------------------------------------------------------ tidyverse 1.2.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.0     v purrr   0.3.3
## v tibble  3.0.0     v dplyr   0.8.3
## v tidyr   1.0.0     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts --------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params &amp;lt;- 
  list(
    p0 = seq(0.2, 0.8, 0.2),
    SMD = seq(0.2, 0.8, 0.2),
    r = seq(0, 0.9, 0.1)
  ) %&amp;gt;%
  cross_df()

SMDs &amp;lt;- 
  params %&amp;gt;%
  mutate(res = pmap(., simulate_OR_to_SMD, n0 = 50000, n1 = 50000)) %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(RB = SMD_est / SMD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required.
## Please use `cols = c(res)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(SMDs, aes(r, RB, color = type)) + 
  geom_point() + geom_line() + 
  facet_grid(SMD ~ p0, labeller = &amp;quot;label_both&amp;quot;) + 
  theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Converting-odds-ratios-to-standardized-mean-differences_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures</title>
      <link>/publication/procedural-sensitivities-of-scd-effect-sizes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/procedural-sensitivities-of-scd-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Between-case standardized effect size analysis of single case design: Examination of the two methods</title>
      <link>/publication/bcsmd-examination-of-two-methods/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/publication/bcsmd-examination-of-two-methods/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: A gradual effects model for single-case designs</title>
      <link>/gradual-effects-model-paper/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      <guid>/gradual-effects-model-paper/</guid>
      <description>


&lt;p&gt;I’m very happy to share a new paper, co-authored with my student Danny Swan, “A gradual effects model for single-case designs,” which is now available online at &lt;em&gt;Multivariate Behavioral Research&lt;/em&gt;. You can access the published version &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/00273171.2018.1466681?journalCode=hmbr20&#34;&gt;at the journal website&lt;/a&gt; (&lt;a href=&#34;https://www.tandfonline.com/eprint/dkVfazwZaxnyaa9MfUjw/full&#34;&gt;click here for free access while supplies last&lt;/a&gt;) or &lt;a href=&#34;https://psyarxiv.com/vh964/&#34;&gt;the pre-print on PsyArxiv&lt;/a&gt; (always free!). Here’s &lt;a href=&#34;/publication/gradual-effects-model&#34;&gt;the abstract&lt;/a&gt; and &lt;a href=&#34;https://osf.io/uzkq6/&#34;&gt;the supplementary materials&lt;/a&gt;. Danny wrote R functions for fitting the model, (available as part of the &lt;a href=&#34;/software/singlecasees/&#34;&gt;SingleCaseES package&lt;/a&gt;) as well as a &lt;a href=&#34;https://jepusto.shinyapps.io/gem-scd/&#34;&gt;slick web interface&lt;/a&gt;, if you prefer to point-and-click.&lt;/p&gt;
&lt;p&gt;This paper grew out of Danny’s qualifying process (QP), which is the major exam that our doctoral students have to pass before they can begin their dissertation work. For the QP, students work with a faculty advisor to develop an extensive literature review and proposal for an original research project. They produce a written research proposal, then take written and oral exams on their work. For Danny’s QP, he picked up one of the many loose ends in &lt;a href=&#34;/publication/operationally-comparable-effect-sizes&#34;&gt;my dissertation&lt;/a&gt;, studied up on generalized linear models to understand how to express and fit the model, and developed a simulation study evaluating the model. After he successfully passed his QP, we worked together to refine the estimation methods and the simulation design, and then draft a manuscript (much of it cribbed from his QP proposal). I’m very proud and pleased that Danny continued to develop the work and saw it through to a first-authored publication.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A gradual effects model for single-case designs</title>
      <link>/publication/gradual-effects-model/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/publication/gradual-effects-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sampling variance of Pearson r in a two-level design</title>
      <link>/variance-of-r-in-two-level-design/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/variance-of-r-in-two-level-design/</guid>
      <description>


&lt;p&gt;Consider Pearson’s correlation coefficient, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, calculated from two variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; with population correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. If one calculates &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observations, then its sampling variance will be approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(r) \approx \frac{1}{N}\left(1 - \rho^2\right)^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; will actually be a weighted average of within-cluster and between-cluster correlations (see Snijders &amp;amp; Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from a multi-stage sample would not necessarily be the same as that from a simple random sample. What is the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; in this design?&lt;/p&gt;
&lt;p&gt;Let me be more precise here by formalizing the sampling process. Suppose that we have a sample with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; clusters, &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; observations in cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, and total sample size &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{j=1}^m n_j\)&lt;/span&gt;. Assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X_{ij} &amp;amp;= \mu_x + v^x_j + e^x_{ij} \\
Y_{ij} &amp;amp;= \mu_y + v^y_j + e^y_{ij},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=1,...,n_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j=1,...,m\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left[\begin{array}{c} v^x_j \\ v^y_j \end{array}\right] &amp;amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\omega_x^2 &amp;amp; \phi \omega_x \omega_y \\ \phi \omega_x \omega_y &amp;amp; \omega_y^2\end{array}\right]\right) \\ 
\left[\begin{array}{c} e^x_{ij} \\ e^y_{ij} \end{array}\right] &amp;amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\sigma_x^2 &amp;amp; \rho \sigma_x \sigma_y \\ \rho \sigma_x \sigma_y &amp;amp; \sigma_y^2\end{array}\right]\right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the error terms are mutually independent unless otherwise noted. The raw Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is calculated using the total sums of squares and cross-products:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = \frac{SS_{xy}}{\sqrt{SS_{xx} SS_{yy}}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
SS_{xx} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right)^2, \qquad \bar{\bar{x}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} X_{ij} \\
SS_{xy} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(Y_{ij} - \bar{\bar{y}}\right)^2, \qquad \bar{\bar{y}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} Y_{ij} \\
SS_{xy} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right) \left(Y_{ij} - \bar{\bar{y}}\right).
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;common-correlation-and-icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common correlation and ICC&lt;/h3&gt;
&lt;p&gt;The distribution of the total correlation seems to be pretty complicated. So far, I’ve been able to obtain the variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; for a special case that makes some further, fairly restrictive assumptions. Specifically, assume that the correlation is constant across the two levels, so that &lt;span class=&#34;math inline&#34;&gt;\(\phi = \rho\)&lt;/span&gt;, and that the intra-class correlation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same as that of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(k = \omega_x^2 / \sigma_x^2 = \omega_y^2 / \sigma_y^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi = k / (k + 1) = \omega_x^2 / (\omega_x^2 + \sigma_x^2)\)&lt;/span&gt;. Then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(r) \approx \frac{(1 - \rho^2)^2}{\tilde{N}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{N[g_1 k + 1]^2}{g_2 k^2 + 2 g_1 k + 1} \approx \frac{N}{1 + (g_2 - g_1^2)\psi^2},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{g_1 = 1 - \frac{1}{N^2}\sum_{j=1}^m n_j^2}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{g_2 = \frac{1}{N}\sum_{j=1}^m n_j^2 - \frac{2}{N^2}\sum_{j=1}^m n_j^3 + \frac{1}{N^3} \left(\sum_{j=1}^m n_j^2 \right)^2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the clusters are all of equal size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{nm[k(m - 1) / m + 1]^2}{k^2 n (m - 1)/m + 2 k (m - 1) / m + 1} \approx \frac{N}{1 + (n - 1) \psi^2},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The right-hand expression is a further approximation that will be very close to right so long as &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is not too too small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;z-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Z-transformation&lt;/h3&gt;
&lt;p&gt;Under the (restrictive) assumptions of common correlation and equal ICCs, Fisher’s z transformation is variance-stabilizing (as it is under simple random sampling), so it seems reasonable to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(z(r)\right) \approx \frac{1}{\tilde{N} - 3}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;design-effect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Design effect&lt;/h3&gt;
&lt;p&gt;The design effect (&lt;span class=&#34;math inline&#34;&gt;\(DEF\)&lt;/span&gt;) is the ratio of the actual sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; to the sampling variance in a simple random sample of the same size. For the special case that I’ve described,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
DEF = \frac{N}{\tilde{N}} = 1 + (g_2 - g_1^2) \psi^2,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or with equal cluster-sizes, &lt;span class=&#34;math inline&#34;&gt;\(DEF = 1 + (n - 1)\psi^2\)&lt;/span&gt;. These expressions make it clear that the design effect for the correlation is &lt;em&gt;not&lt;/em&gt; equivalent to the well-known design effect for means or mean differences in cluster-randomized designs, which is &lt;span class=&#34;math inline&#34;&gt;\(1 + (n - 1)\psi\)&lt;/span&gt;. We need to take the &lt;em&gt;square&lt;/em&gt; of the ICC here, which will make the design effect for &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; &lt;em&gt;smaller&lt;/em&gt; than the design effect for a mean (or difference in means) based on the same sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-special-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other special cases&lt;/h3&gt;
&lt;p&gt;There are some further special cases that are not to hard to work out and could be useful as rough approximations at least. One is if the within-cluster correlation is zero &lt;span class=&#34;math inline&#34;&gt;\((\rho = 0)\)&lt;/span&gt; and we’re interested in the between-cluster correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. Then the total correlation can be corrected for what is essentially measurement error using formulas from &lt;a href=&#34;https://www.amazon.com/Methods-Meta-Analysis-Correcting-Research-Findings/dp/141290479X&#34;&gt;Hunter and Schmidt (2004)&lt;/a&gt;. A further specialization is if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a cluster-level measure, so that &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x^2 = 0\)&lt;/span&gt;. I’ll consider these in a later post, perhaps.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes</title>
      <link>/using-response-ratios-paper/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/using-response-ratios-paper/</guid>
      <description>


&lt;p&gt;I’m pleased to announce that my article “Using response ratios for meta-analyzing SCDs with behavioral outcomes” has been accepted at &lt;em&gt;Journal of School Psychology&lt;/em&gt;. There’s a multitude of ways that you can access this work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the next 6 weeks or so, the published version of the article will be &lt;a href=&#34;https://authors.elsevier.com/a/1Wj5D56ZN7p98&#34;&gt;available at the journal website&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The pre-print will always remain &lt;a href=&#34;https://psyarxiv.com/nj28d/&#34;&gt;available at PsyArXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Some supporting materials and replication code are &lt;a href=&#34;https://osf.io/c3fe9/&#34;&gt;available on the Open Science Framework&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s the abstract of the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settingsand to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomesin single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Using response ratios for meta-analyzing single-case designs with behavioral outcomes</title>
      <link>/publication/using-response-ratios/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/publication/using-response-ratios/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: procedural sensitivities of effect size measures for SCDs</title>
      <link>/procedural-sensitivities-paper/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/procedural-sensitivities-paper/</guid>
      <description>


&lt;p&gt;I’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. There’s no need to delay in reading it, since you can check out the &lt;a href=&#34;https://psyarxiv.com/vxa86&#34;&gt;pre-print&lt;/a&gt; and &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supporting materials&lt;/a&gt;. Here’s the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This paper was a long time coming. The core idea came out of a grant proposal I wrote during the summer of 2014, which I fleshed out for a &lt;a href=&#34;/files/AERA-2015-poster-Non-overlap-measures.pdf&#34;&gt;poster presented at AERA&lt;/a&gt; in April of 2015. After finishing a draft of the paper, I tried to publish it in a special education journal, reasoning that the main audience for the paper is researchers interested in meta-analyzing single case research studies that are commonly used in some parts of special education. That turned out to be a non-starter. Four rejection letters later, I re-worked the paper a bit to give more technical details, then submitted it to a more methods-ish journal. This yielded an R&amp;amp;R, I revised the paper extensively, resubmitted it, and it was declined. Buying in fully to the sunk costs fallacy, I sent the paper to Psychological Methods. This time, I received very extensive and helpful feedback from several anonymous reviewers and an associate editor (thank you, anonymous peers!), which helped me to revise the paper yet again, and this time it was accepted. Sixth time is the charm, as they say.&lt;/p&gt;
&lt;p&gt;Here’s the complete time-line of submissions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;August 5, 2015: submitted to journal #1 (special education)&lt;/li&gt;
&lt;li&gt;August 28, 2015: desk reject decision from journal #1&lt;/li&gt;
&lt;li&gt;September 3, 2015: submitted to journal #2 (special education)&lt;/li&gt;
&lt;li&gt;November 6, 2015: reject decision (after peer review) from journal #2&lt;/li&gt;
&lt;li&gt;November 18, 2015: submitted to journal #3 (special education)&lt;/li&gt;
&lt;li&gt;November 22, 2015: desk reject decision from journal #3 as not appropriate for their audience. I was grateful to get a quick decision.&lt;/li&gt;
&lt;li&gt;November 23, 2015: submitted to journal #4 (special education)&lt;/li&gt;
&lt;li&gt;February 17, 2016: reject decision (after peer review) from journal #4&lt;/li&gt;
&lt;li&gt;April 19, 2016: submitted to journal #5 (methods)&lt;/li&gt;
&lt;li&gt;August 16, 2016: revise-and-resubmit decision from journal #5&lt;/li&gt;
&lt;li&gt;October 14, 2016: re-submitted to journal #5&lt;/li&gt;
&lt;li&gt;February 2, 2017: reject decision from journal #5&lt;/li&gt;
&lt;li&gt;May 10, 2017: submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 1, 2017: revise-and-resubmit decision from &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 26, 2017: re-submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;November 22, 2017: conditional acceptance&lt;/li&gt;
&lt;li&gt;December 6, 2017: re-submitted with minor revisions&lt;/li&gt;
&lt;li&gt;January 10, 2018: accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to the special issue on single-case systematic reviews and meta-analysis</title>
      <link>/publication/rase-special-issue-introduction/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/publication/rase-special-issue-introduction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes</title>
      <link>/using-log-response-ratios/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/using-log-response-ratios/</guid>
      <description>


&lt;p&gt;One of the papers that came out of my dissertation work (&lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;Pustejovsky, 2015&lt;/a&gt;) introduced an effect size metric called the &lt;strong&gt;log response ratio&lt;/strong&gt; (or LRR) for use in meta-analysis of single-case research—particularly for single-case studies that measure behavioral outcomes through systematic direct observation. The original paper was pretty technical since it focused mostly on a formal measurement model for behavioral observation data. I’ve just completed a tutorial paper that demonstrates how to use the LRR for meta-analyzing single-case studies with behavioral outcomes. In this paper, I’ve tried to present the methods in as accessible a manner as I could muster, to provide a sort of “user’s guide” for researchers wanting to apply the LRR for their own work. You can find the &lt;a href=&#34;https://osf.io/4fe6u/&#34;&gt;working paper&lt;/a&gt; and &lt;a href=&#34;https://osf.io/c3fe9/&#34;&gt;supplementary materials&lt;/a&gt; (including data and replication code) on the Open Science Framework. I would welcome your feedback and questions about this work!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods</title>
      <link>/publication/fabi-meta-analysis/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/publication/fabi-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research synthesis and meta-analysis of single-case designs</title>
      <link>/publication/meta-analysis-of-scd/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>/publication/meta-analysis-of-scd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Between-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application</title>
      <link>/publication/bc-smd-primer-and-tutorial/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/publication/bc-smd-primer-and-tutorial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New tutorial paper on BC-SMD effect sizes</title>
      <link>/scdhlm-tutorial/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/scdhlm-tutorial/</guid>
      <description>


&lt;p&gt;I’m pleased to announce that the &lt;a href=&#34;https://www.campbellcollaboration.org/&#34;&gt;Campbell Collaboration&lt;/a&gt; has just published a new discussion paper that I wrote with my colleagues Jeff Valentine and Emily Tanner-Smith about &lt;a href=&#34;https://campbellcollaboration.org/library/effect-sizes-single-case-designs-campbell-discussion-paper-1.html&#34;&gt;between-case standardized mean difference effect sizes for single-case designs&lt;/a&gt;.
The paper provides a relatively non-technical introduction to BC-SMD effect sizes and a tutorial on how to use the &lt;a href=&#34;https://jepusto.shinyapps.io/scdhlm/&#34;&gt;scdhlm web-app&lt;/a&gt; for calculating estimates of the BC-SMD for user-provided data.
If you have any questions or feedback about the app, please feel free to contact me!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presentation at IES 2016 PI meeting</title>
      <link>/ies-2016-pi-meeting/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/ies-2016-pi-meeting/</guid>
      <description>


&lt;p&gt;I am just back from the Institute of Education Sciences 2016 Principal Investigators meeting. Rob Horner had organized a session titled “Single-case methods: Current status and needed directions” as a tribute to our colleague Will Shadish, who passed away this past year. Rob invited me to give some brief remarks about Will as a mentor, and then to present some of my work with Will and Larry Hedges on effect sizes for single-case research. Here are &lt;a href=&#34;/files/Single-case-methods-IES-PI-meeting-2016.pdf&#34;&gt;the slides from my part of the presentation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is Tau-U?</title>
      <link>/what-is-tau-u/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/what-is-tau-u/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber (2011)&lt;/a&gt; proposed the Tau-U index—actually several indices, rather—as effect size measures for single-case designs. The original paper describes several different indices that involve corrections for trend during the baseline phase, treatment phase, both phases, or neither phase. Without correcting for trends in either phase, the index is equal to the Mann-Whitney &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; statistic calculated by comparing every pair of observations containing one point from each phase, scaled by the total number of such pairs. This version, which I’ll call just “Tau”, is simply a &lt;a href=&#34;/NAP-SEs-and-CIs&#34;&gt;linear re-scaling of the NAP statistic&lt;/a&gt; to the range [-1,1].&lt;/p&gt;
&lt;p&gt;To correct for baseline trend, the original paper proposes to calculate Kendall’s rank correlation (&lt;span class=&#34;math inline&#34;&gt;\(\tau_A\)&lt;/span&gt;) between the phase A outcome data and the session numbers and use the result to make an adjustment to Tau. The other analyses presented in the original paper (incorporating adjustments for time trends during the treatment phase) are not presented in subsequent review papers, nor are they implemented in &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;the web-calculator&lt;/a&gt; created by the authors, and so I won’t discuss them further here. Instead, in this post I will examine the calculation of the version of Tau-U that incorporates a baseline trend correction. This version seems to be the most widely applied in practice (likely due to the availability of the web-calculator) and is presented in several review papers by the same authors. It turns out though, that the definition of the index has shifted from the original paper to subsequent presentations.&lt;/p&gt;
&lt;p&gt;To make this concrete, let me first define a couple of things. Suppose that we have data from the baseline and treatment phases for a single case, where the baseline phase has &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; observations and treatment phase has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Let &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; denote Kendall’s S statistic calculated for the comparison between phases and &lt;span class=&#34;math inline&#34;&gt;\(S_A\)&lt;/span&gt; denote Kendall’s S statistic calculated on the baseline trend. More precisely,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_P &amp;amp;= \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right] \\
S_A &amp;amp;= \sum_{i=1}^{m - 1} \sum_{j = i + 1}^m \left[I\left(y^A_j &amp;gt; y^A_i\right) - I\left(y^A_j &amp;lt; y^A_i\right)\right].
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; is calculated from &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; pairs of observations, and Tau (without trend correction) is equal to &lt;span class=&#34;math inline&#34;&gt;\(\text{Tau} = S_P / (m n)\)&lt;/span&gt;. Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(S_A\)&lt;/span&gt; is calculated from &lt;span class=&#34;math inline&#34;&gt;\(m (m - 1) / 2\)&lt;/span&gt; pairs of observations and Kendall’s rank correlation coefficient for the baseline phase observations is &lt;span class=&#34;math inline&#34;&gt;\(t_A = S_A / [m (m - 1) / 2]\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;the-original-version&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The original version&lt;/h2&gt;
&lt;p&gt;In the original paper, the authors explain that values of Tau-U can be calculated by adding or substracting values of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, weighted by the corresponding number of pairs. Thus, Tau-U would be calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau-U} = \frac{S_P - S_A}{mn + m(m - 1) / 2} = \frac{2n}{2n + m - 1} \text{Tau} - \frac{m - 1}{2n + m - 1} t_A.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both &lt;span class=&#34;math inline&#34;&gt;\(\text{Tau}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_A\)&lt;/span&gt; have range [-1,1], and so Tau-U has the same range. This version of Tau-U can be calculated using &lt;a href=&#34;https://manolov.shinyapps.io/Overlap/&#34;&gt;this web app by Rumen Manolov&lt;/a&gt;, which is based on &lt;a href=&#34;https://dl.dropboxusercontent.com/u/2842869/Tau_U.R&#34;&gt;this R code by Kevin Tarlow&lt;/a&gt;. (The app and the R script also provide the other variants of Tau-U described in &lt;a href=&#34;http://dx.doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber (2011)&lt;/a&gt;.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-revised-version&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The revised (?) version&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.1177/0145445511399147&#34;&gt;Parker, Vannest, and Davis (2011)&lt;/a&gt; reviewed nine different non-overlap indices for use with data from single-case designs, including Tau-U. Rather than describing all four variations from the original paper, the authors define the index as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tau-U (Parker et al., in press) extends [Tau] to control for undesirable positive baseline trend (monotonic trend). Monotonic trend is the upward progression of data points in any configuration, whether linear, curvilinear, or even in a mixed pattern of “fits and starts” (p. 11).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this and subsequent review articles, Tau-U seems to refer exclusively to the variant involving comparison between phases A and B, with an adjustment for phase A trend. That seems a sensible enough choice, which could have been due to space limitations, guidance from the journal editor, or further refinement of the methods (i.e., recognizing which of the variants would be most useful in application). However, the presentation of Tau-U in this article involved more than a change in emphasis—the definition of the index also changed. Following the notation above, Tau-U was now defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau-U} = \frac{S_P - S_A}{mn} = \text{Tau} - \frac{m - 1}{2n} t_A.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The logical range of this version of the index is from &lt;span class=&#34;math inline&#34;&gt;\(-(2n + m - 1) / (2n)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((2n + m - 1) / (2n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is the version of Tau-U implemented in the &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;singlecaseresearch.org&lt;/a&gt; web calculator. It is also the version described in a later chapter by the same authors (&lt;a href=&#34;http://dx.doi.org/10.1037/14376-005&#34;&gt;Parker, Vannest, &amp;amp; Davis, 2014&lt;/a&gt;) and a review article by &lt;a href=&#34;http://dx.doi.org/10.1111/1467-8578.12091&#34;&gt;Rakap (2015)&lt;/a&gt;. &lt;a href=&#34;/Tau-U&#34;&gt;My previous post about Tau-U&lt;/a&gt; also presented this version of the index and noted that its magnitude is sensitive to the lengths of the baseline and treatment phases, which makes it rather difficult to interpret the Tau-U index as a measure of treatment effect magnitude.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;Here is an R function for calculating the original or revised versions of Tau-U:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tau_U &amp;lt;- function(A_data, B_data, version = &amp;quot;revised&amp;quot;) {
    m &amp;lt;- length(A_data)
    n &amp;lt;- length(B_data)
    Q_A &amp;lt;- sapply(A_data, function(j) (j &amp;gt; A_data) - (j &amp;lt; A_data))
    Q_P &amp;lt;- sapply(B_data, function(j) (j &amp;gt; A_data) - (j &amp;lt; A_data))
    
    if (version==&amp;quot;original&amp;quot;) {
      (sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n + m * (m - 1) / 2)
    } else {
      (sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The papers I’ve mentioned above all provide examples of the calculation of Tau-U. The following table reports the data from each of these examples (&lt;a href=&#34;http://dx.doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber, 2011a&lt;/a&gt;; &lt;a href=&#34;http://dx.doi.org/10.1177/0145445511399147&#34;&gt;Parker, Vannest, and Davis, 2011b&lt;/a&gt;; &lt;a href=&#34;http://dx.doi.org/10.1037/14376-005&#34;&gt;Parker, Vannest, &amp;amp; Davis, 2014&lt;/a&gt;), along with the value of Tau-U based on the original and revised formulas. The differences in magnitude are non-trivial.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Source&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Phase A data&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Phase B data&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;original&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;revised&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2011a, Figure 2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2, 3, 5, 3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4, 5, 5, 7, 6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2011b, Figure 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20, 20, 26, 25, 22, 23&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;28, 25, 24, 27, 30, 30, 29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5438596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7380952&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2011b, Table 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3, 3, 4, 5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4, 5, 6, 7, 7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4230769&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2014, Figure 4.1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;22, 21, 23, 23, 23, 22&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24, 22, 23, 23, 24, 26, 25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4385965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5952381&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;implications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;Rather than one effect size index called “Tau-U”, there are instead two different definitions, which can lead to quite different values of the index. Given this, researchers who apply Tau-U should endeavor to &lt;strong&gt;be clear and unambiguous about which version of the index they use&lt;/strong&gt;. This can be done by stating exactly which software routine, web-app, or formula was used in making the calculations. If the calculations are done using a computer script, then the script should be made available (e.g., through the &lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework&lt;/a&gt;) so that other researchers can replicate the calculations.&lt;/p&gt;
&lt;p&gt;Furthermore, researchers need to &lt;strong&gt;be careful about applying interpretive guidelines for Tau-U&lt;/strong&gt;, since those guidelines will not apply uniformly across the different versions of the index.&lt;/p&gt;
&lt;p&gt;Finally, I would recommend that any researchers who conduct a meta-analysis of single-case research &lt;strong&gt;make available the raw data used for effect size calculations&lt;/strong&gt;, so that other researchers can scrutinize, replicate, and extend their analyses. The whole enterprise of research synthesis rests on the availability of data from primary studies (at least in summary form). It seems to me that meta-analysts thus have a duty to make the data that they assemble and organize readily accessible for others to use. Particularly in the context of meta-analysis of single-case research—where new methods are developing rapidly and there is not currently consensus around best practices—it seems especially appropriate and prudent to make one’s data available for future re-analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New working paper: Procedural sensitivities of SCD effect sizes</title>
      <link>/scd-effect-size-sensitivities/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      <guid>/scd-effect-size-sensitivities/</guid>
      <description>


&lt;p&gt;I’ve just posted a new version of my working paper, &lt;em&gt;Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures&lt;/em&gt;. The abstract is below. This version is a major update of an &lt;a href=&#34;/files/Pustejovsky-2015-Nov-Non-overlap-measures.pdf&#34;&gt;earlier paper&lt;/a&gt; that focused only on the non-overlap measures. The new version also includes analysis of two other effect sizes (the within-case standardized mean difference and the log response ratio) as well as additional results and more succinct summaries of the main findings.&lt;/p&gt;
&lt;p&gt;The paper itself is available on the Open Science Framework (&lt;a href=&#34;https://osf.io/pxn24/&#34;&gt;here&lt;/a&gt;), as are the &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supplementary materials&lt;/a&gt; and &lt;a href=&#34;https://osf.io/j4gvt/&#34;&gt;Source code&lt;/a&gt;. I also created interaction versions of the graphics from the main paper and the supplementary materials, which can be viewed in &lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-size-sensitivities/&#34;&gt;this shiny app&lt;/a&gt;. I would welcome any comments, questions, or feedback that readers may have.&lt;/p&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures, such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common approach to outcome measurement in single-case research. This study uses computer simulation to investigate the properties of several single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Alternative formulas for the standardized mean difference</title>
      <link>/alternative-formulas-for-the-smd/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      <guid>/alternative-formulas-for-the-smd/</guid>
      <description>


&lt;p&gt;The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.&lt;/p&gt;
&lt;p&gt;There’s some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I’ll leave that discussion for another day. Here, I’d like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so getting the variance calculations right is an important (and sometimes time consuming) part of any meta-analysis project. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I’d like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in some non-standard cases (and also leads to slight differences from conventional formulas for the standard ones). All of this will be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.&lt;/p&gt;
&lt;p&gt;To start, let me review (regurgitate?) the standard presentation.&lt;/p&gt;
&lt;div id=&#34;smd-from-a-simple-independent-groups-design&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SMD from a simple, independent groups design&lt;/h3&gt;
&lt;p&gt;Textbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a &lt;strong&gt;simple, independent groups design&lt;/strong&gt;. Call the groups T and C, the sample sizes &lt;span class=&#34;math inline&#34;&gt;\(n_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_C\)&lt;/span&gt;, the sample means &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_C\)&lt;/span&gt;, and the sample variances &lt;span class=&#34;math inline&#34;&gt;\(s_T^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_C^2\)&lt;/span&gt;. A basic moment estimator of the SMD is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_T - \bar{y}_C}{s_p}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s_p^2 = \frac{\left(n_T - 1\right)s_T^2 + \left(n_C - 1\right) s_C^2}{n_T + n_C - 2}\)&lt;/span&gt; is a pooled estimator of the population variance. The standard estimator for the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = \frac{n_T + n_C}{n_T n_C} + \frac{d^2}{2\left(n_T + n_C - 2\right)},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is well known that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; has a small sample bias that depends on sample sizes. Letting&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
J(x) = 1 - \frac{3}{4x - 1},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the bias-corrected estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g = J\left(n_T + n_C - 2\right) \times d,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and is often referred to as Hedges’ &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; because it was proposed in &lt;a href=&#34;http://doi.org/10.3102/10769986006002107&#34;&gt;Hedges (1981)&lt;/a&gt;. Some meta-analysts use &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt;, but with &lt;span class=&#34;math inline&#34;&gt;\(d^2\)&lt;/span&gt; replaced by &lt;span class=&#34;math inline&#34;&gt;\(g^2\)&lt;/span&gt;, as an estimator of the large-sample variance of &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;; others use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_g = J^2\left(n_T + n_C - 2\right) \left(\frac{n_T + n_C}{n_T n_C} + \frac{g^2}{2\left(n_T + n_C - 2\right)}\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.3102/1076998606298034&#34;&gt;Viechtbauer (2007)&lt;/a&gt; provides further details on variance estimation and confidence intervals for the SMD in this case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-general-formula-for-g-and-its-sampling-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A general formula for &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; and its sampling variance&lt;/h3&gt;
&lt;p&gt;The above formulas are certainly useful, but in practice meta-analyses often include studies that use other, more complex designs.
Good textbook presentations also cover computation of &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; given above applies in general. With other types of studies, &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; can be a wildly biased estimator of the actual sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, because it is derived under the assumption that the numerator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise.&lt;/p&gt;
&lt;p&gt;Here’s what I think is a more useful way to think about the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let’s suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, its sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(b)\)&lt;/span&gt;, and its standard error &lt;span class=&#34;math inline&#34;&gt;\(se_{b}\)&lt;/span&gt;. Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt;, with expectation &lt;span class=&#34;math inline&#34;&gt;\(\text{E}\left(S^2\right) = \sigma^2\)&lt;/span&gt; and sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(S^2)\)&lt;/span&gt;. Finally, suppose that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d = b / S\)&lt;/span&gt; is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(d\right) \approx \frac{\text{Var}(b)}{\sigma^2} + \frac{\delta^2}{2 \nu},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\nu = 2 \left[\text{E}\left(S^2\right)\right]^2 / \text{Var}\left(S^2\right)\)&lt;/span&gt;. Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = \left(\frac{se_b}{S}\right)^2 + \frac{d^2}{2 \nu}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This estimator has two parts. The first part involves &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt;, which is just the standard error of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, but re-scaled into standard deviation units; this part captures the variability in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; from its numerator. This scaled standard error can be calculated directly if an article reports &lt;span class=&#34;math inline&#34;&gt;\(se_b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second part of &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(d^2 / (2 \nu)\)&lt;/span&gt;, which captures the variability in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; due to its denominator. More precise estimates of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; will have larger degrees of freedom, so that the second part will be smaller. For some designs, the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; depend only on sample sizes, and thus can be calculated exactly. For some other designs, &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; must be estimated.&lt;/p&gt;
&lt;p&gt;The same degrees of freedom can also be used in the small-sample correction for the bias of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, as given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g = J(\nu) \times d.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This small-sample correction is based on a Satterthwaite-type approximation to the distribution of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here’s another way to express the variance estimator for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = d^2 \left(\frac{1}{t^2} + \frac{1}{2 \nu}\right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the test statistic corresponding to the hypothesis test for no difference between groups. I’ve never seen that formula in print before, but it could be convenient if an article reports the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; statistic (or &lt;span class=&#34;math inline&#34;&gt;\(F = t^2\)&lt;/span&gt; statistic).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-standard-estimators-of-d&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-standard estimators of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The advantage of this formulation of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; is that it can be applied in quite a wide variety of circumstances, including cases that aren’t usually covered in textbook treatments. Rather than having to use separate formulas for every combination of design and analytic approach under the sun, the same formulas apply throughout. What changes are the components of the formulas: the scaled standard error &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt; and the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. The general formulation also makes it easier to swap in different estimates of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;—i.e., if you estimate the numerator a different way but keep the denominator the same, you’ll need a new scaled standard error but can still use the same degrees of freedom. A bunch of examples:&lt;/p&gt;
&lt;div id=&#34;independent-groups-with-different-variances&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Independent groups with different variances&lt;/h4&gt;
&lt;p&gt;Suppose that we’re looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that &lt;span class=&#34;math inline&#34;&gt;\(d = \left(\bar{y}_T - \bar{y}_C\right) / s_C\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(s_C^2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C - 1\)&lt;/span&gt; degrees of freedom, the small-sample bias correction will then need to be &lt;span class=&#34;math inline&#34;&gt;\(J(n_C - 1)\)&lt;/span&gt;. The scaled standard error will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{s_C} = \sqrt{\frac{s_T^2}{s_C^2 n_T} + \frac{1}{n_C}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is then everything that we need to calculate &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V_g\)&lt;/span&gt;, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-independent-groups&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiple independent groups&lt;/h4&gt;
&lt;p&gt;Suppose that the study involves &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; treatment groups, 1 control group, and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; total participants. If the meta-analysis will include SMDs comparing &lt;em&gt;each&lt;/em&gt; treatment group to the control group, it would make sense to pool the sample variance across all &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s_p^2 = \frac{1}{N - K} \sum_{k=0}^K (n_k - 1) s_k^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a comparison between treatment group &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and the control group, we would then use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_k - \bar{y}_C}{s_p}, \qquad \nu = N - K, \qquad \frac{se_b}{s_p} = \sqrt{\frac{1}{n_C} + \frac{1}{n_k}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n_k\)&lt;/span&gt; is the sample size for treatment group &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; (cf. Gleser &amp;amp; Olkin, 2009).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-group-pre-test-post-test-design&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Single group, pre-test post-test design&lt;/h4&gt;
&lt;p&gt;Suppose that a study involves taking pre-test and post-test measurements on a single group of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_{post} - \bar{y}_{pre}}{s_p}, \qquad \text{where} \qquad s_p^2 = \frac{1}{2}\left(s_{pre}^2 + s_{post}^2\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this design,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{s_p} = \frac{2(1 - r)}{n},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the sample correlation between the pre- and post-tests. The remaining question is what to use for &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. Borenstein (2009) uses &lt;span class=&#34;math inline&#34;&gt;\(\nu = n - 1\)&lt;/span&gt;. My previous post &lt;a href=&#34;/distribution-of-sample-variances/&#34;&gt;on the sampling covariance of sample variances&lt;/a&gt; gave the result that &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(s_p^2) = \sigma^4 (1 + \rho^2) / (n - 1)\)&lt;/span&gt;, which would instead suggest using&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nu = \frac{2 (n - 1)}{1 + r^2}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula will tend to give slightly larger degrees of freedom, but probably won’t be that discrepant from Borenstein’s approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator itself is based on a large-sample approximation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-group-pre-test-post-test-design-ancova-estimation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Two group, pre-test post-test design: ANCOVA estimation&lt;/h4&gt;
&lt;p&gt;Suppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes &lt;span class=&#34;math inline&#34;&gt;\(n_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_C\)&lt;/span&gt; respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{S} = \frac{(n_C + n_T)(1 - r^2)}{n_C n_T},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the pooled, within-group sample correlation between the pre-test and the post-test measures (this approximation assumes that the pre-test SMD between groups is relatively small). Alternately, if &lt;span class=&#34;math inline&#34;&gt;\(se_b\)&lt;/span&gt; is provided then the scaled standard error could be calculated directly.&lt;/p&gt;
&lt;p&gt;Borenstein (2009) suggests calculating &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;. (Borenstein instead uses &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2 - q\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the number of covariates in the analysis, but this won’t usually make much difference unless the total sample size is quite small.)&lt;/p&gt;
&lt;p&gt;Scaling by the pooled post-test sample variance isn’t the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt; directly and use &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;. If it is reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test &lt;em&gt;and&lt;/em&gt; post-test sample variances in each group. Using this approach, you would again need to calculate &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt; directly and then use &lt;span class=&#34;math inline&#34;&gt;\(\nu = 2(n_C + n_T - 2) / (1 + r^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-group-pre-test-post-test-design-repeated-measures-estimation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Two group, pre-test post-test design: repeated measures estimation&lt;/h4&gt;
&lt;p&gt;Another way to analyze the data from the same type of study design is to use repeated measures ANOVA. I’ve recently encountered a number of studies that use this approach (here’s a recent example from &lt;a href=&#34;http://dx.doi.org/10.1371/journal.pone.0154075&#34;&gt;a highly publicized study in PLOS ONE&lt;/a&gt;—see Table 2). The studies I’ve seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{gt}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{gt}^2\)&lt;/span&gt; denote the sample mean and sample variance in group &lt;span class=&#34;math inline&#34;&gt;\(g = T, C\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t = 0, 1\)&lt;/span&gt;. The numerator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; would then be calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
b = \left(\bar{y}_{T1} - \bar{y}_{T0}\right) - \left(\bar{y}_{C1} - \bar{y}_{C0}\right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which has sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(b) = 2(1 - \rho)\sigma^2\left(n_C + n_T \right) / (n_C n_T)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{S} = \frac{2(1 - r)(n_C + n_T)}{n_C n_T}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As with ANCOVA, there are several potential options for calculating the denominator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using the pooled sample variances on the post-test measures, with &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the pooled sample variances on the pre-test measures, with &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;; or&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the pooled sample variances at both time points and in both groups, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  S^2 = \frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},
  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\nu = 2(n_C + n_T - 2) / (1 + r^2)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The range of approaches to scaling is the same as for ANCOVA. This makes sense because both analyses are based on data from the same study design, so the parameter of interest should be the same (i.e., the target parameter should not change based on the analytic method). Note that all of these approaches are a bit different than the effect size estimator proposed by &lt;a href=&#34;http://doi.org/10.1037//1082-989X.7.1.105&#34;&gt;Morris and DeShon (2002)&lt;/a&gt; for the two-group, pre-post design; their approach does not fit into my framework because it involves taking a difference between standardized effect sizes (and therefore involves two separate estimates of scale, rather than just one).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomized-trial-with-longitudinal-follow-up&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Randomized trial with longitudinal follow-up&lt;/h4&gt;
&lt;p&gt;Many independent-groups designs—especially randomized trials in field settings—involve repeated, longitudinal follow-up assessments. An increasingly common approach to analysis of such data is through hierarchical linear models, which can be used to account for the dependence structure among measurements taken on the same individual. In this setting, &lt;a href=&#34;http://doi.org/10.1037/a0014699&#34;&gt;Feingold (2009)&lt;/a&gt; proposes that the SMD be calculated as the model-based estimate of the treatment effect at the final follow-up time, scaled by the within-groups variance of the outcome at that time point. Let &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\)&lt;/span&gt; denote the estimated difference in slopes (change per unit time) between groups in a linear growth model, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; denote the duration of the study, and &lt;span class=&#34;math inline&#34;&gt;\(s_{pF}^2\)&lt;/span&gt; denote the pooled sample variance of the outcome at the final time point. For this model, Feingold (2009) proposes to calculate the standardized mean difference as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{F \hat\beta_1}{s_{pF}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a later paper, &lt;a href=&#34;http://doi.org/10.1037/a0037721&#34;&gt;Feingold (2015)&lt;/a&gt; proposes that the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; be estimated as &lt;span class=&#34;math inline&#34;&gt;\(F \times se_{\hat\beta_1} / s_{pF}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(se_{\hat\beta_1}\)&lt;/span&gt; is the standard error of the estimated slope. My framework suggests that a better estimate of the sampling variance, which accounts for the uncertainty of the scale estimate, would be to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = \left(\frac{F \times se_{\hat\beta_1}}{s_{pF}}\right)^2 + \frac{d^2}{2 \nu},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_T + n_C - 2\)&lt;/span&gt;. The same &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; could be used to bias-correct the effect size estimate.&lt;/p&gt;
&lt;p&gt;If estimates of the variance components of the HLM are reported, one could use them to construct a model-based estimate of the scale parameter in the denominator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. I explored this approach in a paper that uses HLM to model single-case designs, which are a certain type of longitudinal experiment that typically involve a very small number of participants (&lt;a href=&#34;http://doi.org/10.3102/1076998614547577&#34;&gt;Pustejovsky, Hedges, &amp;amp; Shadish, 2014&lt;/a&gt;). Estimates of the scale parameter can usually be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{model}^2 = \mathbf{r}&amp;#39;\boldsymbol\omega,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\omega\)&lt;/span&gt; is a vector of all the variance components in the model and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{r}\)&lt;/span&gt; is a vector of weights that depend on the model specification and length of follow-up. This estimate of scale will usually be more precise than &lt;span class=&#34;math inline&#34;&gt;\(s_{pF}^2\)&lt;/span&gt; because it makes use of all of the data (and modeling assumptions). However, it can be challenging to determine appropriate degrees of freedom for &lt;span class=&#34;math inline&#34;&gt;\(S_{model}^2\)&lt;/span&gt;. For single-case designs, I used estimates of &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\boldsymbol\omega)\)&lt;/span&gt; based on the inverse of the expected information matrix—call the estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_{\boldsymbol\omega}\)&lt;/span&gt;—in which case&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nu = \frac{2 S_{model}^4}{\mathbf{r}&amp;#39; \mathbf{V}_{\boldsymbol\omega} \mathbf{r}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, most published articles will not provide estimates of the sampling variances of the variance components—in fact, a lot of software for estimating HLMs does not even provide these. It would be useful to work out some reasonable approximations for the degrees of freedom in these models—approximations that can be calculated based on the information that’s typically available—and to investigate the extent to which there’s any practical benefit to using &lt;span class=&#34;math inline&#34;&gt;\(S_{model}^2\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(s_{pF}^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cluster-randomized-trials&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Cluster-randomized trials&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.3102/1076998606298043&#34;&gt;Hedges (2007)&lt;/a&gt; addresses estimation of standardized mean differences for cluster-randomized trials, in which the units of measurement are nested within higher-level clusters that comprise the units of randomization. Such designs involve two variance components (within- and between-cluster variance), and thus there are three potential approaches to scaling the treatment effect: standardize by the total variance (i.e., the sum of the within- and between-cluster components), standardize by the within-cluster variance, or standardize by the between-cluster variance. Furthermore, some of the effect sizes can be estimated in several different ways, each with a different sampling variance. &lt;a href=&#34;http://doi.org/10.3102/1076998606298043&#34;&gt;Hedges (2007)&lt;/a&gt; gives sampling variance estimates for each estimator of each effect size, but they all follow the same general formula as given above. (The appendix of the article actually gives the same formula as above, but using a more abstract formulation.)&lt;/p&gt;
&lt;p&gt;For example, suppose the target SMD parameter uses the total variance and that we have data from a two-level, two-arm cluster randomized trial with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; clusters, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations per cluster, and total sample sizes in each arm of &lt;span class=&#34;math inline&#34;&gt;\(N_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_C\)&lt;/span&gt;, respectively. Let &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; be the between-cluster variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; be the within-cluster variance, and &lt;span class=&#34;math inline&#34;&gt;\(\rho = \tau^2 / (\tau^2 + \sigma^2)\)&lt;/span&gt;. The target parameter is &lt;span class=&#34;math inline&#34;&gt;\(\delta = \left(\mu_T - \mu_C\right) / \left(\tau^2 + \sigma^2\right)\)&lt;/span&gt;. The article assumes that the treatment effect will be estimated by the difference in grand means, &lt;span class=&#34;math inline&#34;&gt;\(\bar{\bar{y}}_T - \bar{\bar{y}}_C\)&lt;/span&gt;. Letting &lt;span class=&#34;math inline&#34;&gt;\(S_B^2\)&lt;/span&gt; be the pooled sample variance of the cluster means within each arm and &lt;span class=&#34;math inline&#34;&gt;\(S_W^2\)&lt;/span&gt; be the pooled within-cluster sample variance, the total variance is estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{total}^2 = S_B^2 + \frac{n - 1}{n} S_W^2. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;An estimate of the SMD is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \left(\bar{\bar{y}}_T - \bar{\bar{y}}_C \right) / \sqrt{S_{total}^2}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The scaled standard error of &lt;span class=&#34;math inline&#34;&gt;\(\bar{\bar{y}}_T - \bar{\bar{y}}_C\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
se_b = \left(\frac{N_C + N_T}{N_C N_T}\right)\left[1 + (n - 1)\rho\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The appendix of the article demonstrates that &lt;span class=&#34;math inline&#34;&gt;\(\text{E}\left(S_{total}^2\right) = \tau^2 + \sigma^2\)&lt;/span&gt; and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left( S_{total}^2 \right) = \frac{2}{n^2}\left(\frac{(n \tau^2 + \sigma^2)^2}{M - 2} + \frac{(n - 1)^2 \sigma^4}{N_C + N_T - M}\right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;by which it follows that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nu = \frac{n^2 M (M - 2)}{M[(n - 1)\rho + 1]^2 + (M - 2)(n - 1)(1 - \rho)^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting &lt;span class=&#34;math inline&#34;&gt;\(se_b / S_{total}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; into the formula for &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; gives the same as Expression (14) in the article.&lt;/p&gt;
&lt;p&gt;A limitation of &lt;a href=&#34;http://doi.org/10.3102/1076998606298043&#34;&gt;Hedges (2007)&lt;/a&gt; is that it only covers the case where the treatment effect is estimated by the difference in grand means (although it does cover the case of unequal cluster sizes, which gets quite messy). In practice, every cluster-randomized trial I’ve ever seen uses baseline covariates to adjust the mean difference (often based on a hierarchical linear model) and improve the precision of the treatment effect estimate. The SMD estimate should also be based on this covariate-adjustment estimate, scaled by the total variance &lt;em&gt;without adjusting for the covariate&lt;/em&gt;. An advantage of the general formulation given above is that its clear how to estimate the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. I would guess that it will often be possible to calculate the scaled standard error directly, given the standard error of the covariate-adjusted treatment effect estimate. And since &lt;span class=&#34;math inline&#34;&gt;\(S_{total}\)&lt;/span&gt; would be estimated just as before, its degrees of freedom remain the same.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.3102/1076998610376617&#34;&gt;Hedges (2011)&lt;/a&gt; discusses estimation of SMDs in three-level cluster-randomized trials—an even more complicated case. However, the general approach is the same; all that’s needed are the scaled standard error and the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; of whatever combination of variance components go into the denominator of the effect size. In both the two-level and three-level cases, the degrees of freedom get quite complicated in unbalanced samples and are probably not calculable from the information that is usually provided in an article. Hedges (2007, 2011) comments on a couple of cases where more tractable approximations can be used, although it seems like there might be room for further investigation here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing thoughts&lt;/h3&gt;
&lt;p&gt;I think this framework is useful in that it unifies a large number of cases that have been treated separately, and can also be applied (more-or-less immediately) to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; estimators that haven’t been widely considered before, such as the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; that involves scaling by the pooled pre-and-post, treatment-and-control sample variance. I hope it also illustrates that, while the point estimator &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be applied across a large number of study designs, the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; depends on the details of the design and estimation methods. The same is true for other families of effect sizes as well. For example, in other work I’ve demonstrated that the sampling variance of the correlation coefficient depends on the design from which the correlations are estimated (&lt;a href=&#34;http://doi.org/10.1037/a0033788&#34;&gt;Pustejovsky, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If you have read this far, I’d love to get your feedback about whether you think this is a useful way to organize the calculations of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; estimators. Is this helpful? Or nothing you didn’t already know? Or still more complicated than it should be? Leave a comment!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, &amp;amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–236). New York, NY: Russell Sage Foundation.&lt;/p&gt;
&lt;p&gt;Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43–53. &lt;a href=&#34;doi:10.1037/a0014699&#34; class=&#34;uri&#34;&gt;doi:10.1037/a0014699&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Feingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. &lt;a href=&#34;doi:10.1037/a0037721&#34; class=&#34;uri&#34;&gt;doi:10.1037/a0037721&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gleser, L. J., &amp;amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp;amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357–376). New York, NY: Russell Sage Foundation.&lt;/p&gt;
&lt;p&gt;Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. &lt;a href=&#34;doi:10.3102/1076998606298043&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998606298043&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. &lt;a href=&#34;doi:10.3102/1076998610376617&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998610376617&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morris, S. B., &amp;amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. Psychological Methods, 7(1), 105–125. &lt;a href=&#34;doi:10.1037//1082-989X.7.1.105&#34; class=&#34;uri&#34;&gt;doi:10.1037//1082-989X.7.1.105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92–112. &lt;a href=&#34;doi:10.1037/a0033788&#34; class=&#34;uri&#34;&gt;doi:10.1037/a0033788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pustejovsky, J. E., Hedges, L. V, &amp;amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. &lt;a href=&#34;doi:10.3102/1076998614547577&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998614547577&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Viechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. &lt;a href=&#34;doi:10.3102/1076998606298034&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998606298034&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>scdhlm</title>
      <link>/software/scdhlm/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/software/scdhlm/</guid>
      <description>&lt;p&gt;An R package implementing several methods of estimating a design-comparable standardized mean difference effect size based on data from a single-case design. Methods include those from Hedges, Pustejovsky, &amp;amp; Shadish (
&lt;a href=&#34;/publication/SMD-for-SCD&#34;&gt;2012&lt;/a&gt;, 
&lt;a href=&#34;/publication/SMD-for-MBD&#34;&gt;2013&lt;/a&gt;) and 
&lt;a href=&#34;/publication/design-comparable-effect-sizes/&#34;&gt;Pustejovsky, Hedges, &amp;amp; Shadish (2014)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=scdhlm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/getting-started-with-scdhlm&#34;&gt;Installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/scdhlm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/scdhlm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scdhlm&lt;/a&gt;: An interactive web application for calculating design-comparable standardized mean difference effect sizes.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SingleCaseES</title>
      <link>/software/singlecasees/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/software/singlecasees/</guid>
      <description>&lt;p&gt;An R package for calculating basic effect size indices for single-case designs, including several non-overlap measures and parametric effect size measures, and for estimating the gradual effects model developed by Swan and Pustejovsky (2017).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=SingleCaseES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/SingleCaseES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code and installation instructions on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-sizes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Single case effect size calculator&lt;/a&gt;: An interactive web application for calculating basic effect size indices.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/gem-scd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradual Effect Model calculator&lt;/a&gt;: An interactive web application for estimating effect sizes using the gradual effects model.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tau-U</title>
      <link>/tau-u/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/tau-u/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber (2011)&lt;/a&gt; proposed Tau-U as an effect size measure for use in single-case designs that exhibit baseline trend. In their original paper, they actually conceptualize Tau-U as a family of four distinct indices, distinguished by a) whether the index includes an adjustment for the presence of baseline trend and b) whether the index incorporates information about trend during the intervention phase. However, in subsequent presentations the authors seem to have focused exclusively on the index that adjusts for baseline trend but not for intervention phase trend, and so I’ll do the same here. (This version is also the one available in the web-tool at &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;singlecaseresearch.org&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Tau-U is an elaboration on their previously proposed effect sizes &lt;a href=&#34;/NAP-SEs-and-CIs&#34;&gt;NAP and Tau&lt;/a&gt;, which do not account for baseline trends. The index is calculated as follows. Suppose that we have data from A and B phases from a single case, where the baseline phase has &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; observations and treatment phase has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Tau-U is then calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau-U} = \frac{S_P - S_B}{mn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; is Kendall’s S statistic calculated for the comparison between phases and &lt;span class=&#34;math inline&#34;&gt;\(S_B\)&lt;/span&gt; is Kendall’s S statistic calculated on the baseline trend. More precisely,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_P &amp;amp;= \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right] \\
S_B &amp;amp;= \sum_{i=1}^{m - 1} \sum_{j = i + 1}^m \left[I\left(y^A_j &amp;gt; y^A_i\right) - I\left(y^A_j &amp;lt; y^A_i\right)\right].
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the first term in Tau-U is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\text{Tau} = S_P / (m n)\)&lt;/span&gt;, which in turn is a re-scaling of NAP. The second term is related to the rank-correlation between the measurement occasions and outcomes in the baseline phase. Subtracting the second from the first thus adjusts for baseline trend, in the sense that more pronounced baseline trends will lead to smaller values of Tau-U. But looking at the measure a bit more deeply, it has some very odd features. In this post, I’ll show that the distribution of Tau-U is sensitive to the number of observations in each phase.&lt;/p&gt;
&lt;div id=&#34;sample-size-sensitivity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample size sensitivity&lt;/h2&gt;
&lt;p&gt;Consider first the logical range of Tau-U. The minimum and maximum possible values of &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(-m n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m n\)&lt;/span&gt;; the minimum and maximum of &lt;span class=&#34;math inline&#34;&gt;\(S_B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(-m (m-1) / 2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m (m - 1) / 2\)&lt;/span&gt;. Consequently, the logical range of Tau-U is from &lt;span class=&#34;math inline&#34;&gt;\(-(2n + m - 1) / (2n)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((2n + m - 1) / (2n)\)&lt;/span&gt;. If the treatment phase is quite long compared to the baseline phase, then this range will be close to [-1, 1]. On the other hand, in a study with a baseline that is twice as long as the treatment phase, the range of Tau-U will be closer to [-2, 2]. That’s a very odd property.&lt;/p&gt;
&lt;p&gt;The average magnitude of Tau-U is similarly influenced by the lengths of each phase. To see this, it’s helpful to think first about its target parameter–the quantity that is estimated when calculating Tau-U based on a sample of data. Since Tau-U is not defined in parametric terms, I will assume that the Tau-U statistic is an unbiased estimator of its target parameter &lt;span class=&#34;math inline&#34;&gt;\(\tau_U = \text{E}\left(\text{Tau-U}\right)\)&lt;/span&gt;. It follows that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tau_U = \tau_P - \frac{m - 1}{2n} \tau_B,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\tau_P\)&lt;/span&gt; is Kendall’s rank correlation between the outcomes and an indicator for the treatment phase and &lt;span class=&#34;math inline&#34;&gt;\(\tau_B\)&lt;/span&gt; is Kendall’s rank correlation between the measurement occasions and outcomes during baseline:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tau_P &amp;amp;= \frac{1}{mn}\sum_{i=1}^m \sum_{j=1}^n \left[\text{Pr}\left(Y^B_j &amp;gt; Y^A_i\right) - \text{Pr}\left(Y^B_j &amp;lt; Y^A_i\right)\right] \\
\tau_B &amp;amp;= \frac{2}{m(m-1)} \sum_{i=1}^{m - 1} \sum_{j = i + 1}^m \left[\text{Pr}\left(Y^A_j &amp;gt; Y^A_i\right) - \text{Pr}\left(Y^A_j &amp;lt; Y^A_i\right)\right].
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now consider a positive a baseline trend, so that &lt;span class=&#34;math inline&#34;&gt;\(\tau_B &amp;gt; 0\)&lt;/span&gt;, and assume that &lt;span class=&#34;math inline&#34;&gt;\(\tau_P\)&lt;/span&gt; is constant. A longer baseline phase will then lead to smaller values of Tau-U (on average), while a longer treatment phase will lead to larger values of Tau-U (on average). Again, that’s really weird. This is not a good feature for an effect size measure because it means that Tau-U values from different cases are only on the same scale if the cases have identical baseline and treatment phase lengths. In a multiple baseline study, each case is necessarily observed for a different number of occasions in baseline (otherwise it wouldn’t be a multiple baseline). Thus, it seems inadvisable to use Tau-U to quantify the magnitude of treatment effects in a multiple baseline study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sensitivity-under-a-parametric-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sensitivity under a parametric model&lt;/h2&gt;
&lt;p&gt;Things may be different if we allow for the magnitude of &lt;span class=&#34;math inline&#34;&gt;\(\tau_P\)&lt;/span&gt; to change along with the sample size. Such would be the case under a model where the intervention phase also exhibits a trend. For example, let’s suppose that the outcome follows a linear model with a non-zero trend and the intervention leads to an immediate shift in the outcome, as in the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y_t = \beta_0 + \beta_1 t + \beta_2 I(t &amp;gt; m) + \epsilon_t.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, I’ll assume that the errors in this model are normally distributed with unit variance. Under this model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tau_B &amp;amp;= \frac{4}{m (m - 1)} \left[\sum_{i=1}^{m-1} \sum_{j=i+1}^m \Phi\left[\beta_1\left(j - i\right) / \sqrt{2}\right]\right] - 1, \\
\tau_P &amp;amp;= \frac{2}{m n} \left[\sum_{i=1}^m \sum_{j=1}^n \Phi\left[\left(\beta_1 (m + j - i) + \beta_2\right) / \sqrt{2}\right]\right] - 1,
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Phi()\)&lt;/span&gt; is the standard normal cumulative distribution function. I can use the above formulas to calculate the average value of Tau-U for various degrees of baseline trend &lt;span class=&#34;math inline&#34;&gt;\((\beta_1)\)&lt;/span&gt;, level shift &lt;span class=&#34;math inline&#34;&gt;\((\beta_2)\)&lt;/span&gt;, and phase lengths &lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;E_TauU &amp;lt;- function(b1, b2, m, n) {
  tau_B &amp;lt;- sum(sapply(1:(m - 1), function(i) 
    sum(pnorm(b1 * ((i+1):m - i) / sqrt(2))))) * 4 / (m * (m - 1)) - 1
  tau_P &amp;lt;- sum(sapply(1:m, function(i) 
    sum(pnorm((b1 * (m + 1:n - i) + b2) / sqrt(2))))) * 2 / (m * n) - 1
  tau_P - tau_B * (m - 1) / (2 * n)
}

library(dplyr)
library(tidyr)
b1 &amp;lt;- c(-0.2, -0.1, 0, 0.1, 0.2)
b2 &amp;lt;- c(0, 0.5, 1.0, 2.0)
m &amp;lt;- c(5, 10, 15, 20)
n &amp;lt;- 5:20

expand.grid(b1 = b1, b2 = b2, m = m, n = n) %&amp;gt;%
  group_by(b1, b2, m, n) %&amp;gt;% 
  mutate(TauU = E_TauU(b1, b2, m, n)) -&amp;gt;
  TauU_values
ex &amp;lt;- filter(TauU_values, b1 == -0.2 &amp;amp; b2 == 0)


library(ggplot2)
ggplot(TauU_values, aes(n, TauU, color = factor(m))) + 
  facet_grid(b1 ~ b2, labeller = &amp;quot;label_both&amp;quot;) + 
  geom_line() + 
  labs(y = &amp;quot;Expected magnitude of Tau-U&amp;quot;, color = &amp;quot;m&amp;quot;) + 
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Tau-U_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, each plot corresponds to a different value of the baseline slope (&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, ranging from -0.2 in the top row to 0.2 in the bottom row) and treatment shift (&lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;, ranging from 0 in the first column to 2 in the last column). Within each plot, the x axis corresponds to treatment phase length and the different lines correspond to different baseline phase lengths. The thing to note is that, when the baseline slope is non-zero, the expected value of Tau-U ranges quite widely within each plot, depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. For example, when &lt;span class=&#34;math inline&#34;&gt;\(\beta_2 = 0\)&lt;/span&gt; (in the first column), the data follow a simple linear trend with no shift. If the slope of the trend is equal to -0.2 (the first row), then the expected magnitude of Tau-U ranges from -0.8 to 0.3 depending on the phase lengths, which is quite a wide range.&lt;/p&gt;
&lt;p&gt;Generally, the degree of sample size sensitivity depends on the absolute magnitude of the baseline slope, with steeper slopes leading to increased sensitivity. For steeper values of slope, it appears that the degree to which the measure is affected by sample size even swamps the degree to which the measure is sensitive to the magnitude of the treatment effect. Very peculiar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-final-thought&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A final thought&lt;/h2&gt;
&lt;p&gt;Of course, these results are contingent on the particular model under which I derived the expected magnitude of Tau-U. If the data followed some other model, such as a log-linear model with Poisson-distributed outcomes, then the behavior described above might change. Still, I think all of this raises the reasonable question: under what model (i.e., what sort of patterns of baseline trend, what sort of patterns of response to the intervention) does Tau-U provide a meaningful effect size measure that clearly quantifies the magnitude of treatment effects without being strongly affected by phase lengths? Unless and until such a model can be identified, I would be wary of interpreting Tau-U as a measure of treatment effect magnitude.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Standard errors and confidence intervals for NAP</title>
      <link>/nap-ses-and-cis/</link>
      <pubDate>Sun, 28 Feb 2016 00:00:00 +0000</pubDate>
      <guid>/nap-ses-and-cis/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1016/j.beth.2008.10.006&#34;&gt;Parker and Vannest (2009)&lt;/a&gt; proposed non-overlap of all pairs (NAP) as an effect size index for use in single-case research. NAP is defined in terms of all pair-wise comparisons between the data points in two different phases for a given case (i.e., a treatment phase versus a baseline phase). For an outcome that is desirable to increase, NAP is the proportion of all such pair-wise comparisons where the treatment phase observation exceeds the baseline phase observation, with pairs that are exactly tied getting a weight of 1/2. NAP belongs to the family of non-overlap measures, which also includes the percentage of non-overlapping data, the improvement rate difference, and several other indices. It is exactly equivalent to &lt;a href=&#34;http://doi.org/10.2307/1165329&#34;&gt;Vargha and Delaney’s (2000)&lt;/a&gt; modified Common Language Effect Size and has been proposed as an effect size index in other contexts too (e.g., &lt;a href=&#34;http://doi.org/10.1002/sim.2256&#34;&gt;Acion, Peterson, Temple, &amp;amp; Arndt, 2006&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The developers of NAP have created a &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;web-based tool&lt;/a&gt; for calculating it (as well as several other non-overlap indices), and I have the impression that the tool is fairly widely used. For example, &lt;a href=&#34;http://doi.org/10.1007%2Fs10864-013-9189-x&#34;&gt;Roth, Gillis, and DiGennaro Reed (2014)&lt;/a&gt; and &lt;a href=&#34;http://doi.org/10.1007/s10803-015-2373-1&#34;&gt;Whalon, Conroy, Martinez, and Welch (2015)&lt;/a&gt; both used NAP in their meta-analyses of single-case research, and both noted that they used &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; for calculating the effect size measure. Given that the web tool is being used, it is worth scrutinizing the methods behind the calculations it reports. As of this writing, the standard error and confidence intervals reported along with the NAP statistic are incorrect, and should not be used. After introducing a bit of notation, I’ll explain why the existing methods are deficient. I’ll also suggest some methods for calculating standard errors and confidence intervals that are potentially more accurate.&lt;/p&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;Suppose that we have data from the baseline phase and treatment phase for a single case. Let &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; denote the number of baseline observations and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; denote the number of treatment phase observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Then NAP is calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{NAP} = \frac{1}{m n} \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What is NAP an estimate of? The parameter of interest is the probability that a randomly selected treatment phase observation will exceed a randomly selected baseline phase observation (again, with an adjustment for ties):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \text{Pr}(Y^B &amp;gt; Y^A) + 0.5 \text{Pr}(Y^B = Y^A).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Vargha and Delaney call &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; the &lt;em&gt;measure of stochastic superiority&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;NAP is very closely related to another non-overlap index called Tau (&lt;a href=&#34;http://doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, &amp;amp; Sauber, 2011&lt;/a&gt;). Tau is nothing more than a linear re-scaling of NAP to the range of [-1, 1]:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau} = \frac{S}{m n} = 2 \times \text{NAP} - 1,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is Kendall’s S statistic, which is closely related to the Mann-Whitney &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; test.&lt;/p&gt;
&lt;p&gt;Here is an R function for calculating NAP:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NAP &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sum(sapply(yA, function(i) sapply(yB, function(j) (j &amp;gt; i) + 0.5 * (j == i))))
  U / (m * n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the data from the worked example in &lt;a href=&#34;http://doi.org/10.1016/j.beth.2008.10.006&#34;&gt;Parker and Vannest (2009)&lt;/a&gt;, the function result agrees with their reported NAP of 0.96:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yA &amp;lt;- c(4, 3, 4, 3, 4, 7, 5, 2, 3, 2)
yB &amp;lt;- c(5, 9, 7, 9, 7, 5, 9, 11, 11, 10, 9)
NAP(yA, yB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9636364&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard errors&lt;/h2&gt;
&lt;p&gt;The webtool at &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; reports a standard error for NAP (it is labelled as “SDnap”), which from what I can tell is based on the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SE}_{\text{Tau}} = \sqrt{\frac{m + n + 1}{3 m n}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula appears to actually be the standard error for Tau, rather than for NAP. Since &lt;span class=&#34;math inline&#34;&gt;\(\text{NAP} = \left(\text{Tau} + 1\right) / 2\)&lt;/span&gt;, the standard error for NAP should be half as large:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SE}_{null} = \sqrt{\frac{m + n + 1}{12 m n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(cf. &lt;a href=&#34;http://dx.doi.org/10.1037/1082-989X.6.2.135&#34;&gt;Grissom &amp;amp; Kim, 2001, p. 141&lt;/a&gt;). However, even the latter formula is not always correct. It is valid only when the observations are all mutually independent and when the treatment phase data are drawn from the same distribution as the baseline phase data—that is, when the treatment has no effect on the outcome. I’ve therefore denoted it as &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;other-standard-error-estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other standard error estimators&lt;/h3&gt;
&lt;p&gt;Because an equivalent effect size measure is used in other contexts like clinical medicine, there has actually been a fair bit of research into better approaches for assessing the uncertainty in NAP. &lt;a href=&#34;http://dx.doi.org/10.1148/radiology.143.1.7063747&#34;&gt;Hanley and McNeil (1982)&lt;/a&gt; proposed an estimator for the sampling variance of NAP that is designed for continuous outcome measures, where exact ties are impossible. Modifying it slightly (and in entirely ad hoc fashion) to account for ties, let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Q_1 &amp;amp;= \frac{1}{m n^2}\sum_{i=1}^m \left[\sum_{j=1}^n I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]^2 \\
Q_2 &amp;amp;= \frac{1}{m^2 n}\sum_{j=1}^n \left[\sum_{i=1}^m I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]^2.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then the Hanley-McNeil variance estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{HM} = \frac{1}{mn} \left[\text{NAP}\left(1 - \text{NAP}\right) + (n - 1)\left(Q_1 - \text{NAP}^2\right) + (m - 1)\left(Q_2 - \text{NAP}^2\right)\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM} = \sqrt{V_{HM}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The same authors also propose a different estimator, which is based on the assumption that the outcome data are exponentially distributed. Even though this is a strong and often inappropriate assumption, there is evidence that this estimator works even for other, non-exponential distributions. &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; suggested a further modification of their estimator, and I’ll describe his version. Let &lt;span class=&#34;math inline&#34;&gt;\(h = (m + n) / 2 - 1\)&lt;/span&gt;. Then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{New} = \frac{h}{mn} \text{NAP}\left(1 - \text{NAP}\right)\left[\frac{1}{h} + \frac{1 - \text{NAP}}{2 - \text{NAP}} + \frac{\text{NAP}}{1 + \text{NAP}}\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New} = \sqrt{V_{New}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here are R functions to calculate each of these variance estimators.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V_HM &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sapply(yB, function(j) (j &amp;gt; yA) + 0.5 * (j == yA))
  t &amp;lt;- sum(U) / (m * n)
  Q1 &amp;lt;- sum(rowSums(U)^2) / (m * n^2)
  Q2 &amp;lt;- sum(colSums(U)^2) / (m^2 * n)
  (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)
}

V_New &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  t &amp;lt;- NAP(yA, yB)
  h &amp;lt;- (m + n) / 2 - 1
  t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)
}

sqrt(V_HM(yA, yB))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03483351&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(V_New(yA, yB))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04370206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the worked example dataset from Parker and Vannest, the Newcombe estimator yields a standard error that is about 25% larger than the Hanley-McNeil estimator. Both of these are substantially smaller than the null standard error, which in this example is &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null} = 0.129\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-small-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A small simulation&lt;/h3&gt;
&lt;p&gt;Simulation methods can be used to examine how well these various standard error formulas estimate the actual sampling variation of NAP. For simplicity, I’ll simulate normally distributed data where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^A \sim N(0, 1) \qquad \text{and} \qquad Y^B \sim N\left(\sqrt{2}\Phi^{-1}(\theta), 1\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for varying values of the effect size estimand (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) and a couple of different sample sizes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_NAP &amp;lt;- function(delta, m, n, iterations) {
  NAPs &amp;lt;- replicate(iterations, {
    yA &amp;lt;- rnorm(m)
    yB &amp;lt;- rnorm(n, mean = delta)
    c(NAP = NAP(yA, yB), V_HM = V_HM(yA, yB), V_New = V_New(yA, yB))
  })
  data.frame(sd = sd(NAPs[&amp;quot;NAP&amp;quot;,]), 
             SE_HM = sqrt(mean(NAPs[&amp;quot;V_HM&amp;quot;,])), 
             SE_New = sqrt(mean(NAPs[&amp;quot;V_New&amp;quot;,])))
}

library(dplyr)
library(tidyr)
theta &amp;lt;- seq(0.5, 0.95, 0.05)
m &amp;lt;- c(5, 10, 15, 20, 30)
n &amp;lt;- c(5, 10, 15, 20, 30)

expand.grid(theta = theta, m = m, n = n) %&amp;gt;%
  group_by(theta, m, n) %&amp;gt;% 
  mutate(delta = sqrt(2) * qnorm(theta)) -&amp;gt;
  params 

params %&amp;gt;%
  do(sample_NAP(.$delta, .$m, .$n, iterations = 2000)) %&amp;gt;%
  mutate(se_null = sqrt((m + n + 1) / (12 * m * n))) %&amp;gt;%
  gather(&amp;quot;sd&amp;quot;,&amp;quot;val&amp;quot;, sd, SE_HM, SE_New, se_null) -&amp;gt;
  NAP_sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(NAP_sim, aes(theta, val, color = sd)) + 
  facet_grid(n ~ m, labeller = &amp;quot;label_both&amp;quot;) + 
  geom_line() + 
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/NAP-SEs-and-CIs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above figure, the actual sampling standard deviation of NAP (in red) and the value of &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt; (in purple) are plotted against the true value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, with separate plots for various combinations of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The expected value of the standard errors &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New}\)&lt;/span&gt; (actually the square root of the expectation of the variance estimators) are depicted in green and blue, respectively. The value of &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt; agrees with the actual standard error when &lt;span class=&#34;math inline&#34;&gt;\(\delta = 0\)&lt;/span&gt;, but the two diverge when there is a positive treatment effect. It appears that &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New}\)&lt;/span&gt; both under-estimate the actual standard error when &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is equal to 5, and over-estimate for the largest values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. However, both of these estimators offer a marked improvement over &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confidence intervals&lt;/h2&gt;
&lt;p&gt;The webtool at &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; also reports 85% and 90% confidence intervals for NAP. These confidence intervals appear to have the same two problems as the standard errors. First, they are constructed as CIs for Tau rather than for NAP. For the &lt;span class=&#34;math inline&#34;&gt;\(100\% \times (1 - \alpha)\)&lt;/span&gt; CI, let &lt;span class=&#34;math inline&#34;&gt;\(z_{\alpha / 2}\)&lt;/span&gt; be the appropriate critical value from a standard normal distribution. The CIs reported by the webtool are given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau} \pm \text{SE}_{\text{Tau}} \times z_{\alpha / 2}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is probably just an oversight in the programming, which could be corrected by instead using&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{NAP} \pm \text{SE}_{null} \times z_{\alpha / 2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In parallel with the standard error formulas, I’ll call this formula the null confidence interval. Funnily enough, the upper bound of the null CI is the same as the upper bound of the Tau CI. However, the lower bound is going to be quite a bit larger than the lower bound for the Tau CI, so that the null CI will be much narrower.&lt;/p&gt;
&lt;p&gt;The second problem is that even the null CI has poor coverage properties because it is based on &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;, which can drastically over-estimate the standard error of NAP for non-null values.&lt;/p&gt;
&lt;div id=&#34;other-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other confidence intervals&lt;/h3&gt;
&lt;p&gt;As I noted above, there has been a fair amount of previous research into how to construct CIs for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the parameter estimated by NAP. As is often the case with these sorts of problems, there are many different methods available, scattered across the literature. Fortunately, there are two (at least) fairly comprehensive simulation studies that compare the performance of various methods under a wide range of conditions. &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; examined a range of methods based on inverting Wald-type test statistics (which give CIs of the form &lt;span class=&#34;math inline&#34;&gt;\(\text{estimate} \pm \text{SE} \times z_{\alpha / 2}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}\)&lt;/span&gt; is some standard error estimate) and score-based methods (in which the standard error is estimated using the candidate parameter value). Based on an extensive simulation, he suggested a score-based method in which the end-points of the CI are defined the values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that satisfy:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\text{NAP} - \theta)^2 = \frac{z^2_{\alpha / 2} h \theta (1 - \theta)}{mn}\left[\frac{1}{h} + \frac{1 - \theta}{2 - \theta} + \frac{\theta}{1 + \theta}\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(h = (m + n) / 2 - 1\)&lt;/span&gt;. This equation is a fourth-degree polynomial in &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, easily solved using a numerical root-finding algorithm.&lt;/p&gt;
&lt;p&gt;In a different simulation study, &lt;a href=&#34;http://dx.doi.org/10.1080/00273171.2012.658329&#34;&gt;Ruscio and Mullen (2012)&lt;/a&gt; examined the performance of a selection of different confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, including several methods not considered by Newcombe. Among the methods that they examined, they find that the bias-corrected, accelerated (BCa) bootstrap CI performs particularly well (and seems to outperform the score-based CI recommended by Newcombe).&lt;/p&gt;
&lt;p&gt;Neither &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; nor &lt;a href=&#34;http://dx.doi.org/10.1080/00273171.2012.658329&#34;&gt;Ruscio and Mullen (2012)&lt;/a&gt; considered constructing a confidence interval by directly pivoting the Mann-Whitney U test (the same technique used to construct confidence intervals for the Hodges-Lehmann estimator of location shift), although it seems to me that this would be possible and potentially an attractive approach in the context of SCDs. The main caveat is that such a CI would require stronger distributional assumptions than those studied in the simulations, such as that the distributions of &lt;span class=&#34;math inline&#34;&gt;\(Y^A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y^B\)&lt;/span&gt; differ by an additive (or multiplicative) constant. In any case, it seems like it would be worth exploring this approach too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-small-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Another small simulation&lt;/h3&gt;
&lt;p&gt;Here is an R function for calculating several different CIs for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, including the null CI, Wald-type CIs based on &lt;span class=&#34;math inline&#34;&gt;\(V_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_{New}\)&lt;/span&gt;, and the score-type CI recommended by &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt;. I haven’t programmed the BCa bootstrap because it would take a bit more thought to figure out how to simulate it efficiently.&lt;/p&gt;
&lt;p&gt;The following code simulates the coverage rates of nominal 90% CIs based on each of these methods, following the same simulation set-up as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NAP_CIs &amp;lt;- function(yA, yB, alpha = .05) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sapply(yB, function(j) (j &amp;gt; yA) + 0.5 * (j == yA))
  t &amp;lt;- sum(U) / (m * n)
  
  # variance estimators
  V_null &amp;lt;- (m + n + 1) / (12 * m * n)
  
  Q1 &amp;lt;- sum(rowSums(U)^2) / (m * n^2)
  Q2 &amp;lt;- sum(colSums(U)^2) / (m^2 * n)
  V_HM &amp;lt;- (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)
  
  h &amp;lt;- (m + n) / 2 - 1
  V_New &amp;lt;- t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)
  
  # Wald-type confidence intervals
  z &amp;lt;- qnorm(1 - alpha / 2)
  SEs &amp;lt;- sqrt(c(null = V_null, HM = V_HM, Newcombe = V_New))
  Wald_lower &amp;lt;- t - z * SEs
  Wald_upper &amp;lt;- t + z * SEs
  
  # score-type confidence interval
  f &amp;lt;- function(x) m * n * (t - x)^2 * (2 - x) * (1 + x) - 
    z^2 * x * (1 - x) * (2 + h + (1 + 2 * h) * x * (1 - x))
  score_lower &amp;lt;- if (t &amp;gt; 0) uniroot(f, c(0, t))$root else 0
  score_upper &amp;lt;- if (t &amp;lt; 1) uniroot(f, c(t, 1))$root else 1
  list(NAP = t, 
       CI = data.frame(lower = c(Wald_lower, score = score_lower), 
                       upper = c(Wald_upper, score = score_upper)))
}

NAP_CIs(yA, yB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $NAP
## [1] 0.9636364
## 
## $CI
##              lower     upper
## null     0.7106061 1.2166666
## HM       0.8953639 1.0319088
## Newcombe 0.8779819 1.0492908
## score    0.7499741 0.9950729&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_CIs &amp;lt;- function(delta, m, n, alpha = .05, iterations) {
  NAPs &amp;lt;- replicate(iterations, {
    yA &amp;lt;- rnorm(m)
    yB &amp;lt;- rnorm(n, mean = delta)
    NAP_CIs(yA, yB, alpha = alpha)
  }, simplify = FALSE)
  theta &amp;lt;- mean(sapply(NAPs, function(x) x$NAP))
  coverage &amp;lt;- rowMeans(sapply(NAPs, function(x) (x$CI$lower &amp;lt; theta) &amp;amp; (theta &amp;lt; x$CI$upper)))
  data.frame(CI = rownames(NAPs[[1]]$CI), coverage = coverage)
}

params %&amp;gt;% 
  do(sample_CIs(delta = .$delta, m = .$m, n = .$n, alpha = .10, iterations = 5000)) -&amp;gt;
  NAP_CI_sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(NAP_CI_sim, aes(theta, coverage, color = CI)) + 
  facet_grid(n ~ m, labeller = &amp;quot;label_both&amp;quot;, scales = &amp;quot;free_y&amp;quot;) + 
  geom_line() + 
  labs(y = &amp;quot;SE&amp;quot;) + 
  geom_hline(yintercept=.90, linetype=&amp;quot;dashed&amp;quot;) +
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/NAP-SEs-and-CIs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure above plots the coverage rates of several different confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;: the naive CI (in blue), the HM Wald CI (red), the Newcombe Wald CI (green), and the Newcombe score CI (purple). The dashed horizontal line is the nominal coverage rate of 90%. It can be seen that the null CI has the correct coverage only when &lt;span class=&#34;math inline&#34;&gt;\(\theta \leq .6\)&lt;/span&gt;; for larger values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, its coverage becomes too conservative (tending towards 100%). The Wald-type CIs have below-nominal coverage rates, which improve as the sample size in each phase increases but remain too liberal even at the largest sample size considered. Finally, Newcombe’s score CI maintains close-to-nominal coverage over a wider range of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values. Although these CIs have below-nominal coverage for the smallest sample sizes, they generally have good coverage for &lt;span class=&#34;math inline&#34;&gt;\(\theta &amp;lt; .9\)&lt;/span&gt; and when the sample size in each phase is 10 or more. It is also notable that their coverage rates appear to become more accurate as the sample size in a given group increases, even if the sample size in the other group is fairly small and remains constant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;My aim in this post was to highlight the problems with how &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; calculates standard errors and CIs for the NAP statistic. Some of these issues could easily be resolved by correcting the relevant formulas so that they are appropriate for NAP rather than Tau. However, even with these corrections, better approaches exist for calculating standard errors and CIs. I’ve highlighted some promising ones above, which seem worthy of further investigation. But I should also emphasize that these methods do come with some important caveats too.&lt;/p&gt;
&lt;p&gt;First, all of the methods I’ve discussed are premised on having mutually independent observations. In the presence of serial correlation, I would anticipate that any of these standard errors will be too small and any of the confidence intervals will be too narrow. (This could readily be verified through simulation, although I have not done so here.)&lt;/p&gt;
&lt;p&gt;Second, my small simulations are based on the assumption of normally distributed, homoskedastic observations in each phase, which is not a particularly good model for the types of outcome measures commonly used in single case research. In some of my other work, I’ve developed statistical models for data collected by systematic direct observation of behavior, which is the most prevalent type of outcome data in single-case research. Before recommending any particular method, the performance of the standard error formulas (e.g., the Hanley-McNeil and Newcombe estimators) and CI methods (such as Newcombe’s score CI) should be examined under more realistic models for behavioral observation data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Special Education Pro-Sem</title>
      <link>/sped-pro-sem-again/</link>
      <pubDate>Tue, 24 Nov 2015 00:00:00 +0000</pubDate>
      <guid>/sped-pro-sem-again/</guid>
      <description>


&lt;p&gt;Yesterday evening I again had the pleasure of visiting Dr. Barnes’ pro seminar for first year students in Special Education, where I shared some of my work on research synthesis and meta-analysis of single-case research. &lt;a href=&#34;/files/Barnes-Pro-Sem-2015-11.pdf&#34;&gt;Here are the slides&lt;/a&gt; from my presentation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correlations between standardized mean differences</title>
      <link>/correlations-between-smds/</link>
      <pubDate>Thu, 17 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/correlations-between-smds/</guid>
      <description>


&lt;p&gt;Several students and colleagues have asked me recently about an issue that comes up in multivariate meta-analysis when some of the studies include multiple treatment groups and multiple outcome measures. In this situation, one might want to include effect size estimates for each treatment group and each outcome measure. In order to do so in fully multivariate meta-analysis, estimates of the covariances among all of these efffect sizes are needed. The covariance among effect sizes arises for several reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For a single outcome measure, effect sizes based on different treatment groups compared to a common control group will be correlated because the same control group data is used to calculate both effect sizes;&lt;/li&gt;
&lt;li&gt;Effect sizes based on a single treatment group and a single control group, but for different outcome measures, will be correlated because the outcomes are measured on the same set of units (in both the treatment group and the control group).&lt;/li&gt;
&lt;li&gt;Effect sizes based on different treatment groups and for different outcome measures will be correlated because the outcomes are measured on the same set of units in the control group (though not in the treatment group).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For standardized mean difference (SMD) measures of effect size, formulas for the covariance are readily available for the first two cases (see e.g., Gleser &amp;amp; Olkin, 2009), but not for the third case. Below I review the formulas for the covariance between SMDs in the first two cases and provide a formula for the third case.&lt;/p&gt;
&lt;div id=&#34;notation-and-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Notation and Model&lt;/h1&gt;
&lt;p&gt;Suppose that the experiment has a control group that includes &lt;span class=&#34;math inline&#34;&gt;\(n_0\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; treatment groups that include &lt;span class=&#34;math inline&#34;&gt;\(n_1,...,n_T\)&lt;/span&gt; units, respectively. Also suppose that &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; outcome measures are made on each unit in each group. The formulas below assume that the data follow a one-way MANOVA model. Let &lt;span class=&#34;math inline&#34;&gt;\(y_{ijt}\)&lt;/span&gt; denote the score for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in group &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Then I assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijt} = \mu_{jt} + \epsilon_{ijt},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the errors are multi-variate normally distributed with mean zero, variance that can differ across outcome but not across treatment group, and correlation that is constant across treatment groups, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(\epsilon_{ijt}\right) = \sigma^2_j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}\left(\epsilon_{ijt}, \epsilon_{ikt} \right) = \rho_{jk}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Denote the mean score on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in group &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{jt}\)&lt;/span&gt; and the standard deviation of the scores on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in group &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(s_{jt}\)&lt;/span&gt;, both for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t = 0,...,T\)&lt;/span&gt; (with &lt;span class=&#34;math inline&#34;&gt;\(t = 0\)&lt;/span&gt; corresponding to the control group). Also required are estimates of the correlations among outcome measures 1 through &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;, after partialling out differences between treatment groups. Let &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; denote the partial correlation between measure &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and measure &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J - 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = j + 1,...,J\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With multiple treatment groups, one might wonder how best to compute the standard deviation for purposes of scaling the treatment effect estimates. In their discussion of SMDs from multiple treatment studies, Gleser and Olkin (2009) assume (though they don’t actually state outright) that the standard deviation will be pooled across all &lt;span class=&#34;math inline&#34;&gt;\(T + 1\)&lt;/span&gt; groups. The pooled standard deviation for outcome &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is calculated as the square root of the pooled variance,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s_{jP}^2 = \frac{1}{N - T - 1} \sum_{t=0}^T (n_t - 1)s_{jt}^2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{t=0}^T n_t\)&lt;/span&gt;. The standardized mean difference for treatment &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is then estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d_{jt} = \frac{\bar{y}_{jt} - \bar{y}_{j0}}{s_{jP}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t = 1,...,T\)&lt;/span&gt;. The conventional estimate of the large-sample variance of &lt;span class=&#34;math inline&#34;&gt;\(d_{jt}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Var}(d_{jt}) \approx \frac{1}{n_0} + \frac{1}{n_t} + \frac{d_{jt}^2}{2 (N - T - 1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covariances&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Covariances&lt;/h1&gt;
&lt;p&gt;For SMDs based on a common outcome measure and a common control group, but different treatment groups, the large-sample covariance between the effect size estimates can be estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{ju}) \approx \frac{1}{n_0} + \frac{d_{jt} d_{ju}}{2 (N - T - 1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above differs slightly from Gleser and Olkin (2009, Formula 19.19) because it uses the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(N - T - 1\)&lt;/span&gt; in the denominator of the second term, rather than the total sample size. If the total sample size is larger relative to the number of treatment groups, the discrepancy should be minor.&lt;/p&gt;
&lt;p&gt;SMDs based on a single treatment group but for different outcome measures follow a structure that is essentially equivalent to what Gleser and Olkin (2009) call a “multiple-endpoint” study. The large-sample covariance between the effect size estimates can be estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{kt}) \approx r_{jk} \left(\frac{1}{n_0} + \frac{1}{n_t}\right) + \frac{r_{jk}^2 d_{jt} d_{kt}}{2 (N - T - 1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(cf. Gleser &amp;amp; Olkin, 2009, Formula 19.19). Note that if the degrees of freedom are large relative to &lt;span class=&#34;math inline&#34;&gt;\(d_{jt}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_{kt}\)&lt;/span&gt;, then the correlation between the effect sizes will be approximately equal to &lt;span class=&#34;math inline&#34;&gt;\(\text{Cor}(d_{jt},d_{kt}) \approx r_{jk}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the large-sample covariance between SMDs based on different treatment groups and different outcome measures can be estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{ku}) \approx \frac{r_{jk}}{n_0} + \frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is similar to the previous formula, but does not include the term corresponding to the covariance between different outcome measures in a common treatment group.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(r_{jj} = 1\)&lt;/span&gt; is used for the correlation of an outcome measure with itself, all of the above formulas (including the variance of &lt;span class=&#34;math inline&#34;&gt;\(d_{jt}\)&lt;/span&gt;) can be expressed compactly as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{ku}) \approx r_{jk} \left(\frac{1}{n_0} + \frac{I(t = u)}{n_t}\right) + \frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(A)\)&lt;/span&gt; is equal to one if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is true and equal to zero otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Gleser, L. J., &amp;amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp;amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357-376). New York, NY: Russell Sage Foundation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Measurement-comparable effect sizes for single-case studies of free-operant behavior</title>
      <link>/publication/measurement-comparable-effect-sizes/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/measurement-comparable-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Four methods for analyzing partial interval recording data, with application to single-case research</title>
      <link>/publication/four-methods-for-pir/</link>
      <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
      <guid>/publication/four-methods-for-pir/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Design-comparable effect sizes in multiple baseline designs: A general modeling framework</title>
      <link>/publication/design-comparable-effect-sizes/</link>
      <pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate>
      <guid>/publication/design-comparable-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework</title>
      <link>/design-comparable-effect-sizes-in-multiple-baseline-designs/</link>
      <pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate>
      <guid>/design-comparable-effect-sizes-in-multiple-baseline-designs/</guid>
      <description>


&lt;p&gt;My article with Larry Hedges and Will Shadish, titled “Design-comparable effect sizes in multiple baseline designs: A general modeling framework” has been accepted at Journal of Educational and Behavioral Statistics. The abstract is below. Here’s the article at &lt;a href=&#34;http://doi.org/10.3102/1076998614547577&#34;&gt;the journal website&lt;/a&gt;. &lt;a href=&#34;/files/Effect-sizes-in-multiple-baseline-designs-JEBS.pdf&#34;&gt;Postprint&lt;/a&gt; and &lt;a href=&#34;/files/Effect-sizes-in-multiple-baseline-designs-Simulation-results.pdf&#34;&gt;supporting materials&lt;/a&gt; are available. An R package that implements the proposed methods is &lt;a href=&#34;/getting-started-with-scdhlm/&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general approach for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small-sample correction analogous to Hedges’ g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses</title>
      <link>/publication/analyzing-scd-hopes-and-fears/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      <guid>/publication/analyzing-scd-hopes-and-fears/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Special Education Pro-Sem</title>
      <link>/sped-pro-sem/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      <guid>/sped-pro-sem/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://www.edb.utexas.edu/education/departments/sped/about/fac_dir/barnes/&#34;&gt;Dr. Marcia Barnes&lt;/a&gt; from the department of Special Education invited me to visit her pro-seminar this afternoon and talk about some of my work on meta-analytic methods for single-case research. Thanks very much to the students for asking such thoughtful and engaging questions. &lt;a href=&#34;/files/Barnes-Pro-Sem-2014-04-10.pdf&#34;&gt;Here are the slides&lt;/a&gt;, which include some additional material that we didn’t get to talk about.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications</title>
      <link>/publication/bc-smd-primer-and-applications/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      <guid>/publication/bc-smd-primer-and-applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control</title>
      <link>/publication/converting-from-d-to-r-to-z/</link>
      <pubDate>Sat, 01 Mar 2014 00:00:00 +0000</pubDate>
      <guid>/publication/converting-from-d-to-r-to-z/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New article: Measurement-comparable effect sizes for single-case studies of free-operant behavior</title>
      <link>/measurement-comparable-effect-sizes/</link>
      <pubDate>Tue, 04 Feb 2014 00:00:00 +0000</pubDate>
      <guid>/measurement-comparable-effect-sizes/</guid>
      <description>


&lt;p&gt;My article “Measurement-comparable effect sizes for single-case studies of free-operant behavior” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;Postprint&lt;/a&gt; and &lt;a href=&#34;/files/Measuerment-comparable-ES-Appendix.pdf&#34;&gt;supporting materials&lt;/a&gt; are available. Here’s the abstract:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic technique for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by two examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A standardized mean difference effect size for multiple baseline designs</title>
      <link>/publication/smd-for-mbd/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 +0000</pubDate>
      <guid>/publication/smd-for-mbd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics</title>
      <link>/publication/between-groups-d-statistic/</link>
      <pubDate>Thu, 18 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/between-groups-d-statistic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Operationally comparable effect sizes for meta-analysis of single-case research</title>
      <link>/publication/operationally-comparable-effect-sizes/</link>
      <pubDate>Sat, 01 Jun 2013 00:00:00 +0000</pubDate>
      <guid>/publication/operationally-comparable-effect-sizes/</guid>
      <description>&lt;p&gt;This thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate. I argue that operational comparability, or invariance to heterogeneous operational procedures, is crucial property for an effect size metric. I then consider two problems with operational comparability that arise in single-case research. The first problem is to find effect sizes that can be applied across studies that use different research designs, such as single-case designs and two-group randomized experiments. The second problem is to find effect sizes that can be applied across studies that use varied operations for measuring the same construct. To address each of these problems, I propose structural models that capture essential features of multiple relevant operations (either design-related operations or measurement-related operations). I then use these structural models to precisely define target effect size parameters and to consider identification issues and estimation strategies.&lt;/p&gt;
&lt;p&gt;Chapter 1 defines operational comparability and situates the concept within the broad methodological concerns of meta-analysis, then reviews relevant features of single-case research and previously proposed effect sizes. Chapter 2 describes an abstract set of modeling criteria for constructing design-comparable effect sizes. Chapters 3 applies the general criteria to the case of standardized mean differences and proposes an effect size estimator based on restricted maximum likelihood. Chapter 4 presents several applications of the proposed models and methods. Chapter 5 proposes measurement-comparability model and defines effect size measures for use in studies of free-operant behavior, one of the most common classes of outcomes in single-case research. Chapter 6 extends the proposed effect size models to incorporate more complex features, including time trends and serial dependence, and studies a method of estimating those models through a combination of marginal quasi-likelihood and Gaussian pseudo-likelihood estimating equations. Chapter 7 collects various further extensions, areas for further research, and concluding thoughts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A standardized mean difference effect size for single case designs</title>
      <link>/publication/smd-for-scd/</link>
      <pubDate>Tue, 14 Aug 2012 00:00:00 +0000</pubDate>
      <guid>/publication/smd-for-scd/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
