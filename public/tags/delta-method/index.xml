<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>delta method | James E. Pustejovsky</title>
    <link>/tags/delta-method/</link>
      <atom:link href="/tags/delta-method/index.xml" rel="self" type="application/rss+xml" />
    <description>delta method</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020</copyright><lastBuildDate>Thu, 19 Apr 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>delta method</title>
      <link>/tags/delta-method/</link>
    </image>
    
    <item>
      <title>Sampling variance of Pearson r in a two-level design</title>
      <link>/variance-of-r-in-two-level-design/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/variance-of-r-in-two-level-design/</guid>
      <description>


&lt;p&gt;Consider Pearson’s correlation coefficient, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, calculated from two variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; with population correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. If one calculates &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observations, then its sampling variance will be approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(r) \approx \frac{1}{N}\left(1 - \rho^2\right)^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; will actually be a weighted average of within-cluster and between-cluster correlations (see Snijders &amp;amp; Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from a multi-stage sample would not necessarily be the same as that from a simple random sample. What is the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; in this design?&lt;/p&gt;
&lt;p&gt;Let me be more precise here by formalizing the sampling process. Suppose that we have a sample with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; clusters, &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; observations in cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, and total sample size &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{j=1}^m n_j\)&lt;/span&gt;. Assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X_{ij} &amp;amp;= \mu_x + v^x_j + e^x_{ij} \\
Y_{ij} &amp;amp;= \mu_y + v^y_j + e^y_{ij},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=1,...,n_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j=1,...,m\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left[\begin{array}{c} v^x_j \\ v^y_j \end{array}\right] &amp;amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\omega_x^2 &amp;amp; \phi \omega_x \omega_y \\ \phi \omega_x \omega_y &amp;amp; \omega_y^2\end{array}\right]\right) \\ 
\left[\begin{array}{c} e^x_{ij} \\ e^y_{ij} \end{array}\right] &amp;amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\sigma_x^2 &amp;amp; \rho \sigma_x \sigma_y \\ \rho \sigma_x \sigma_y &amp;amp; \sigma_y^2\end{array}\right]\right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the error terms are mutually independent unless otherwise noted. The raw Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is calculated using the total sums of squares and cross-products:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = \frac{SS_{xy}}{\sqrt{SS_{xx} SS_{yy}}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
SS_{xx} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right)^2, \qquad \bar{\bar{x}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} X_{ij} \\
SS_{xy} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(Y_{ij} - \bar{\bar{y}}\right)^2, \qquad \bar{\bar{y}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} Y_{ij} \\
SS_{xy} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right) \left(Y_{ij} - \bar{\bar{y}}\right).
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;common-correlation-and-icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common correlation and ICC&lt;/h3&gt;
&lt;p&gt;The distribution of the total correlation seems to be pretty complicated. So far, I’ve been able to obtain the variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; for a special case that makes some further, fairly restrictive assumptions. Specifically, assume that the correlation is constant across the two levels, so that &lt;span class=&#34;math inline&#34;&gt;\(\phi = \rho\)&lt;/span&gt;, and that the intra-class correlation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same as that of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(k = \omega_x^2 / \sigma_x^2 = \omega_y^2 / \sigma_y^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi = k / (k + 1) = \omega_x^2 / (\omega_x^2 + \sigma_x^2)\)&lt;/span&gt;. Then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(r) \approx \frac{(1 - \rho^2)^2}{\tilde{N}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{N[g_1 k + 1]^2}{g_2 k^2 + 2 g_1 k + 1} \approx \frac{N}{1 + (g_2 - g_1^2)\psi^2},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{g_1 = 1 - \frac{1}{N^2}\sum_{j=1}^m n_j^2}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{g_2 = \frac{1}{N}\sum_{j=1}^m n_j^2 - \frac{2}{N^2}\sum_{j=1}^m n_j^3 + \frac{1}{N^3} \left(\sum_{j=1}^m n_j^2 \right)^2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the clusters are all of equal size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{nm[k(m - 1) / m + 1]^2}{k^2 n (m - 1)/m + 2 k (m - 1) / m + 1} \approx \frac{N}{1 + (n - 1) \psi^2},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The right-hand expression is a further approximation that will be very close to right so long as &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is not too too small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;z-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Z-transformation&lt;/h3&gt;
&lt;p&gt;Under the (restrictive) assumptions of common correlation and equal ICCs, Fisher’s z transformation is variance-stabilizing (as it is under simple random sampling), so it seems reasonable to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(z(r)\right) \approx \frac{1}{\tilde{N} - 3}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;design-effect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Design effect&lt;/h3&gt;
&lt;p&gt;The design effect (&lt;span class=&#34;math inline&#34;&gt;\(DEF\)&lt;/span&gt;) is the ratio of the actual sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; to the sampling variance in a simple random sample of the same size. For the special case that I’ve described,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
DEF = \frac{N}{\tilde{N}} = 1 + (g_2 - g_1^2) \psi^2,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or with equal cluster-sizes, &lt;span class=&#34;math inline&#34;&gt;\(DEF = 1 + (n - 1)\psi^2\)&lt;/span&gt;. These expressions make it clear that the design effect for the correlation is &lt;em&gt;not&lt;/em&gt; equivalent to the well-known design effect for means or mean differences in cluster-randomized designs, which is &lt;span class=&#34;math inline&#34;&gt;\(1 + (n - 1)\psi\)&lt;/span&gt;. We need to take the &lt;em&gt;square&lt;/em&gt; of the ICC here, which will make the design effect for &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; &lt;em&gt;smaller&lt;/em&gt; than the design effect for a mean (or difference in means) based on the same sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-special-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other special cases&lt;/h3&gt;
&lt;p&gt;There are some further special cases that are not to hard to work out and could be useful as rough approximations at least. One is if the within-cluster correlation is zero &lt;span class=&#34;math inline&#34;&gt;\((\rho = 0)\)&lt;/span&gt; and we’re interested in the between-cluster correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. Then the total correlation can be corrected for what is essentially measurement error using formulas from &lt;a href=&#34;https://www.amazon.com/Methods-Meta-Analysis-Correcting-Research-Findings/dp/141290479X&#34;&gt;Hunter and Schmidt (2004)&lt;/a&gt;. A further specialization is if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a cluster-level measure, so that &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x^2 = 0\)&lt;/span&gt;. I’ll consider these in a later post, perhaps.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The multivariate delta method</title>
      <link>/multivariate-delta-method/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/multivariate-delta-method/</guid>
      <description>


&lt;p&gt;The delta method is surely one of the most useful techniques in classical statistical theory. It’s perhaps a bit odd to put it this way, but I would say that the delta method is something like the precursor to the bootstrap, in terms of its utility and broad range of applications—both are “first-line” tools for solving statistical problems. There are many good references on the delta-method, ranging from &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34;&gt;the Wikipedia page&lt;/a&gt; to a short introduction in &lt;em&gt;The American Statistician&lt;/em&gt; (&lt;a href=&#34;https://doi.org/10.1080%2F00031305.1992.10475842&#34;&gt;Oehlert, 1992&lt;/a&gt;). Many statistical theory textbooks also include a longer or shorter discussion of the method (e.g., Stuart &amp;amp; Ord, 1996; Casella &amp;amp; Berger, 2002).&lt;/p&gt;
&lt;p&gt;I use the delta method all the time in my work, especially to derive approximations to the sampling variance of some estimator (or covariance between two estimators). Here I’ll give one formulation of the multivariate delta method that I find particularly useful for this purpose. (This is nothing at all original. I’m only posting it on the off chance that others might find my crib notes helpful—and by “others” I mostly mean myself in six months…)&lt;/p&gt;
&lt;div id=&#34;multi-variate-delta-method-covariances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multi-variate delta method covariances&lt;/h3&gt;
&lt;p&gt;Suppose that we have a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-dimensional vector of statistics &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T} = \left(T_1,...,T_p \right)\)&lt;/span&gt; that converge in distribution to the parameter vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta = \left(\theta_1,...,\theta_p\right)\)&lt;/span&gt; and have asymptotic covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma / n\)&lt;/span&gt;, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{n} \left(\mathbf{T} - \boldsymbol\theta\right) \stackrel{D}{\rightarrow} N\left( \mathbf{0}, \boldsymbol\Sigma \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now consider two functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, both of which take vectors as inputs, return scalar quantities, and don’t have funky (discontinuous) derivatives. The asymptotic covariance between &lt;span class=&#34;math inline&#34;&gt;\(f(\mathbf{T})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(\mathbf{T})\)&lt;/span&gt; is then approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov} \left(f(\mathbf{T}), g(\mathbf{T}) \right) \approx \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^p  \frac{\partial f}{ \partial \theta_j}\frac{\partial g}{ \partial \theta_k}\sigma_{jk}, 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jk}\)&lt;/span&gt; is the entry in row &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma\)&lt;/span&gt;. If the entries of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt; are asymptotically uncorrelated , then this simplifies to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov} \left(f(\mathbf{T}), g(\mathbf{T}) \right) \approx \frac{1}{n} \sum_{j=1}^p \frac{\partial f}{ \partial \theta_j}\frac{\partial g}{ \partial \theta_j} \sigma_{jj}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we are interested in the variance of a single statistic, then the above formulas simplify further to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var} \left(f(\mathbf{T})\right) \approx \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^p  \frac{\partial f}{ \partial \theta_j}\frac{\partial f}{ \partial \theta_k}\sigma_{jk} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var} \left(f(\mathbf{T}) \right) \approx \frac{1}{n}\sum_{j=1}^p \left(\frac{\partial f}{ \partial \theta_j}\right)^2 \sigma_{jj}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;in the case of uncorrelated &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, if we are dealing with a univariate transformation &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;, then of course the above simplifies even further to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(f(T)\right) = \left(\frac{\partial f}{\partial \theta}\right)^2 \text{Var}(T)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pearsons-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;These formulas are useful for all sorts of things. For example, they can be used to derive the sampling variance of Pearson’s correlation coefficient. Suppose we have a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations from a multivariate normal distribution with mean 0 and variance-covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Phi = \left[\begin{array}{cc}\phi_{xx} &amp;amp; \phi_{xy} \\ \phi_{xy} &amp;amp; \phi_{yy} \end{array}\right]\)&lt;/span&gt;. Pearson’s correlation is calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = \frac{s_{xy}}{\sqrt{s_{xx} s_{yy}}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s_{xx}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{yy}\)&lt;/span&gt; are sample variances and &lt;span class=&#34;math inline&#34;&gt;\(s_{xy}\)&lt;/span&gt; is the sample covariance. These sample variances and covariances are unbiased estimates of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{xx}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\phi_{yy}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\phi_{xy}\)&lt;/span&gt;, respectively. So in terms of the above notation, we have &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T} = \left(s_{xx}, s_{yy}, s_{xy}\right)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta = \left(\phi_{xx}, \phi_{yy}, \phi_{xy}\right)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\rho = \phi_{xy} / \sqrt{\phi_{xx} \phi_{yy}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;/distribution-of-sample-variances&#34;&gt;a previous post&lt;/a&gt;, we can work out the variance-covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(\sqrt{n - 1} \left[\begin{array}{c} s_{xx} \\ s_{yy} \\ s_{xy}\end{array}\right]\right) = \boldsymbol\Sigma = \left[\begin{array}{ccc} 2 \phi_{xx}^2 &amp;amp; &amp;amp; \\ 2 \phi_{xy}^2 &amp;amp; 2 \phi_{yy}^2 &amp;amp; \\ 2 \phi_{xy} \phi_{xx} &amp;amp; 2 \phi_{xy} \phi_{yy} &amp;amp; \phi_{xy}^2 + \phi_{xx} \phi_{yy}\end{array}\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The last piece is to find the derivatives of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial r}{\partial \phi_{xy}} &amp;amp;= \phi_{xx}^{-1/2} \phi_{yy}^{-1/2} \\
\frac{\partial r}{\partial \phi_{xx}} &amp;amp;= -\frac{1}{2} \phi_{xy} \phi_{xx}^{-3/2} \phi_{yy}^{-1/2} \\
\frac{\partial r}{\partial \phi_{yy}} &amp;amp;= -\frac{1}{2} \phi_{xy} \phi_{xx}^{-1/2} \phi_{yy}^{-3/2}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Putting the pieces together, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(n - 1) \text{Var}(r) &amp;amp;\approx \sigma_{11} \left(\frac{\partial r}{\partial \phi_{xy}}\right)^2 + \sigma_{22} \left(\frac{\partial r}{ \partial \phi_{xx}}\right)^2 + \sigma_{33} \left(\frac{\partial r}{ \partial \phi_{yy}}\right)^2 \\
&amp;amp; \qquad \qquad + 2 \sigma_{12} \frac{\partial r}{\partial \phi_{xy}}\frac{\partial r}{\partial \phi_{xx}} + 2 \sigma_{13} \frac{\partial r}{\partial \phi_{xy}}\frac{\partial r}{\partial \phi_{yy}}+ 2 \sigma_{23} \frac{\partial r}{\partial \phi_{xx}}\frac{\partial r}{\partial \phi_{yy}} \\
&amp;amp;= \frac{\phi_{xy}^2 + \phi_{xx} \phi_{yy}}{\phi_{xx} \phi_{yy}} + \frac{\phi_{xy}^2\phi_{xx}^2}{2 \phi_{xx}^3 \phi_{yy}} + \frac{\phi_{xy}^2\phi_{yy}^2}{2 \phi_{xx} \phi_{yy}^3} \\
&amp;amp; \qquad \qquad - \frac{2\phi_{xy} \phi_{xx}}{\phi_{xx}^2 \phi_{yy}} - \frac{2\phi_{xy} \phi_{yy}}{\phi_{xx} \phi_{yy}^2} + \frac{\phi_{xy}^4}{\phi_{xx}^2 \phi_{yy}^2} \\
&amp;amp;= 1 - 2\frac{\phi_{xy}^2}{\phi_{xx} \phi_{yy}} + \frac{\phi_{xy}^4}{\phi_{xx}^2 \phi_{yy}^2} \\
&amp;amp;= \left(1 - \rho^2\right)^2.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fishers-z-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fisher’s &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-transformation&lt;/h3&gt;
&lt;p&gt;Meta-analysts will be very familiar with Fisher’s &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-transformation of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, given by &lt;span class=&#34;math inline&#34;&gt;\(z(\rho) = \frac{1}{2} \log\left(\frac{1 + \rho}{1 - \rho}\right)\)&lt;/span&gt;.
Fisher’s &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the variance-stabilizing (and also normalizing) transformation of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, meaning that &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(z(r)\right)\)&lt;/span&gt; is approximately a constant function of sample size, not depending on the degree of correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. We can see this using another application of the delta method:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial z}{\partial \rho} = \frac{1}{1 - \rho^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(z(r)\right) \approx \frac{1}{(1 - \rho^2)^2} \times \text{Var}(r) = \frac{1}{n - 1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The variance of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is usually given as &lt;span class=&#34;math inline&#34;&gt;\(1 / (n - 3)\)&lt;/span&gt;, which is even closer to exact. Here we’ve obtained the variance of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; using two applications of the delta-method. Because of &lt;a href=&#34;https://en.wikipedia.org/wiki/Chain_rule&#34;&gt;the chain rule&lt;/a&gt;, we’d have ended up with the same result if we’d gone straight from the sample variances and covariances, using the multivariate delta method and the derivatives of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covariances-between-correlations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Covariances between correlations&lt;/h3&gt;
&lt;p&gt;These same techniques can be used to work out expressions for the covariances between correlations estimated on the same sample. For instance, suppose you’ve measured four variables, &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, on a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations. What is &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(r_{xy}, r_{xz})\)&lt;/span&gt;? What is &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(r_{wx}, r_{yz})\)&lt;/span&gt;? I’ll leave the derivations for you to work out. See &lt;a href=&#34;http://dx.doi.org/10.1037//0033-2909.87.2.245&#34;&gt;Steiger (1980)&lt;/a&gt; for solutions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2SLS standard errors and the delta-method</title>
      <link>/delta-method-and-2sls-ses/</link>
      <pubDate>Sat, 07 Oct 2017 00:00:00 +0000</pubDate>
      <guid>/delta-method-and-2sls-ses/</guid>
      <description>


&lt;p&gt;I just covered instrumental variables in my course on causal inference, and so I have two-stage least squares (2SLS) estimation on the brain. In this post I’ll share something I realized in the course of prepping for class: that standard errors from 2SLS estimation are equivalent to delta method standard errors based on the Wald IV estimator. (I’m no econometrician, so this had never occurred to me before. Perhaps it will be interesting to other non-econometrician readers. And perhaps the econometricians can point me to the relevant page in Wooldridge or Angrist and Pischke or whomever that explains this better than I have.)&lt;/p&gt;
&lt;p&gt;Let’s consider a system with an outcome &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, a focal treatment &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; identified by a single instrument &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;, along with a row-vector of exogenous covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i\)&lt;/span&gt;, all for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,n\)&lt;/span&gt;. The usual estimating equations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
y_i &amp;amp;= \mathbf{x}_i \delta_0 + t_i \delta_1 + e_i \\
t_i &amp;amp;= \mathbf{x}_i \alpha_0 + z_i \alpha_1 + u_i.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With a single-instrument, the 2SLS estimator of &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; is exactly equivalent to the Wald estimator&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\delta_1 = \frac{\hat\beta_1}{\hat\alpha_1},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat\alpha_1\)&lt;/span&gt; is the OLS estimator from the first-stage regression of &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\)&lt;/span&gt; is the OLS estimator from the regression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y_i = \mathbf{x}_i \beta_0 + z_i \beta_1 + v_i.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The delta-method approximation for &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\hat\delta_1)\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(\hat\delta_1\right) \approx \frac{1}{\alpha_1^2}\left[ \text{Var}\left(\hat\beta_1\right) + \delta_1^2 \text{Var}\left(\hat\alpha_1\right) - 2 \delta_1 \text{Cov}\left(\hat\beta_1, \hat\alpha_1\right) \right]. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting the estimators in place of parameters, and using heteroskedasticity-consistent (HC0, to be precise) estimators for &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(\hat\beta_1\right)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(\hat\alpha_1\right)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}\left(\hat\beta_1, \hat\alpha_1\right)\)&lt;/span&gt;, it turns out the feasible delta-method variance estimator is &lt;em&gt;exactly&lt;/em&gt; equivalent to the HC0 variance estimator from 2SLS.&lt;/p&gt;
&lt;div id=&#34;connecting-delta-method-and-2sls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Connecting delta-method and 2SLS&lt;/h3&gt;
&lt;p&gt;To demonstrate this claim, let’s first partial out the covariates, taking &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{y}} = \left[\mathbf{I} - \mathbf{X}\left(\mathbf{X}&amp;#39;\mathbf{X}\right)^{-1}\mathbf{X}&amp;#39;\right]\mathbf{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{t}} = \left[\mathbf{I} - \mathbf{X}\left(\mathbf{X}&amp;#39;\mathbf{X}\right)^{-1}\mathbf{X}&amp;#39;\right]\mathbf{t}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}} = \left[\mathbf{I} - \mathbf{X}\left(\mathbf{X}&amp;#39;\mathbf{X}\right)^{-1}\mathbf{X}&amp;#39;\right]\mathbf{z}\)&lt;/span&gt;. The OLS estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; are then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\beta_1 = \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{y}}, \qquad \text{and} \qquad \hat\alpha_1 = \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{t}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The HC0 variance and covariance estimators for these coefficients have the usual sandwich form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^{\beta_1} &amp;amp;= \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left(\sum_{i=1}^n \ddot{z}_i^2 \ddot{v}_i^2\right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
V^{\alpha_1} &amp;amp;= \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left(\sum_{i=1}^n \ddot{z}_i^2 \ddot{u}_i^2\right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
V^{\alpha_1\beta_1} &amp;amp;= \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left(\sum_{i=1}^n \ddot{z}_i^2 \ddot{u}_i \ddot{v}_i\right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\ddot{v}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ddot{u}_i\)&lt;/span&gt; are the residuals from the regressions of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{y}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{t}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}}\)&lt;/span&gt;, respectively. Combining all these terms, the delta-method variance estimator is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V^{\delta_1} = \frac{1}{\hat\alpha_1^2}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left[\sum_{i=1}^n \ddot{z}_i^2\left(\ddot{v}_i^2 + \hat\delta_1^2 \ddot{u}_i^2 - 2 \hat\delta_1\ddot{u}_i \ddot{v}_i\right)\right] \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember this formula because we’ll return to it shortly.&lt;/p&gt;
&lt;p&gt;Now consider the 2SLS estimator. To calculate this, we begin by taking the fitted values from the regression of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{t}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{\tilde{t}} = \mathbf{\ddot{z}}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{t}} = \mathbf{\ddot{z}} \hat\alpha_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We then regress &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{y}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\tilde{t}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\delta_1 = \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \mathbf{\tilde{t}}&amp;#39; \mathbf{\ddot{y}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The HC0 variance estimator corresponding to the 2SLS estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V^{2SLS} = \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \left(\sum_{i=1}^n \tilde{t}_i^2 \tilde{e}_i^2 \right) \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\tilde{e}_i = \ddot{y}_i - \ddot{t}_i \hat\delta_1\)&lt;/span&gt;. Note that these residuals are calculated based on &lt;span class=&#34;math inline&#34;&gt;\(\ddot{t}_i\)&lt;/span&gt;, the &lt;em&gt;full&lt;/em&gt; treatment variable, not the fitted values &lt;span class=&#34;math inline&#34;&gt;\(\tilde{t}_i\)&lt;/span&gt;. The full treatment variable can be expressed as &lt;span class=&#34;math inline&#34;&gt;\(\ddot{t}_i = \tilde{t}_i + \ddot{u}_i\)&lt;/span&gt;, by which it follows that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{e}_i = \ddot{y}_i - \tilde{t}_i \hat\delta_1 - \ddot{u}_i \hat\delta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But &lt;span class=&#34;math inline&#34;&gt;\(\tilde{t}_i \hat\delta_1 = \ddot{z}_i \hat\alpha_1 \hat\delta_1 = \ddot{z}_i \hat\beta_1\)&lt;/span&gt;, and so&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{e}_i = \ddot{y}_i - \ddot{z}_i \hat\beta_1 - \ddot{u}_i \hat\delta_1 = \ddot{v}_i - \ddot{u}_i \hat\delta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The 2SLS variance estimator is therefore&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^{2SLS} &amp;amp;= \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \left(\sum_{i=1}^n \tilde{t}_i^2 \tilde{e}_i^2 \right) \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \\
&amp;amp;= \left(\hat\alpha_1^2 \mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \left(\sum_{i=1}^n \hat\alpha_1^2 \ddot{z}_i^2 \tilde{e}_i^2 \right) \left(\hat\alpha_1^2 \mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
&amp;amp;= \frac{1}{\hat\alpha_1^2}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \left(\sum_{i=1}^n \ddot{z}_i^2 \tilde{e}_i^2 \right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
&amp;amp;= \frac{1}{\hat\alpha_1^2}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \left[\sum_{i=1}^n \ddot{z}_i^2 \left(\ddot{v}_i - \ddot{u}_i \hat\delta_1\right)^2 \right] \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which agrees with &lt;span class=&#34;math inline&#34;&gt;\(V^{\delta_1}\)&lt;/span&gt; as given above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-what&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;So what?&lt;/h3&gt;
&lt;p&gt;If you’ve continued reading this far…I’m slightly amazed…but if you have, you may be wondering why it’s worth knowing about this relationship. The equivalence between the 2SLS variance estimator and the delta method interests me for a couple of reasons.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First is that I had always taken the 2SLS variance estimator as being conditional on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{t}\)&lt;/span&gt;–that is, not accounting for random variation in the treatment assignment. The delta-method form of the variance makes it crystal clear that this isn’t the case—the variance &lt;em&gt;does&lt;/em&gt; include terms for &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\hat\alpha_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\hat\beta_1, \hat\alpha_1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;On the other hand, there’s perhaps a sense that equivalence with the 2SLS variance estimator (the more familiar form) validates the delta method variance estimator—that is, we wouldn’t be doing something fundamentally different by using the delta method variance with a Wald estimator. For instance, we might want to estimate &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; and/or &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; by some other means (e.g., by estimating &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; as a marginal effect from a logistic regression or estimating &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; with a multi-level model). It would make good sense in this instance to use the Wald estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 / \hat\alpha_1\)&lt;/span&gt; and to estimate its variance using the delta method form.&lt;/li&gt;
&lt;li&gt;One last reason I’m interested in this is that writing out the variance estimators will likely help in understanding how to approach small-sample corrections to &lt;span class=&#34;math inline&#34;&gt;\(V^{2SLS}\)&lt;/span&gt;. But I’ll save that for another day.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
