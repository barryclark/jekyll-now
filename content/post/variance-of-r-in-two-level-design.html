---
title: Sampling variance of Pearson r in a two-level design
author: 'James'
date: '2018-04-19'
slug: variance-of-r-in-two-level-design
categories: []
tags:
  - effect sizes
  - meta-analysis
  - delta method
  - distribution theory
header:
  caption: ''
  image: ''
---



<p>Consider Pearson’s correlation coefficient, <span class="math inline">\(r\)</span>, calculated from two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with population correlation <span class="math inline">\(\rho\)</span>. If one calculates <span class="math inline">\(r\)</span> from a simple random sample of <span class="math inline">\(N\)</span> observations, then its sampling variance will be approximately</p>
<p><span class="math display">\[
\text{Var}(r) \approx \frac{1}{N}\left(1 - \rho^2\right)^2.
\]</span></p>
<p>But what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the <span class="math inline">\(r\)</span> will actually be a weighted average of within-cluster and between-cluster correlations (see Snijders &amp; Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of <span class="math inline">\(r\)</span> from a multi-stage sample would not necessarily be the same as that from a simple random sample. What is the sampling variance of <span class="math inline">\(r\)</span> in this design?</p>
<p>Let me be more precise here by formalizing the sampling process. Suppose that we have a sample with <span class="math inline">\(m\)</span> clusters, <span class="math inline">\(n_j\)</span> observations in cluster <span class="math inline">\(j\)</span>, and total sample size <span class="math inline">\(N = \sum_{j=1}^m n_j\)</span>. Assume that</p>
<p><span class="math display">\[
\begin{aligned}
X_{ij} &amp;= \mu_x + v^x_j + e^x_{ij} \\
Y_{ij} &amp;= \mu_y + v^y_j + e^y_{ij},
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(i=1,...,n_j\)</span> and <span class="math inline">\(j=1,...,m\)</span>, where</p>
<p><span class="math display">\[
\begin{aligned}
\left[\begin{array}{c} v^x_j \\ v^y_j \end{array}\right] &amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\omega_x^2 &amp; \phi \omega_x \omega_y \\ \phi \omega_x \omega_y &amp; \omega_y^2\end{array}\right]\right) \\ 
\left[\begin{array}{c} e^x_{ij} \\ e^y_{ij} \end{array}\right] &amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\sigma_x^2 &amp; \rho \sigma_x \sigma_y \\ \rho \sigma_x \sigma_y &amp; \sigma_y^2\end{array}\right]\right)
\end{aligned}
\]</span></p>
<p>and the error terms are mutually independent unless otherwise noted. The raw Pearson’s <span class="math inline">\(r\)</span> is calculated using the total sums of squares and cross-products:</p>
<p><span class="math display">\[
r = \frac{SS_{xy}}{\sqrt{SS_{xx} SS_{yy}}},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
SS_{xx} &amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right)^2, \qquad \bar{\bar{x}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} X_{ij} \\
SS_{xy} &amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(Y_{ij} - \bar{\bar{y}}\right)^2, \qquad \bar{\bar{y}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} Y_{ij} \\
SS_{xy} &amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right) \left(Y_{ij} - \bar{\bar{y}}\right).
\end{aligned}
\]</span></p>
<div id="common-correlation-and-icc" class="section level3">
<h3>Common correlation and ICC</h3>
<p>The distribution of the total correlation seems to be pretty complicated. So far, I’ve been able to obtain the variance of <span class="math inline">\(r\)</span> for a special case that makes some further, fairly restrictive assumptions. Specifically, assume that the correlation is constant across the two levels, so that <span class="math inline">\(\phi = \rho\)</span>, and that the intra-class correlation of <span class="math inline">\(X\)</span> is the same as that of <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(k = \omega_x^2 / \sigma_x^2 = \omega_y^2 / \sigma_y^2\)</span> and <span class="math inline">\(\psi = k / (k + 1) = \omega_x^2 / (\omega_x^2 + \sigma_x^2)\)</span>. Then</p>
<p><span class="math display">\[
\text{Var}(r) \approx \frac{(1 - \rho^2)^2}{\tilde{N}},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\tilde{N} = \frac{N[g_1 k + 1]^2}{g_2 k^2 + 2 g_1 k + 1} \approx \frac{N}{1 + (g_2 - g_1^2)\psi^2},
\]</span></p>
<p>with <span class="math inline">\(\displaystyle{g_1 = 1 - \frac{1}{N^2}\sum_{j=1}^m n_j^2}\)</span>, and <span class="math inline">\(\displaystyle{g_2 = \frac{1}{N}\sum_{j=1}^m n_j^2 - \frac{2}{N^2}\sum_{j=1}^m n_j^3 + \frac{1}{N^3} \left(\sum_{j=1}^m n_j^2 \right)^2}\)</span>.</p>
<p>If the clusters are all of equal size <span class="math inline">\(n\)</span>, then</p>
<p><span class="math display">\[
\tilde{N} = \frac{nm[k(m - 1) / m + 1]^2}{k^2 n (m - 1)/m + 2 k (m - 1) / m + 1} \approx \frac{N}{1 + (n - 1) \psi^2},
\]</span></p>
<p>The right-hand expression is a further approximation that will be very close to right so long as <span class="math inline">\(m\)</span> is not too too small.</p>
</div>
<div id="z-transformation" class="section level3">
<h3>Z-transformation</h3>
<p>Under the (restrictive) assumptions of common correlation and equal ICCs, Fisher’s z transformation is variance-stabilizing (as it is under simple random sampling), so it seems reasonable to use</p>
<p><span class="math display">\[
\text{Var}\left(z(r)\right) \approx \frac{1}{\tilde{N} - 3}.
\]</span></p>
</div>
<div id="design-effect" class="section level3">
<h3>Design effect</h3>
<p>The design effect (<span class="math inline">\(DEF\)</span>) is the ratio of the actual sampling variance of <span class="math inline">\(r\)</span> to the sampling variance in a simple random sample of the same size. For the special case that I’ve described,</p>
<p><span class="math display">\[
DEF = \frac{N}{\tilde{N}} = 1 + (g_2 - g_1^2) \psi^2,
\]</span></p>
<p>or with equal cluster-sizes, <span class="math inline">\(DEF = 1 + (n - 1)\psi^2\)</span>. These expressions make it clear that the design effect for the correlation is <em>not</em> equivalent to the well-known design effect for means or mean differences in cluster-randomized designs, which is <span class="math inline">\(1 + (n - 1)\psi\)</span>. We need to take the <em>square</em> of the ICC here, which will make the design effect for <span class="math inline">\(r\)</span> <em>smaller</em> than the design effect for a mean (or difference in means) based on the same sample.</p>
</div>
<div id="other-special-cases" class="section level3">
<h3>Other special cases</h3>
<p>There are some further special cases that are not to hard to work out and could be useful as rough approximations at least. One is if the within-cluster correlation is zero <span class="math inline">\((\rho = 0)\)</span> and we’re interested in the between-cluster correlation <span class="math inline">\(\phi\)</span>. Then the total correlation can be corrected for what is essentially measurement error using formulas from <a href="https://www.amazon.com/Methods-Meta-Analysis-Correcting-Research-Findings/dp/141290479X">Hunter and Schmidt (2004)</a>. A further specialization is if <span class="math inline">\(X\)</span> is a cluster-level measure, so that <span class="math inline">\(\sigma_x^2 = 0\)</span>. I’ll consider these in a later post, perhaps.</p>
</div>
